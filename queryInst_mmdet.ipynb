{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saidineshpola/mesh-transformer-jax/blob/master/queryInst_mmdet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EokzqTToqttr"
      },
      "source": [
        "![AIcrowd-Logo](https://raw.githubusercontent.com/AIcrowd/AIcrowd/master/app/assets/images/misc/aicrowd-horizontal.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cc_H7z-N0o"
      },
      "source": [
        "# 🍕 Food Recognition Benchmark\n",
        "\n",
        "\n",
        "# Problem Statement\n",
        "Detecting & Segmenting various kinds of food from an image. For ex. Someone got into new restaurent and get a food that he has never seen, well our DL model is in rescue, so our DL model will help indentifying which food it is from the class our model is being trained on!    \n",
        "\n",
        "<img src=\"https://i.imgur.com/zS2Nbf0.png\" width=\"300\" />\n",
        "\n",
        "\n",
        "# Dataset\n",
        "We will be using data from Food Recognition Challenge - A benchmark for image-based food recognition challange which is running since 2020.\n",
        "\n",
        "\n",
        "https://www.aicrowd.com/challenges/food-recognition-benchmark-2022#datasets\n",
        "\n",
        "We have a total of **39k training images** with **3k validation set** and **4k public-testing set**. All the images are RGB and annotations exist in **MS-COCO format**. \n",
        "\n",
        "<img src=\"https://lh5.googleusercontent.com/iySoTCAHFoEKxjvzELzCJKbZaTG2TzMcjuBxAlBVGupjkpE_XI1xNPnE71UIBthTu9_fZ4A1tz-ArABpI0DD2ZeF87qHPccRogEezd-UbhkQgZcQBYCE1HMeDusaKtj8ClCWjw-p\">\n",
        "\n",
        "<small>Reference: This notebook is based on the notebook created by [Shraddhaa Mohan](https://www.linkedin.com/in/shraddhaa-mohan-20a008185/) and [Rohit Midha](https://www.linkedin.com/in/rohitmidha/) for previous iteration of the challenge. You can find the [original notebook here](https://colab.research.google.com/drive/1vKAQ9D3dgubbBc2jGYGQB0-lZXlT8hTh#scrollTo=Dha6_NXmIzB9).</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhuIg5-yqtt2"
      },
      "source": [
        "In this Notebook, we will first do an analysis of the Food Recognition Dataset and then use maskrcnn for training on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piyKwiK0qtt6"
      },
      "source": [
        "## The Challenge\n",
        "\n",
        "\n",
        "*   Given Images of Food, we are asked to provide Instance Segmentation over the images for the food items.\n",
        "*   The Training Data is provided in the COCO format, making it simpler to load with pre-available COCO data processors in popular libraries.\n",
        "*   The test set provided in the public dataset is similar to Validation set, but with no annotations.\n",
        "*   The test set after submission is much larger and contains private images upon which every submission is evaluated.\n",
        "*   Pariticipants have to submit their trained model along with trained weights. Immediately after the submission the AICrowd Grader picks up the submitted model and produces inference on the private test set using Cloud GPUs.\n",
        "*   This requires Users to structure their repositories and follow a provided paradigm for submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd3Gi0g5qtuE"
      },
      "source": [
        "## The Notebook\n",
        "> *  Installation of MMDetection\n",
        "> *  Training a simple model with MMDetection\n",
        "> *  Local Evaluation/Quick Submision using MMDetection\n",
        "> * Active Submission using trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l2EucOyJL3u"
      },
      "source": [
        "# GPU Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx5Yb4oKRpkK"
      },
      "source": [
        "Do a quick check if you have been allocated a GPU. \n",
        "\n",
        "If this command fails for you, please go to `Runtime` -> `Change Runtime Type` -> `Hardware Accelerator` -> `GPU`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H0j5oHHMNUd",
        "outputId": "e72f83b3-abde-46d6-c392-696970495533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  6 10:11:40 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwDa7_wkVQhK"
      },
      "source": [
        "# Setting our Workspace 💼\n",
        "\n",
        "In this section we will be downloading our dataset, unzipping it & downloading mmdetection repo/library and importing all libraries that we will be using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXhfQ9xp-aJe",
        "outputId": "9fe98008-5295-4e10-b95a-09ec75e933ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[32mAPI Key valid\u001b[0m\n",
            "\u001b[33mGitlab oauth token invalid or absent.\n",
            "It is highly recommended to simply run `aicrowd login` without passing the API Key.\u001b[0m\n",
            "\u001b[32mSaved details successfully!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Login to AIcrowd\n",
        "!pip install aicrowd-cli > /dev/null\n",
        "#!aicrowd login\n",
        "\n",
        "########## or ################\n",
        "# Get your API key from https://www.aicrowd.com/participants/me\n",
        "API_KEY = \"61a473a8ff6ff34c77e7f9f8544ef7dd\"\n",
        "!aicrowd login --api-key $API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qRdC5yyd-c0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3631890-c086-4547-f8ae-f8ba7a005724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[3m                          Datasets for challenge #962                           \u001b[0m\n",
            "┌───┬───────────────────────────────┬───────────────────────────────┬──────────┐\n",
            "│\u001b[1;35m \u001b[0m\u001b[1;35m#\u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;35m \u001b[0m\u001b[1;35mTitle                        \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;35m \u001b[0m\u001b[1;35mDescription                  \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;35m \u001b[0m\u001b[1;35m    Size\u001b[0m\u001b[1;35m \u001b[0m│\n",
            "├───┼───────────────────────────────┼───────────────────────────────┼──────────┤\n",
            "│ 0 │ random_prediction.json        │ Random prediction for Quick   │  4.36 MB │\n",
            "│   │                               │ Submission into Round 2       │          │\n",
            "│ 1 │ [Round 1]                     │ [Public] Testing Dataset      │     197M │\n",
            "│   │ public_test_release_2.0.tar.… │ (contains 3000 images and 498 │          │\n",
            "│   │                               │ categories, without           │          │\n",
            "│   │                               │ annotations)                  │          │\n",
            "│ 2 │ [Round 1]                     │ Training Dataset (contains    │   2.14GB │\n",
            "│   │ public_training_set_release_… │ 39962 images and 498          │          │\n",
            "│   │                               │ categories)                   │          │\n",
            "│ 3 │ [Round 1]                     │ Validation Dataset (contains  │      59M │\n",
            "│   │ public_validation_set_2.0.ta… │ 1000 images and 498           │          │\n",
            "│   │                               │ categories, with annotations) │          │\n",
            "│ 4 │ [Round 2]                     │ [Public] Testing Dataset      │     185M │\n",
            "│   │ public_test_release_2.1.tar.… │ (contains 2819 images,        │          │\n",
            "│   │                               │ without annotations)          │          │\n",
            "│ 5 │ [Round 2]                     │ (optional) annotations file   │ 32.66 MB │\n",
            "│   │ public_training_annotations_… │ for v2.1 dataset in case you  │          │\n",
            "│   │                               │ already have images           │          │\n",
            "│   │                               │ downloaded                    │          │\n",
            "│ 6 │ [Round 2]                     │ Training Dataset (contains    │     3.1G │\n",
            "│   │ public_training_set_release_… │ 54392 images and 323          │          │\n",
            "│   │                               │ categories)                   │          │\n",
            "│ 7 │ [Round 2]                     │ Validation Dataset (contains  │      57M │\n",
            "│   │ public_validation_set_2.1.ta… │ 946 images and 323            │          │\n",
            "│   │                               │ categories, with annotations) │          │\n",
            "└───┴───────────────────────────────┴───────────────────────────────┴──────────┘\n",
            "public_test_release_2.1.tar.gz:   0% 0.00/166M [00:00<?, ?B/s]\n",
            "public_test_release_2.1.tar.gz:  40% 67.1M/166M [00:07<00:10, 9.14MB/s]\n",
            "public_test_release_2.1.tar.gz:  61% 101M/166M [00:11<00:07, 8.96MB/s] \n",
            "public_test_release_2.1.tar.gz:  81% 134M/166M [00:14<00:03, 9.18MB/s]\n",
            "public_test_release_2.1.tar.gz: 100% 166M/166M [00:16<00:00, 9.82MB/s]\n",
            "\n",
            "public_validation_set_release_2.1.tar.gz:   0% 0.00/55.1M [00:00<?, ?B/s]\n",
            "public_validation_set_release_2.1.tar.gz:  61% 33.6M/55.1M [00:06<00:03, 5.44MB/s]\n",
            "public_validation_set_release_2.1.tar.gz: 100% 55.1M/55.1M [00:09<00:00, 5.67MB/s]\n",
            "\n",
            "public_training_set_release_2.1.tar.gz:   8% 235M/3.12G [00:29<05:44, 8.37MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:   9% 268M/3.12G [00:32<05:24, 8.78MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  10% 302M/3.12G [00:37<05:44, 8.19MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  11% 336M/3.12G [00:42<05:53, 7.86MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  12% 369M/3.12G [00:46<05:58, 7.66MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  13% 403M/3.12G [00:51<06:02, 7.50MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  14% 436M/3.12G [00:55<05:39, 7.91MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  15% 470M/3.12G [00:59<05:29, 8.03MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  16% 503M/3.12G [01:02<05:13, 8.35MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  17% 537M/3.12G [01:05<04:49, 8.93MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  18% 570M/3.12G [01:08<04:22, 9.69MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  19% 604M/3.12G [01:11<04:14, 9.90MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  20% 638M/3.12G [01:15<04:05, 10.1MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  22% 671M/3.12G [01:18<03:56, 10.4MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  23% 705M/3.12G [01:21<03:59, 10.1MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  24% 738M/3.12G [01:24<03:50, 10.3MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  25% 772M/3.12G [01:27<03:39, 10.7MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  26% 805M/3.12G [01:31<03:54, 9.85MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  27% 839M/3.12G [01:34<03:39, 10.4MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  28% 872M/3.12G [01:37<03:26, 10.9MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  29% 906M/3.12G [01:39<03:06, 11.8MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  30% 940M/3.12G [01:42<03:06, 11.7MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  31% 973M/3.12G [01:45<03:10, 11.3MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  32% 1.01G/3.12G [01:48<03:04, 11.4MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  33% 1.04G/3.12G [01:51<03:11, 10.8MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  34% 1.07G/3.12G [01:55<03:15, 10.5MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  36% 1.11G/3.12G [01:58<03:13, 10.4MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  37% 1.14G/3.12G [02:01<03:07, 10.5MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  38% 1.17G/3.12G [02:05<03:08, 10.3MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  39% 1.21G/3.12G [02:08<03:09, 10.1MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  40% 1.24G/3.12G [02:12<03:13, 9.72MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  41% 1.28G/3.12G [02:16<03:17, 9.31MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  42% 1.31G/3.12G [02:20<03:18, 9.11MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  43% 1.34G/3.12G [02:24<03:18, 8.93MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  44% 1.38G/3.12G [02:28<03:28, 8.38MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  45% 1.41G/3.12G [02:32<03:20, 8.51MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  46% 1.44G/3.12G [02:36<03:11, 8.76MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  47% 1.48G/3.12G [02:39<02:57, 9.24MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  48% 1.51G/3.12G [02:43<03:07, 8.56MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  49% 1.54G/3.12G [02:47<03:01, 8.69MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  51% 1.58G/3.12G [02:50<02:38, 9.73MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  52% 1.61G/3.12G [02:53<02:35, 9.71MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  53% 1.64G/3.12G [02:57<02:37, 9.38MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  54% 1.68G/3.12G [03:00<02:31, 9.54MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  55% 1.71G/3.12G [03:03<02:22, 9.84MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  56% 1.74G/3.12G [03:07<02:17, 9.97MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  57% 1.78G/3.12G [03:10<02:17, 9.76MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  58% 1.81G/3.12G [03:13<02:07, 10.2MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  59% 1.85G/3.12G [03:16<02:01, 10.5MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  60% 1.88G/3.12G [03:20<02:01, 10.2MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  61% 1.91G/3.12G [03:22<01:51, 10.8MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  62% 1.95G/3.12G [03:26<01:49, 10.7MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  63% 1.98G/3.12G [03:29<01:45, 10.8MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  65% 2.01G/3.12G [03:32<01:41, 10.9MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  66% 2.05G/3.12G [03:35<01:41, 10.5MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  67% 2.08G/3.12G [03:39<01:41, 10.2MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  68% 2.11G/3.12G [03:42<01:38, 10.2MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  69% 2.15G/3.12G [03:45<01:36, 10.0MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  70% 2.18G/3.12G [03:49<01:33, 10.0MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  71% 2.21G/3.12G [03:52<01:32, 9.82MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  72% 2.25G/3.12G [03:56<01:27, 9.95MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  73% 2.28G/3.12G [03:59<01:26, 9.66MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  74% 2.32G/3.12G [04:03<01:22, 9.70MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  75% 2.35G/3.12G [04:06<01:16, 10.1MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  76% 2.38G/3.12G [04:08<01:07, 10.9MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  77% 2.42G/3.12G [04:11<01:02, 11.3MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  79% 2.45G/3.12G [04:14<00:56, 11.8MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  80% 2.48G/3.12G [04:16<00:51, 12.4MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  81% 2.52G/3.12G [04:19<00:53, 11.3MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  82% 2.55G/3.12G [04:22<00:48, 11.7MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  83% 2.58G/3.12G [04:25<00:44, 12.0MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  84% 2.62G/3.12G [04:27<00:40, 12.3MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  85% 2.65G/3.12G [04:30<00:39, 11.7MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  86% 2.68G/3.12G [04:33<00:36, 11.8MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  87% 2.72G/3.12G [04:36<00:34, 11.7MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  88% 2.75G/3.12G [04:39<00:32, 11.4MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  89% 2.79G/3.12G [04:42<00:27, 12.3MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  90% 2.82G/3.12G [04:44<00:24, 12.4MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  91% 2.85G/3.12G [04:47<00:22, 12.0MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  93% 2.89G/3.12G [04:51<00:21, 11.0MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  94% 2.92G/3.12G [04:54<00:17, 11.2MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  95% 2.95G/3.12G [04:57<00:14, 11.1MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  96% 2.99G/3.12G [04:59<00:11, 11.6MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  97% 3.02G/3.12G [05:03<00:08, 11.2MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  98% 3.05G/3.12G [05:06<00:06, 10.7MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz:  99% 3.09G/3.12G [05:09<00:02, 10.8MB/s]\u001b[A\n",
            "public_training_set_release_2.1.tar.gz: 100% 3.12G/3.12G [05:12<00:00, 9.97MB/s]\n"
          ]
        }
      ],
      "source": [
        "# List dataset for this challenge\n",
        "!aicrowd dataset list -c food-recognition-benchmark-2022\n",
        "\n",
        "# Download dataset\n",
        "!aicrowd dataset download -c food-recognition-benchmark-2022 4 6 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qTwjnrWz-igm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7240f1cd-de70-4777-8b1d-8f2ff4bcf483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting test dataset\n",
            "Extracting val dataset\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
            "Extracting train dataset\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/ data/train data/val data/test\n",
        "!echo \"Extracting test dataset\" && tar -xvf public_test_release_2.1.tar.gz -C data/test  > /dev/null\n",
        "!echo \"Extracting val dataset\" && tar -xvf public_validation_set_release_2.1.tar.gz -C data/val  > /dev/null\n",
        "!echo \"Extracting train dataset\" && tar -xvf public_training_set_release_2.1.tar.gz -C data/train  > /dev/null\n",
        "!rm -r *.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9_amZaW_AVxA",
        "outputId": "75733da0-38b7-4d3c-9ecd-89564420875b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.5G\t/content/data/train/\n"
          ]
        }
      ],
      "source": [
        "!du -sh /content/data/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGQXbjCPV0_g"
      },
      "source": [
        "## Mount the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kXSsyJk_rGQ",
        "outputId": "6c484497-66b8-4fef-aeee-406581b682d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#alternatively copy files from drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDNcibwIxRPm"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OJkCDXw49FJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c42319f-f7ff-4358-f57c-6e0f0381fe63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch:  1.10.0 ; cuda:  cu111\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n",
            "Collecting mmcv-full==1.4.0\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/mmcv_full-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (58.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 58.0 MB 98 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.4.0) (21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.4.0) (3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.4.0) (7.1.2)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.4.0) (1.21.5)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.4.0) (4.1.2.30)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full==1.4.0) (3.0.7)\n",
            "Installing collected packages: yapf, addict, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.4.0 yapf-0.32.0\n",
            "Cloning into 'mmdetection'...\n",
            "remote: Enumerating objects: 1281, done.\u001b[K\n",
            "remote: Counting objects: 100% (1281/1281), done.\u001b[K\n",
            "remote: Compressing objects: 100% (832/832), done.\u001b[K\n",
            "remote: Total 1281 (delta 474), reused 1235 (delta 447), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1281/1281), 7.46 MiB | 6.60 MiB/s, done.\n",
            "Resolving deltas: 100% (474/474), done.\n",
            "/content/mmdetection\n",
            "Obtaining file:///content/mmdetection\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (1.15.0)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (2.0.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.12.0) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.12.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.12.0) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mmdet==2.12.0) (3.10.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm->mmdet==2.12.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm->mmdet==2.12.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm->mmdet==2.12.0) (7.1.2)\n",
            "Installing collected packages: timm, terminaltables, mmdet\n",
            "  Running setup.py develop for mmdet\n",
            "Successfully installed mmdet-2.12.0 terminaltables-3.1.10 timm-0.5.4\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Found existing installation: pycocotools 2.0.4\n",
            "Uninstalling pycocotools-2.0.4:\n",
            "  Successfully uninstalled pycocotools-2.0.4\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "TORCH_VERSION = torch.__version__.split(\"+\")[0]\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "#we have used torch version 1.10.0 and cuda 11.1 as it is preinstalled in this colab version\n",
        "!pip install mmcv-full==1.4.0 -f https://download.openmmlab.com/mmcv/dist/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
        "# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n",
        "#don't forget to restart the runtime \n",
        "\n",
        "# Install mmdetection\n",
        "!rm -rf mmdetection\n",
        "#!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "!git clone https://github.com/hustvl/QueryInst.git mmdetection\n",
        "%cd mmdetection\n",
        "\n",
        "!pip install -e .\n",
        "\n",
        "!pip install Pillow\n",
        "!pip uninstall pycocotools -y\n",
        "!pip install -q git+https://github.com/waleedka/coco.git#subdirectory=PythonAPI\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oswZEmthIcAf"
      },
      "source": [
        "#### **Note:** Before continuing restart runtime\n",
        "\n",
        "To restart runtime : `Runtime` > `Restart Runtime`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4Uwa47-XuNu"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdDzM_8-qtuL"
      },
      "outputs": [],
      "source": [
        "#%cd /content/\n",
        "\n",
        "#Directories present\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('data/'):\n",
        "        print(dirname)\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"mmdetection\")\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "plt.rcParams[\"axes.grid\"] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pPSEFV4gryz"
      },
      "source": [
        "So, the `data` directory is something like this:\n",
        "\n",
        "<img src=\"https://images.aicrowd.com/uploads/ckeditor/pictures/674/content_carbon__3_.png\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ0LiLXFI9k4"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5QSFOoV8x8H"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "# For reading annotations file\n",
        "import json\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Reading annotations.json\n",
        "TRAIN_ANNOTATIONS_PATH = \"data/train/annotations.json\"\n",
        "TRAIN_IMAGE_DIRECTIORY = \"data/train/images/\"\n",
        "\n",
        "VAL_ANNOTATIONS_PATH = \"data/val/annotations.json\"\n",
        "VAL_IMAGE_DIRECTIORY = \"data/val/images/\"\n",
        "\n",
        "train_coco = COCO(TRAIN_ANNOTATIONS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwzbU8zaKUw3"
      },
      "outputs": [],
      "source": [
        "# Reading the annotation files\n",
        "with open(TRAIN_ANNOTATIONS_PATH) as f:\n",
        "  train_annotations_data = json.load(f)\n",
        "\n",
        "with open(VAL_ANNOTATIONS_PATH) as f:\n",
        "  val_annotations_data = json.load(f)\n",
        "#train_annotations_data['annotations'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REa66aaYKKXE"
      },
      "source": [
        "## Data Format 🔍 \n",
        "\n",
        "Our COCO data format is something like this -\n",
        "\n",
        "```\n",
        "\"info\": {...},\n",
        "\"categories\": [...],\n",
        "\"images\": [...],\n",
        "\"annotations\": [...],\n",
        "```\n",
        "\n",
        "In which categories is like this\n",
        "```\n",
        "[\n",
        "  {'id': 2578,\n",
        "  'name': 'water',\n",
        "  'name_readable': 'Water',\n",
        "  'supercategory': 'food'},\n",
        "  {'id': 1157,\n",
        "  'name': 'pear',\n",
        "  'name_readable': 'Pear',\n",
        "  'supercategory': 'food'},\n",
        "  ...\n",
        "  {'id': 1190,\n",
        "  'name': 'peach',\n",
        "  'name_readable': 'Peach',\n",
        "  'supercategory': 'food'}\n",
        "]\n",
        "```\n",
        "\n",
        "Info is empty ( not sure why )\n",
        "\n",
        "images is like this\n",
        "\n",
        "```\n",
        "[\n",
        "  {'file_name': '065537.jpg', \n",
        "  'height': 464, \n",
        "  'id': 65537, \n",
        "  'width': 464},\n",
        "  {'file_name': '065539.jpg', \n",
        "  'height': 464, \n",
        "  'id': 65539, \n",
        "  'width': 464},\n",
        " ...\n",
        "  {'file_name': '069900.jpg', \n",
        "  'height': 391, \n",
        "  'id': 69900, \n",
        "  'width': 392},\n",
        "]\n",
        "```\n",
        "Annotations is like this\n",
        "\n",
        "```\n",
        "{'area': 44320.0,\n",
        " 'bbox': [86.5, 127.49999999999999, 286.0, 170.0],\n",
        " 'category_id': 2578,\n",
        " 'id': 102434,\n",
        " 'image_id': 65537,\n",
        " 'iscrowd': 0,\n",
        " 'segmentation': [[235.99999999999997,\n",
        "   372.5,\n",
        "   169.0,\n",
        "   372.5,\n",
        "   ...\n",
        "   368.5,\n",
        "   264.0,\n",
        "   371.5]]}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-rglUXYaB8g"
      },
      "source": [
        "## Fixing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTPhgkXkA4OE",
        "outputId": "8734486a-c694-4dcc-e51b-8d5b12cd8be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54392/54392 [05:14<00:00, 172.81it/s]\n",
            "100%|██████████| 946/946 [00:05<00:00, 179.87it/s]\n"
          ]
        }
      ],
      "source": [
        "#fix dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reading annotations.json\n",
        "TRAIN_ANNOTATIONS_PATH = \"data/train/annotations.json\"\n",
        "TRAIN_IMAGE_DIRECTIORY = \"data/train/images/\"\n",
        "\n",
        "VAL_ANNOTATIONS_PATH = \"data/val/annotations.json\"\n",
        "VAL_IMAGE_DIRECTIORY = \"data/val/images/\"\n",
        "\n",
        "# train_coco = COCO(TRAIN_ANNOTATIONS_PATH)\n",
        "\n",
        "# Reading the annotation files\n",
        "with open(TRAIN_ANNOTATIONS_PATH) as f:\n",
        "  train_annotations_data = json.load(f)\n",
        "\n",
        "with open(VAL_ANNOTATIONS_PATH) as f:\n",
        "  val_annotations_data = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "# Function for taking a annotation & directiory of images and returning new annoation json with fixed image size info\n",
        "def fix_data(annotations, directiory, VERBOSE = False):\n",
        "  for n, i in enumerate(tqdm((annotations['images']))):\n",
        "   \n",
        "      img = cv2.imread(directiory+i[\"file_name\"])\n",
        " \n",
        "      if img.shape[0] != i['height']:\n",
        "          annotations['images'][n]['height'] = img.shape[0]\n",
        "          if VERBOSE:\n",
        "            print(i[\"file_name\"])\n",
        "            print(annotations['images'][n], img.shape)\n",
        "\n",
        "      if img.shape[1] != i['width']:\n",
        "          annotations['images'][n]['width'] = img.shape[1]\n",
        "          if VERBOSE:\n",
        "            print(i[\"file_name\"])\n",
        "            print(annotations['images'][n], img.shape)\n",
        "\n",
        "  return annotations\n",
        "\n",
        "train_annotations_data = fix_data(train_annotations_data, TRAIN_IMAGE_DIRECTIORY)\n",
        "\n",
        "with open('data/train/new_ann.json', 'w') as f:\n",
        "    json.dump(train_annotations_data, f)\n",
        "\n",
        "val_annotations_data = fix_data(val_annotations_data, VAL_IMAGE_DIRECTIORY)\n",
        "\n",
        "with open('data/val/new_ann.json', 'w') as f:\n",
        "    json.dump(val_annotations_data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TOQaXMDfWo4"
      },
      "source": [
        "## Setting up hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veNf4K3FXT0_"
      },
      "source": [
        "Modify the model configuration hyperparameters for our training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1ecXHv0Su06"
      },
      "source": [
        "* Load the configuration files and modify them for our dataset.\n",
        "* Set the desired hyperparameters as well\n",
        "* Start training and logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tHtoyJt9gRUm",
        "outputId": "87738f03-dac8-4721-b597-9a1c2efc95b9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mmdetection/configs/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco.py'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can add more model configs like below.\n",
        "MODELS_CONFIG = {\n",
        "    'mask_rcnn_swin-s': {\n",
        "        'config_file': 'configs/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco.py'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Pick the model you want to use\n",
        "selected_model = 'mask_rcnn_swin-s' # chose any config you want from the MODELS_CONFIG\n",
        "\n",
        "# Name of the config file.\n",
        "config_file = MODELS_CONFIG[selected_model]['config_file']\n",
        "\n",
        "config_fname = os.path.join('mmdetection', config_file)\n",
        "assert os.path.isfile(config_fname), '`{}` not exist'.format(config_fname)\n",
        "config_fname"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pEFUeZH1JEp"
      },
      "source": [
        "### Edit config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfUytmAQW9MH"
      },
      "source": [
        "We will edit the config to be suited to the food dataset, there are a lot of parameters other than the ones we have changed below that one can edit in the existing config file that might lead to a better score. We leave that upto you, do feel free to explore documentation for [mmdetection](https://github.com/open-mmlab/mmdetection/tree/master/docs).\n",
        "\n",
        "**Note:** Instead of using regular expressions to edit the existing file, feel free to download the config file and edit it using the text editor of your choice and then reupload the same and have the variable config_fname point to the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8ow9qkmgRFs"
      },
      "outputs": [],
      "source": [
        "# import re\n",
        "# fname = config_fname\n",
        "# with open(fname) as f:\n",
        "#     s = f.read()\n",
        "\n",
        "#     s = re.sub('num_classes=.*?,',\n",
        "#                'num_classes={},'.format(len(classes_names)), s)\n",
        "\n",
        "# with open(fname, 'w') as f:\n",
        "#     f.write(s)\n",
        "# #lets check if the changes have been updated\n",
        "# !cat {config_fname}\n",
        "# #print(len(classes_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mId3_vhbLo_2"
      },
      "outputs": [],
      "source": [
        "# import re\n",
        "# fname2 = 'mmdetection/configs/htc/htc_without_semantic_r50_fpn_1x_coco.py'\n",
        "\n",
        "# with open(fname2) as f:\n",
        "#     s = f.read()\n",
        "\n",
        "#     s = re.sub('num_classes=.*?,',\n",
        "#                'num_classes={},'.format(len(classes_names)), s)\n",
        "# with open(fname2, 'w') as f:\n",
        "#     f.write(s)    \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpZBAJngAVxH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "fname2 = 'mmdetection/configs/_base_/datasets/coco_instance.py'\n",
        "\n",
        "with open(fname2) as f:\n",
        "    s = f.read()\n",
        "    s = re.sub(\"data_root = 'data/coco/'\",\n",
        "                \"data_root = 'data/'\", s)\n",
        "    s = re.sub(\"annotations/instances_train2017.json\",\n",
        "                \"train/new_ann.json\", s)\n",
        "    s = re.sub(\"annotations/instances_val2017.json\",\n",
        "                \"val/new_ann.json\", s)\n",
        "    s = re.sub(\"annotations/instances_val2017.json\",\n",
        "                \"val/new_ann.json\", s)\n",
        "    s = re.sub(\"train2017\", \"train/images\", s)\n",
        "    s = re.sub(\"val2017\", \"val/images\", s)\n",
        "    s = re.sub(\"workers_per_gpu=2\",\"workers_per_gpu=0\",s)\n",
        "    s = re.sub(\"samples_per_gpu=2\",\"samples_per_gpu=4\",s) \n",
        "   \n",
        "\n",
        "with open(fname2, 'w') as f:\n",
        "    f.write(s)\n",
        "\n",
        "#to check if the changes have been updated\n",
        "# !cat {fname2}\n",
        "\n",
        "\n",
        "total_epochs = 22\n",
        "fname = 'mmdetection/configs/_base_/schedules/schedule_1x.py'\n",
        "with open(fname) as f:\n",
        "    s = f.read()\n",
        "    s = re.sub('max_epochs=\\d+',\n",
        "               'max_epochs={}'.format(total_epochs), s)\n",
        "    s = re.sub(\"lr=0.02\",\"lr=0.0001\",s)  #need to change lr to 0.0025 since we are working with only 1 gpu\n",
        "with open(fname, 'w') as f:\n",
        "    f.write(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPmik-f4VE-W"
      },
      "source": [
        "### Just Run this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdCf_A0ML8vT",
        "outputId": "44935e2a-c386-4100-9ded-77d32461957f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting mmdetection/mmdet/datasets/coco.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mmdetection/mmdet/datasets/coco.py\n",
        "\n",
        "#@title Don't forget to run this cell, Modify coco dataset mmdet file (set classes list) { display-mode: \"form\" }\n",
        "# Copyright (c) OpenMMLab. All rights reserved.\n",
        "import contextlib\n",
        "import io\n",
        "import itertools\n",
        "import logging\n",
        "import os.path as osp\n",
        "import tempfile\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "\n",
        "import mmcv\n",
        "import numpy as np\n",
        "from mmcv.utils import print_log\n",
        "from terminaltables import AsciiTable\n",
        "\n",
        "from mmdet.core import eval_recalls\n",
        "from .api_wrappers import COCO, COCOeval\n",
        "from .builder import DATASETS\n",
        "from .custom import CustomDataset\n",
        "\n",
        "\n",
        "@DATASETS.register_module()\n",
        "class CocoDataset(CustomDataset):\n",
        "\n",
        "    CLASSES = ('bread-wholemeal', 'jam', 'water', 'bread-sourdough', 'banana', 'soft-cheese', 'ham-raw', 'hard-cheese', 'cottage-cheese', 'bread-half-white', 'coffee-with-caffeine', 'fruit-salad', 'pancakes', 'tea', 'salmon-smoked', 'avocado', 'spring-onion-scallion', 'ristretto-with-caffeine', 'ham', 'egg', 'bacon-frying', 'chips-french-fries', 'juice-apple', 'chicken', 'tomato-raw', 'broccoli', 'shrimp-boiled', 'beetroot-steamed-without-addition-of-salt', 'carrot-raw', 'chickpeas', 'french-salad-dressing', 'pasta-hornli', 'sauce-cream', 'meat-balls', 'pasta', 'tomato-sauce', 'cheese', 'pear', 'cashew-nut', 'almonds', 'lentils', 'mixed-vegetables', 'peanut-butter', 'apple', 'blueberries', 'cucumber', 'cocoa-powder', 'greek-yaourt-yahourt-yogourt-ou-yoghourt', 'maple-syrup-concentrate', 'buckwheat-grain-peeled', 'butter', 'herbal-tea', 'mayonnaise', 'soup-vegetable', 'wine-red', 'wine-white', 'green-bean-steamed-without-addition-of-salt', 'sausage', 'pizza-margherita-baked', 'salami', 'mushroom', 'bread-meat-substitute-lettuce-sauce', 'tart', 'tea-verveine', 'rice', 'white-coffee-with-caffeine', 'linseeds', 'sunflower-seeds', 'ham-cooked', 'bell-pepper-red-raw', 'zucchini', 'green-asparagus', 'tartar-sauce', 'lye-pretzel-soft', 'cucumber-pickled', 'curry-vegetarian', 'yaourt-yahourt-yogourt-ou-yoghourt-natural', 'soup-of-lentils-dahl-dhal', 'soup-cream-of-vegetables', 'balsamic-vinegar', 'salmon', 'salt-cake-vegetables-filled', 'bacon', 'orange', 'pasta-noodles', 'cream', 'cake-chocolate', 'pasta-spaghetti', 'black-olives', 'parmesan', 'spaetzle', 'salad-lambs-ear', 'salad-leaf-salad-green', 'potatoes-steamed', 'white-cabbage', 'halloumi', 'beetroot-raw', 'bread-grain', 'applesauce-unsweetened-canned', 'cheese-for-raclette', 'mushrooms', 'bread-white', 'curds-natural-with-at-most-10-fidm', 'bagel-without-filling', 'quiche-with-cheese-baked-with-puff-pastry', 'soup-potato', 'bouillon-vegetable', 'beef-sirloin-steak', 'taboule-prepared-with-couscous', 'eggplant', 'bread', 'turnover-with-meat-small-meat-pie-empanadas', 'mungbean-sprouts', 'mozzarella', 'pasta-penne', 'lasagne-vegetable-prepared', 'mandarine', 'kiwi', 'french-beans', 'tartar-meat', 'spring-roll-fried', 'pork-chop', 'caprese-salad-tomato-mozzarella', 'leaf-spinach', 'roll-of-half-white-or-white-flour-with-large-void', 'pasta-ravioli-stuffing', 'omelette-plain', 'tuna', 'dark-chocolate', 'sauce-savoury', 'dried-raisins', 'ice-tea', 'kaki', 'macaroon', 'smoothie', 'crepe-plain', 'chicken-nuggets', 'chili-con-carne-prepared', 'veggie-burger', 'cream-spinach', 'cod', 'chinese-cabbage', 'hamburger-bread-meat-ketchup', 'soup-pumpkin', 'sushi', 'chestnuts', 'coffee-decaffeinated', 'sauce-soya', 'balsamic-salad-dressing', 'pasta-twist', 'bolognaise-sauce', 'leek', 'fajita-bread-only', 'potato-gnocchi', 'beef-cut-into-stripes-only-meat', 'rice-noodles-vermicelli', 'tea-ginger', 'tea-green', 'bread-whole-wheat', 'onion', 'garlic', 'hummus', 'pizza-with-vegetables-baked', 'beer', 'glucose-drink-50g', 'chicken-wing', 'ratatouille', 'peanut', 'high-protein-pasta-made-of-lentils-peas', 'cauliflower', 'quiche-with-spinach-baked-with-cake-dough', 'green-olives', 'brazil-nut', 'eggplant-caviar', 'bread-pita', 'pasta-wholemeal', 'sauce-pesto', 'oil', 'couscous', 'sauce-roast', 'prosecco', 'crackers', 'bread-toast', 'shrimp-prawn-small', 'panna-cotta', 'romanesco', 'water-with-lemon-juice', 'espresso-with-caffeine', 'egg-scrambled-prepared', 'juice-orange', 'ice-cubes', 'braided-white-loaf', 'emmental-cheese', 'croissant-wholegrain', 'hazelnut-chocolate-spread-nutella-ovomaltine-caotina', 'tomme', 'water-mineral', 'hazelnut', 'bacon-raw', 'bread-nut', 'black-forest-tart', 'soup-miso', 'peach', 'figs', 'beef-filet', 'mustard-dijon', 'rice-basmati', 'mashed-potatoes-prepared-with-full-fat-milk-with-butter', 'dumplings', 'pumpkin', 'swiss-chard', 'red-cabbage', 'spinach-raw', 'naan-indien-bread', 'chicken-curry-cream-coconut-milk-curry-spices-paste', 'crunch-muesli', 'biscuits', 'bread-french-white-flour', 'meatloaf', 'fresh-cheese', 'honey', 'vegetable-mix-peas-and-carrots', 'parsley', 'brownie', 'dairy-ice-cream', 'tea-black', 'carrot-cake', 'fish-fingers-breaded', 'salad-dressing', 'dried-meat', 'chicken-breast', 'mixed-salad-chopped-without-sauce', 'feta', 'praline', 'tea-peppermint', 'walnut', 'potato-salad-with-mayonnaise-yogurt-dressing', 'kebab-in-pita-bread', 'kolhrabi', 'alfa-sprouts', 'brussel-sprouts', 'bacon-cooking', 'gruyere', 'bulgur', 'grapes', 'pork-escalope', 'chocolate-egg-small', 'cappuccino', 'zucchini-stewed-without-addition-of-fat-without-addition-of-salt', 'crisp-bread-wasa', 'bread-black', 'perch-fillets-lake', 'rosti', 'mango', 'sandwich-ham-cheese-and-butter', 'muesli', 'spinach-steamed-without-addition-of-salt', 'fish', 'risotto-without-cheese-cooked', 'milk-chocolate-with-hazelnuts', 'cake-oblong', 'crisps', 'pork', 'pomegranate', 'sweet-corn-canned', 'flakes-oat', 'greek-salad', 'cantonese-fried-rice', 'sesame-seeds', 'bouillon', 'baked-potato', 'fennel', 'meat', 'bread-olive', 'croutons', 'philadelphia', 'mushroom-average-stewed-without-addition-of-fat-without-addition-of-salt', 'bell-pepper-red-stewed-without-addition-of-fat-without-addition-of-salt', 'white-chocolate', 'mixed-nuts', 'breadcrumbs-unspiced', 'fondue', 'sauce-mushroom', 'tea-spice', 'strawberries', 'tea-rooibos', 'pie-plum-baked-with-cake-dough', 'potatoes-au-gratin-dauphinois-prepared', 'capers', 'vegetables', 'bread-wholemeal-toast', 'red-radish', 'fruit-tart', 'beans-kidney', 'sauerkraut', 'mustard', 'country-fries', 'ketchup', 'pasta-linguini-parpadelle-tagliatelle', 'chicken-cut-into-stripes-only-meat', 'cookies', 'sun-dried-tomatoe', 'bread-ticino', 'semi-hard-cheese', 'margarine', 'porridge-prepared-with-partially-skimmed-milk', 'soya-drink-soy-milk', 'juice-multifruit', 'popcorn-salted', 'chocolate-filled', 'milk-chocolate', 'bread-fruit', 'mix-of-dried-fruits-and-nuts', 'corn', 'tete-de-moine', 'dates', 'pistachio', 'celery', 'white-radish', 'oat-milk', 'cream-cheese', 'bread-rye', 'witloof-chicory', 'apple-crumble', 'goat-cheese-soft', 'grapefruit-pomelo', 'risotto-with-mushrooms-cooked', 'blue-mould-cheese', 'biscuit-with-butter', 'guacamole', 'pecan-nut', 'tofu', 'cordon-bleu-from-pork-schnitzel-fried', 'paprika-chips', 'quinoa', 'kefir-drink', 'm-m-s', 'salad-rocket', 'bread-spelt', 'pizza-with-ham-with-mushrooms-baked', 'fruit-coulis', 'plums', 'beef-minced-only-meat', 'pizza-with-ham-baked', 'pineapple', 'soup-tomato', 'cheddar', 'tea-fruit', 'rice-jasmin', 'seeds', 'focaccia', 'milk', 'coleslaw-chopped-without-sauce', 'pastry-flaky', 'curd', 'savoury-puff-pastry-stick', 'sweet-potato', 'chicken-leg', 'croissant', 'sour-cream', 'ham-turkey', 'processed-cheese', 'fruit-compotes', 'cheesecake', 'pasta-tortelloni-stuffing', 'sauce-cocktail', 'croissant-with-chocolate-filling', 'pumpkin-seeds', 'artichoke', 'champagne', 'grissini', 'sweets-candies', 'brie', 'wienerli-swiss-sausage', 'syrup-diluted-ready-to-drink', 'apple-pie', 'white-bread-with-butter-eggs-and-milk', 'savoury-puff-pastry', 'anchovies', 'tuna-in-oil-drained', 'lemon-pie', 'meat-terrine-pate', 'coriander', 'falafel-balls', 'berries', 'latte-macchiato-with-caffeine', 'faux-mage-cashew-vegan-chers', 'beans-white', 'sugar-melon', 'mixed-seeds', 'hamburger', 'hamburger-bun', 'oil-vinegar-salad-dressing', 'soya-yaourt-yahourt-yogourt-ou-yoghourt', 'chocolate-milk-chocolate-drink', 'celeriac', 'chocolate-mousse', 'cenovis-yeast-spread', 'thickened-cream-35', 'meringue', 'lamb-chop', 'shrimp-prawn-large', 'beef', 'lemon', 'croque-monsieur', 'chives', 'chocolate-cookies', 'birchermuesli-prepared-no-sugar-added', 'fish-crunchies-battered', 'muffin', 'savoy-cabbage-steamed-without-addition-of-salt', 'pine-nuts', 'chorizo', 'chia-grains', 'frying-sausage', 'french-pizza-from-alsace-baked', 'chocolate', 'cooked-sausage', 'grits-polenta-maize-flour', 'gummi-bears-fruit-jellies-jelly-babies-with-fruit-essence', 'wine-rose', 'coca-cola', 'raspberries', 'roll-with-pieces-of-chocolate', 'goat-average-raw', 'lemon-cake', 'coconut-milk', 'rice-wild', 'gluten-free-bread', 'pearl-onions', 'buckwheat-pancake', 'bread-5-grain', 'light-beer', 'sugar-glazing', 'tzatziki', 'butter-herb', 'ham-croissant', 'corn-crisps', 'lentils-green-du-puy-du-berry', 'cocktail', 'rice-whole-grain', 'veal-sausage', 'cervelat', 'sorbet', 'aperitif-with-alcohol-aperol-spritz', 'dips', 'corn-flakes', 'peas', 'tiramisu', 'apricots', 'cake-marble', 'lamb', 'lasagne-meat-prepared', 'coca-cola-zero', 'cake-salted', 'dough-puff-pastry-shortcrust-bread-pizza-dough', 'rice-waffels', 'sekt', 'brioche', 'vegetable-au-gratin-baked', 'mango-dried', 'processed-meat-charcuterie', 'mousse', 'sauce-sweet-sour', 'basil', 'butter-spread-puree-almond', 'pie-apricot-baked-with-cake-dough', 'rusk-wholemeal', 'beef-roast', 'vanille-cream-cooked-custard-creme-dessert', 'pasta-in-conch-form', 'nuts', 'sauce-carbonara', 'fig-dried', 'pasta-in-butterfly-form-farfalle', 'minced-meat', 'carrot-steamed-without-addition-of-salt', 'ebly', 'damson-plum', 'shoots', 'bouquet-garni', 'coconut', 'banana-cake', 'waffle', 'apricot-dried', 'sauce-curry', 'watermelon-fresh', 'sauce-sweet-salted-asian', 'pork-roast', 'blackberry', 'smoked-cooked-sausage-of-pork-and-beef-meat-sausag', 'bean-seeds', 'italian-salad-dressing', 'white-asparagus', 'pie-rhubarb-baked-with-cake-dough', 'tomato-stewed-without-addition-of-fat-without-addition-of-salt', 'cherries', 'nectarine')\n",
        "\n",
        "    def load_annotations(self, ann_file):\n",
        "        \"\"\"Load annotation from COCO style annotation file.\n",
        "\n",
        "        Args:\n",
        "            ann_file (str): Path of annotation file.\n",
        "\n",
        "        Returns:\n",
        "            list[dict]: Annotation info from COCO api.\n",
        "        \"\"\"\n",
        "\n",
        "        self.coco = COCO('./data/train/new_ann.json')\n",
        "        # The order of returned `cat_ids` will not\n",
        "        # change with the order of the CLASSES\n",
        "        self.cat_ids = self.coco.getCatIds()\n",
        "\n",
        "        self.cat2label = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}\n",
        "        self.img_ids = self.coco.getImgIds()\n",
        "        data_infos = []\n",
        "        total_ann_ids = []\n",
        "        for i in self.img_ids:\n",
        "            info = self.coco.load_imgs([i])[0]\n",
        "            info['filename'] = info['file_name']\n",
        "            data_infos.append(info)\n",
        "            ann_ids = self.coco.get_ann_ids(img_ids=[i])\n",
        "            total_ann_ids.extend(ann_ids)\n",
        "        assert len(set(total_ann_ids)) == len(\n",
        "            total_ann_ids), f\"Annotation ids in '{ann_file}' are not unique!\"\n",
        "        return data_infos\n",
        "\n",
        "    def get_ann_info(self, idx):\n",
        "        \"\"\"Get COCO annotation by index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of data.\n",
        "\n",
        "        Returns:\n",
        "            dict: Annotation info of specified index.\n",
        "        \"\"\"\n",
        "\n",
        "        img_id = self.data_infos[idx]['id']\n",
        "        ann_ids = self.coco.get_ann_ids(img_ids=[img_id])\n",
        "        ann_info = self.coco.load_anns(ann_ids)\n",
        "        return self._parse_ann_info(self.data_infos[idx], ann_info)\n",
        "\n",
        "    def getCatIds(self, idx):\n",
        "        \"\"\"Get COCO category ids by index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of data.\n",
        "\n",
        "        Returns:\n",
        "            list[int]: All categories in the image of specified index.\n",
        "        \"\"\"\n",
        "\n",
        "        img_id = self.data_infos[idx]['id']\n",
        "        ann_ids = self.coco.get_ann_ids(img_ids=[img_id])\n",
        "        ann_info = self.coco.load_anns(ann_ids)\n",
        "        return [ann['category_id'] for ann in ann_info]\n",
        "\n",
        "    def _filter_imgs(self, min_size=32):\n",
        "        \"\"\"Filter images too small or without ground truths.\"\"\"\n",
        "        valid_inds = []\n",
        "        # obtain images that contain annotation\n",
        "        ids_with_ann = set(_['image_id'] for _ in self.coco.anns.values())\n",
        "        # obtain images that contain annotations of the required categories\n",
        "        ids_in_cat = set()\n",
        "        for i, class_id in enumerate(self.cat_ids):\n",
        "            ids_in_cat |= set(self.coco.cat_img_map[class_id])\n",
        "        # merge the image id sets of the two conditions and use the merged set\n",
        "        # to filter out images if self.filter_empty_gt=True\n",
        "        ids_in_cat &= ids_with_ann\n",
        "\n",
        "        valid_img_ids = []\n",
        "        for i, img_info in enumerate(self.data_infos):\n",
        "            img_id = self.img_ids[i]\n",
        "            if self.filter_empty_gt and img_id not in ids_in_cat:\n",
        "                continue\n",
        "            if min(img_info['width'], img_info['height']) >= min_size:\n",
        "                valid_inds.append(i)\n",
        "                valid_img_ids.append(img_id)\n",
        "        self.img_ids = valid_img_ids\n",
        "        return valid_inds\n",
        "\n",
        "    def _parse_ann_info(self, img_info, ann_info):\n",
        "        \"\"\"Parse bbox and mask annotation.\n",
        "\n",
        "        Args:\n",
        "            ann_info (list[dict]): Annotation info of an image.\n",
        "            with_mask (bool): Whether to parse mask annotations.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dict containing the following keys: bboxes, bboxes_ignore,\\\n",
        "                labels, masks, seg_map. \"masks\" are raw annotations and not \\\n",
        "                decoded into binary masks.\n",
        "        \"\"\"\n",
        "        gt_bboxes = []\n",
        "        gt_labels = []\n",
        "        gt_bboxes_ignore = []\n",
        "        gt_masks_ann = []\n",
        "        for i, ann in enumerate(ann_info):\n",
        "            if ann.get('ignore', False):\n",
        "                continue\n",
        "            x1, y1, w, h = ann['bbox']\n",
        "            inter_w = max(0, min(x1 + w, img_info['width']) - max(x1, 0))\n",
        "            inter_h = max(0, min(y1 + h, img_info['height']) - max(y1, 0))\n",
        "            if inter_w * inter_h == 0:\n",
        "                continue\n",
        "            if ann['area'] <= 0 or w < 1 or h < 1:\n",
        "                continue\n",
        "            if ann['category_id'] not in self.cat_ids:\n",
        "                continue\n",
        "            bbox = [x1, y1, x1 + w, y1 + h]\n",
        "            if ann.get('iscrowd', False):\n",
        "                gt_bboxes_ignore.append(bbox)\n",
        "            else:\n",
        "                gt_bboxes.append(bbox)\n",
        "                gt_labels.append(self.cat2label[ann['category_id']])\n",
        "                gt_masks_ann.append(ann.get('segmentation', None))\n",
        "\n",
        "        if gt_bboxes:\n",
        "            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n",
        "            gt_labels = np.array(gt_labels, dtype=np.int64)\n",
        "        else:\n",
        "            gt_bboxes = np.zeros((0, 4), dtype=np.float32)\n",
        "            gt_labels = np.array([], dtype=np.int64)\n",
        "\n",
        "        if gt_bboxes_ignore:\n",
        "            gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\n",
        "        else:\n",
        "            gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n",
        "\n",
        "        seg_map = img_info['filename'].replace('jpg', 'png')\n",
        "\n",
        "        ann = dict(\n",
        "            bboxes=gt_bboxes,\n",
        "            labels=gt_labels,\n",
        "            bboxes_ignore=gt_bboxes_ignore,\n",
        "            masks=gt_masks_ann,\n",
        "            seg_map=seg_map)\n",
        "\n",
        "        return ann\n",
        "\n",
        "    def xyxy2xywh(self, bbox):\n",
        "        \"\"\"Convert ``xyxy`` style bounding boxes to ``xywh`` style for COCO\n",
        "        evaluation.\n",
        "\n",
        "        Args:\n",
        "            bbox (numpy.ndarray): The bounding boxes, shape (4, ), in\n",
        "                ``xyxy`` order.\n",
        "\n",
        "        Returns:\n",
        "            list[float]: The converted bounding boxes, in ``xywh`` order.\n",
        "        \"\"\"\n",
        "\n",
        "        _bbox = bbox.tolist()\n",
        "        return [\n",
        "            _bbox[0],\n",
        "            _bbox[1],\n",
        "            _bbox[2] - _bbox[0],\n",
        "            _bbox[3] - _bbox[1],\n",
        "        ]\n",
        "\n",
        "    def _proposal2json(self, results):\n",
        "        \"\"\"Convert proposal results to COCO json style.\"\"\"\n",
        "        json_results = []\n",
        "        for idx in range(len(self)):\n",
        "            img_id = self.img_ids[idx]\n",
        "            bboxes = results[idx]\n",
        "            for i in range(bboxes.shape[0]):\n",
        "                data = dict()\n",
        "                data['image_id'] = img_id\n",
        "                data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
        "                data['score'] = float(bboxes[i][4])\n",
        "                data['category_id'] = 1\n",
        "                json_results.append(data)\n",
        "        return json_results\n",
        "\n",
        "    def _det2json(self, results):\n",
        "        \"\"\"Convert detection results to COCO json style.\"\"\"\n",
        "        json_results = []\n",
        "        for idx in range(len(self)):\n",
        "            img_id = self.img_ids[idx]\n",
        "            result = results[idx]\n",
        "            for label in range(len(result)):\n",
        "                bboxes = result[label]\n",
        "                for i in range(bboxes.shape[0]):\n",
        "                    data = dict()\n",
        "                    data['image_id'] = img_id\n",
        "                    data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
        "                    data['score'] = float(bboxes[i][4])\n",
        "                    data['category_id'] = self.cat_ids[label]\n",
        "                    json_results.append(data)\n",
        "        return json_results\n",
        "\n",
        "    def _segm2json(self, results):\n",
        "        \"\"\"Convert instance segmentation results to COCO json style.\"\"\"\n",
        "        bbox_json_results = []\n",
        "        segm_json_results = []\n",
        "        for idx in range(len(self)):\n",
        "            img_id = self.img_ids[idx]\n",
        "            det, seg = results[idx]\n",
        "            for label in range(len(det)):\n",
        "                # bbox results\n",
        "                bboxes = det[label]\n",
        "                for i in range(bboxes.shape[0]):\n",
        "                    data = dict()\n",
        "                    data['image_id'] = img_id\n",
        "                    data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
        "                    data['score'] = float(bboxes[i][4])\n",
        "                    data['category_id'] = self.cat_ids[label]\n",
        "                    bbox_json_results.append(data)\n",
        "\n",
        "                # segm results\n",
        "                # some detectors use different scores for bbox and mask\n",
        "                if isinstance(seg, tuple):\n",
        "                    segms = seg[0][label]\n",
        "                    mask_score = seg[1][label]\n",
        "                else:\n",
        "                    segms = seg[label]\n",
        "                    mask_score = [bbox[4] for bbox in bboxes]\n",
        "                for i in range(bboxes.shape[0]):\n",
        "                    data = dict()\n",
        "                    data['image_id'] = img_id\n",
        "                    data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
        "                    data['score'] = float(mask_score[i])\n",
        "                    data['category_id'] = self.cat_ids[label]\n",
        "                    if isinstance(segms[i]['counts'], bytes):\n",
        "                        segms[i]['counts'] = segms[i]['counts'].decode()\n",
        "                    data['segmentation'] = segms[i]\n",
        "                    segm_json_results.append(data)\n",
        "        return bbox_json_results, segm_json_results\n",
        "\n",
        "    def results2json(self, results, outfile_prefix):\n",
        "        \"\"\"Dump the detection results to a COCO style json file.\n",
        "\n",
        "        There are 3 types of results: proposals, bbox predictions, mask\n",
        "        predictions, and they have different data types. This method will\n",
        "        automatically recognize the type, and dump them to json files.\n",
        "\n",
        "        Args:\n",
        "            results (list[list | tuple | ndarray]): Testing results of the\n",
        "                dataset.\n",
        "            outfile_prefix (str): The filename prefix of the json files. If the\n",
        "                prefix is \"somepath/xxx\", the json files will be named\n",
        "                \"somepath/xxx.bbox.json\", \"somepath/xxx.segm.json\",\n",
        "                \"somepath/xxx.proposal.json\".\n",
        "\n",
        "        Returns:\n",
        "            dict[str: str]: Possible keys are \"bbox\", \"segm\", \"proposal\", and \\\n",
        "                values are corresponding filenames.\n",
        "        \"\"\"\n",
        "        result_files = dict()\n",
        "        if isinstance(results[0], list):\n",
        "            json_results = self._det2json(results)\n",
        "            result_files['bbox'] = f'{outfile_prefix}.bbox.json'\n",
        "            result_files['proposal'] = f'{outfile_prefix}.bbox.json'\n",
        "            mmcv.dump(json_results, result_files['bbox'])\n",
        "        elif isinstance(results[0], tuple):\n",
        "            json_results = self._segm2json(results)\n",
        "            result_files['bbox'] = f'{outfile_prefix}.bbox.json'\n",
        "            result_files['proposal'] = f'{outfile_prefix}.bbox.json'\n",
        "            result_files['segm'] = f'{outfile_prefix}.segm.json'\n",
        "            mmcv.dump(json_results[0], result_files['bbox'])\n",
        "            mmcv.dump(json_results[1], result_files['segm'])\n",
        "        elif isinstance(results[0], np.ndarray):\n",
        "            json_results = self._proposal2json(results)\n",
        "            result_files['proposal'] = f'{outfile_prefix}.proposal.json'\n",
        "            mmcv.dump(json_results, result_files['proposal'])\n",
        "        else:\n",
        "            raise TypeError('invalid type of results')\n",
        "        return result_files\n",
        "\n",
        "    def fast_eval_recall(self, results, proposal_nums, iou_thrs, logger=None):\n",
        "        gt_bboxes = []\n",
        "        for i in range(len(self.img_ids)):\n",
        "            ann_ids = self.coco.get_ann_ids(img_ids=self.img_ids[i])\n",
        "            ann_info = self.coco.load_anns(ann_ids)\n",
        "            if len(ann_info) == 0:\n",
        "                gt_bboxes.append(np.zeros((0, 4)))\n",
        "                continue\n",
        "            bboxes = []\n",
        "            for ann in ann_info:\n",
        "                if ann.get('ignore', False) or ann['iscrowd']:\n",
        "                    continue\n",
        "                x1, y1, w, h = ann['bbox']\n",
        "                bboxes.append([x1, y1, x1 + w, y1 + h])\n",
        "            bboxes = np.array(bboxes, dtype=np.float32)\n",
        "            if bboxes.shape[0] == 0:\n",
        "                bboxes = np.zeros((0, 4))\n",
        "            gt_bboxes.append(bboxes)\n",
        "\n",
        "        recalls = eval_recalls(\n",
        "            gt_bboxes, results, proposal_nums, iou_thrs, logger=logger)\n",
        "        ar = recalls.mean(axis=1)\n",
        "        return ar\n",
        "\n",
        "    def format_results(self, results, jsonfile_prefix=None, **kwargs):\n",
        "        \"\"\"Format the results to json (standard format for COCO evaluation).\n",
        "\n",
        "        Args:\n",
        "            results (list[tuple | numpy.ndarray]): Testing results of the\n",
        "                dataset.\n",
        "            jsonfile_prefix (str | None): The prefix of json files. It includes\n",
        "                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n",
        "                If not specified, a temp file will be created. Default: None.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (result_files, tmp_dir), result_files is a dict containing \\\n",
        "                the json filepaths, tmp_dir is the temporal directory created \\\n",
        "                for saving json files when jsonfile_prefix is not specified.\n",
        "        \"\"\"\n",
        "        assert isinstance(results, list), 'results must be a list'\n",
        "        assert len(results) == len(self), (\n",
        "            'The length of results is not equal to the dataset len: {} != {}'.\n",
        "            format(len(results), len(self)))\n",
        "\n",
        "        if jsonfile_prefix is None:\n",
        "            tmp_dir = tempfile.TemporaryDirectory()\n",
        "            jsonfile_prefix = osp.join(tmp_dir.name, 'results')\n",
        "        else:\n",
        "            tmp_dir = None\n",
        "        result_files = self.results2json(results, jsonfile_prefix)\n",
        "        return result_files, tmp_dir\n",
        "\n",
        "    def evaluate(self,\n",
        "                 results,\n",
        "                 metric='bbox',\n",
        "                 logger=None,\n",
        "                 jsonfile_prefix=None,\n",
        "                 classwise=False,\n",
        "                 proposal_nums=(100, 300, 1000),\n",
        "                 iou_thrs=None,\n",
        "                 metric_items=None):\n",
        "        \"\"\"Evaluation in COCO protocol.\n",
        "\n",
        "        Args:\n",
        "            results (list[list | tuple]): Testing results of the dataset.\n",
        "            metric (str | list[str]): Metrics to be evaluated. Options are\n",
        "                'bbox', 'segm', 'proposal', 'proposal_fast'.\n",
        "            logger (logging.Logger | str | None): Logger used for printing\n",
        "                related information during evaluation. Default: None.\n",
        "            jsonfile_prefix (str | None): The prefix of json files. It includes\n",
        "                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n",
        "                If not specified, a temp file will be created. Default: None.\n",
        "            classwise (bool): Whether to evaluating the AP for each class.\n",
        "            proposal_nums (Sequence[int]): Proposal number used for evaluating\n",
        "                recalls, such as recall@100, recall@1000.\n",
        "                Default: (100, 300, 1000).\n",
        "            iou_thrs (Sequence[float], optional): IoU threshold used for\n",
        "                evaluating recalls/mAPs. If set to a list, the average of all\n",
        "                IoUs will also be computed. If not specified, [0.50, 0.55,\n",
        "                0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\n",
        "                Default: None.\n",
        "            metric_items (list[str] | str, optional): Metric items that will\n",
        "                be returned. If not specified, ``['AR@100', 'AR@300',\n",
        "                'AR@1000', 'AR_s@1000', 'AR_m@1000', 'AR_l@1000' ]`` will be\n",
        "                used when ``metric=='proposal'``, ``['mAP', 'mAP_50', 'mAP_75',\n",
        "                'mAP_s', 'mAP_m', 'mAP_l']`` will be used when\n",
        "                ``metric=='bbox' or metric=='segm'``.\n",
        "\n",
        "        Returns:\n",
        "            dict[str, float]: COCO style evaluation metric.\n",
        "        \"\"\"\n",
        "\n",
        "        metrics = metric if isinstance(metric, list) else [metric]\n",
        "        allowed_metrics = ['bbox', 'segm', 'proposal', 'proposal_fast']\n",
        "        for metric in metrics:\n",
        "            if metric not in allowed_metrics:\n",
        "                raise KeyError(f'metric {metric} is not supported')\n",
        "        if iou_thrs is None:\n",
        "            iou_thrs = np.linspace(\n",
        "                .5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n",
        "        if metric_items is not None:\n",
        "            if not isinstance(metric_items, list):\n",
        "                metric_items = [metric_items]\n",
        "\n",
        "        result_files, tmp_dir = self.format_results(results, jsonfile_prefix)\n",
        "\n",
        "        eval_results = OrderedDict()\n",
        "        cocoGt = self.coco\n",
        "        for metric in metrics:\n",
        "            msg = f'Evaluating {metric}...'\n",
        "            if logger is None:\n",
        "                msg = '\\n' + msg\n",
        "            print_log(msg, logger=logger)\n",
        "\n",
        "            if metric == 'proposal_fast':\n",
        "                ar = self.fast_eval_recall(\n",
        "                    results, proposal_nums, iou_thrs, logger='silent')\n",
        "                log_msg = []\n",
        "                for i, num in enumerate(proposal_nums):\n",
        "                    eval_results[f'AR@{num}'] = ar[i]\n",
        "                    log_msg.append(f'\\nAR@{num}\\t{ar[i]:.4f}')\n",
        "                log_msg = ''.join(log_msg)\n",
        "                print_log(log_msg, logger=logger)\n",
        "                continue\n",
        "\n",
        "            iou_type = 'bbox' if metric == 'proposal' else metric\n",
        "            if metric not in result_files:\n",
        "                raise KeyError(f'{metric} is not in results')\n",
        "            try:\n",
        "                predictions = mmcv.load(result_files[metric])\n",
        "                if iou_type == 'segm':\n",
        "                    # Refer to https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py#L331  # noqa\n",
        "                    # When evaluating mask AP, if the results contain bbox,\n",
        "                    # cocoapi will use the box area instead of the mask area\n",
        "                    # for calculating the instance area. Though the overall AP\n",
        "                    # is not affected, this leads to different\n",
        "                    # small/medium/large mask AP results.\n",
        "                    for x in predictions:\n",
        "                        x.pop('bbox')\n",
        "                    warnings.simplefilter('once')\n",
        "                    warnings.warn(\n",
        "                        'The key \"bbox\" is deleted for more accurate mask AP '\n",
        "                        'of small/medium/large instances since v2.12.0. This '\n",
        "                        'does not change the overall mAP calculation.',\n",
        "                        UserWarning)\n",
        "                cocoDt = cocoGt.loadRes(predictions)\n",
        "            except IndexError:\n",
        "                print_log(\n",
        "                    'The testing results of the whole dataset is empty.',\n",
        "                    logger=logger,\n",
        "                    level=logging.ERROR)\n",
        "                break\n",
        "\n",
        "            cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n",
        "            cocoEval.params.catIds = self.cat_ids\n",
        "            cocoEval.params.imgIds = self.img_ids\n",
        "            cocoEval.params.maxDets = list(proposal_nums)\n",
        "            cocoEval.params.iouThrs = iou_thrs\n",
        "            # mapping of cocoEval.stats\n",
        "            coco_metric_names = {\n",
        "                'mAP': 0,\n",
        "                'mAP_50': 1,\n",
        "                'mAP_75': 2,\n",
        "                'mAP_s': 3,\n",
        "                'mAP_m': 4,\n",
        "                'mAP_l': 5,\n",
        "                'AR@100': 6,\n",
        "                'AR@300': 7,\n",
        "                'AR@1000': 8,\n",
        "                'AR_s@1000': 9,\n",
        "                'AR_m@1000': 10,\n",
        "                'AR_l@1000': 11\n",
        "            }\n",
        "            if metric_items is not None:\n",
        "                for metric_item in metric_items:\n",
        "                    if metric_item not in coco_metric_names:\n",
        "                        raise KeyError(\n",
        "                            f'metric item {metric_item} is not supported')\n",
        "\n",
        "            if metric == 'proposal':\n",
        "                cocoEval.params.useCats = 0\n",
        "                cocoEval.evaluate()\n",
        "                cocoEval.accumulate()\n",
        "\n",
        "                # Save coco summarize print information to logger\n",
        "                redirect_string = io.StringIO()\n",
        "                with contextlib.redirect_stdout(redirect_string):\n",
        "                    cocoEval.summarize()\n",
        "                print_log('\\n' + redirect_string.getvalue(), logger=logger)\n",
        "\n",
        "                if metric_items is None:\n",
        "                    metric_items = [\n",
        "                        'AR@100', 'AR@300', 'AR@1000', 'AR_s@1000',\n",
        "                        'AR_m@1000', 'AR_l@1000'\n",
        "                    ]\n",
        "\n",
        "                for item in metric_items:\n",
        "                    val = float(\n",
        "                        f'{cocoEval.stats[coco_metric_names[item]]:.3f}')\n",
        "                    eval_results[item] = val\n",
        "            else:\n",
        "                cocoEval.evaluate()\n",
        "                cocoEval.accumulate()\n",
        "\n",
        "                # Save coco summarize print information to logger\n",
        "                redirect_string = io.StringIO()\n",
        "                with contextlib.redirect_stdout(redirect_string):\n",
        "                    cocoEval.summarize()\n",
        "                print_log('\\n' + redirect_string.getvalue(), logger=logger)\n",
        "\n",
        "                if classwise:  # Compute per-category AP\n",
        "                    # Compute per-category AP\n",
        "                    # from https://github.com/facebookresearch/detectron2/\n",
        "                    precisions = cocoEval.eval['precision']\n",
        "                    # precision: (iou, recall, cls, area range, max dets)\n",
        "                    assert len(self.cat_ids) == precisions.shape[2]\n",
        "\n",
        "                    results_per_category = []\n",
        "                    for idx, catId in enumerate(self.cat_ids):\n",
        "                        # area range index 0: all area ranges\n",
        "                        # max dets index -1: typically 100 per image\n",
        "                        nm = self.coco.loadCats(catId)[0]\n",
        "                        precision = precisions[:, :, idx, 0, -1]\n",
        "                        precision = precision[precision > -1]\n",
        "                        if precision.size:\n",
        "                            ap = np.mean(precision)\n",
        "                        else:\n",
        "                            ap = float('nan')\n",
        "                        results_per_category.append(\n",
        "                            (f'{nm[\"name\"]}', f'{float(ap):0.3f}'))\n",
        "\n",
        "                    num_columns = min(6, len(results_per_category) * 2)\n",
        "                    results_flatten = list(\n",
        "                        itertools.chain(*results_per_category))\n",
        "                    headers = ['category', 'AP'] * (num_columns // 2)\n",
        "                    results_2d = itertools.zip_longest(*[\n",
        "                        results_flatten[i::num_columns]\n",
        "                        for i in range(num_columns)\n",
        "                    ])\n",
        "                    table_data = [headers]\n",
        "                    table_data += [result for result in results_2d]\n",
        "                    table = AsciiTable(table_data)\n",
        "                    print_log('\\n' + table.table, logger=logger)\n",
        "\n",
        "                if metric_items is None:\n",
        "                    metric_items = [\n",
        "                        'mAP', 'mAP_50', 'mAP_75', 'mAP_s', 'mAP_m', 'mAP_l'\n",
        "                    ]\n",
        "\n",
        "                for metric_item in metric_items:\n",
        "                    key = f'{metric}_{metric_item}'\n",
        "                    val = float(\n",
        "                        f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}'\n",
        "                    )\n",
        "                    eval_results[key] = val\n",
        "                ap = cocoEval.stats[:6]\n",
        "                eval_results[f'{metric}_mAP_copypaste'] = (\n",
        "                    f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} '\n",
        "                    f'{ap[4]:.3f} {ap[5]:.3f}')\n",
        "        if tmp_dir is not None:\n",
        "            tmp_dir.cleanup()\n",
        "        return eval_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A8_s1WAv29h"
      },
      "source": [
        "## Resume Experiment or Start a new training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzWq6hbr_TDX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#if you want to continue experiment from your last checkpoint, set the RESUME to True and paste the model path in model_path variable,\n",
        "#don't forget to use the same architecture/parameters in above config\n",
        "\n",
        "RESUME = True\n",
        "if RESUME:\n",
        "  model_path = \"'../input/epoch5/epoch_5.pth'\"\n",
        "  fname = 'mmdetection/configs/_base_/default_runtime.py'\n",
        "  with open(fname) as f:\n",
        "    \n",
        "      s = f.read()\n",
        "      s = re.sub('load_from = None','load_from = {}'.format(model_path), s)\n",
        "      #s = re.sub('load_from = None','resume_from = {}'.format(model_path), s)\n",
        "\n",
        "      s = re.sub(r'CLASSES = \\(.*?\\)',\"EMPTY\",s)\n",
        "\n",
        "  with open(fname, 'w') as f:\n",
        "      f.write(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My_config"
      ],
      "metadata": {
        "id": "iwrJY9SekxTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile query_large.py\n",
        "\n",
        "dataset_type = 'CocoDataset'\n",
        "data_root = 'data/coco/'\n",
        "img_norm_cfg = dict(\n",
        "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
        "CLASSES = ['beetroot-steamed-without-addition-of-salt', 'bread_wholemeal', 'jam', 'water', 'bread', 'banana', 'soft_cheese', 'ham_raw', 'hard_cheese', 'cottage_cheese', 'coffee', 'fruit_mixed', 'pancake', 'tea', 'salmon_smoked', 'avocado', 'spring_onion_scallion', 'ristretto_with_caffeine', 'ham_n_s', 'egg', 'bacon', 'chips_french_fries', 'juice_apple', 'chicken', 'tomato', 'broccoli', 'shrimp_prawn', 'carrot', 'chickpeas', 'french_salad_dressing', 'pasta_hornli_ch', 'sauce_cream', 'pasta_n_s', 'tomato_sauce', 'cheese_n_s', 'pear', 'cashew_nut', 'almonds', 'lentil_n_s', 'mixed_vegetables', 'peanut_butter', 'apple', 'blueberries', 'cucumber', 'yogurt', 'butter', 'mayonnaise', 'soup', 'wine_red', 'wine_white', 'green_bean_steamed_without_addition_of_salt', 'sausage', 'pizza_margherita_baked', 'salami_ch', 'mushroom', 'tart_n_s', 'rice', 'white_coffee', 'sunflower_seeds', 'bell_pepper_red_raw', 'zucchini', 'asparagus', 'tartar_sauce', 'lye_pretzel_soft', 'cucumber_pickled_ch', 'curry_vegetarian', 'soup_of_lentils_dahl_dhal', 'salmon', 'salt_cake_ch_vegetables_filled', 'orange', 'pasta_noodles', 'cream_double_cream_heavy_cream_45', 'cake_chocolate', 'pasta_spaghetti', 'black_olives', 'parmesan', 'spaetzle', 'salad_lambs_ear', 'salad_leaf_salad_green', 'potato', 'white_cabbage', 'halloumi', 'beetroot_raw', 'bread_grain', 'applesauce', 'cheese_for_raclette_ch', 'bread_white', 'curds_natural', 'quiche', 'beef_n_s', 'taboule_prepared_with_couscous', 'aubergine_eggplant', 'mozzarella', 'pasta_penne', 'lasagne_vegetable_prepared', 'mandarine', 'kiwi', 'french_beans', 'spring_roll_fried', 'caprese_salad_tomato_mozzarella', 'leaf_spinach', 'roll_of_half_white_or_white_flour_with_large_void', 'omelette_with_flour_thick_crepe_plain', 'tuna', 'dark_chocolate', 'sauce_savoury_n_s', 'raisins_dried', 'ice_tea_on_black_tea_basis', 'kaki', 'smoothie', 'crepe_with_flour_plain', 'nuggets', 'chili_con_carne_prepared', 'veggie_burger', 'chinese_cabbage', 'hamburger', 'soup_pumpkin', 'sushi', 'chestnuts_ch', 'sauce_soya', 'balsamic_salad_dressing', 'pasta_twist', 'bolognaise_sauce', 'leek', 'fajita_bread_only', 'potato_gnocchi', 'rice_noodles_vermicelli', 'bread_whole_wheat', 'onion', 'garlic', 'hummus', 'pizza_with_vegetables_baked', 'beer', 'glucose_drink_50g', 'ratatouille', 'peanut', 'cauliflower', 'green_olives', 'bread_pita', 'pasta_wholemeal', 'sauce_pesto', 'couscous', 'sauce', 'bread_toast', 'water_with_lemon_juice', 'espresso', 'egg_scrambled', 'juice_orange', 'braided_white_loaf_ch', 'emmental_cheese_ch', 'hazelnut_chocolate_spread_nutella_ovomaltine_caotina', 'tomme_ch', 'hazelnut', 'peach', 'figs', 'mashed_potatoes_prepared_with_full_fat_milk_with_butter', 'pumpkin', 'swiss_chard', 'red_cabbage_raw', 'spinach_raw', 'chicken_curry_cream_coconut_milk_curry_spices_paste', 'crunch_muesli', 'biscuit', 'meatloaf_ch', 'fresh_cheese_n_s', 'honey', 'vegetable_mix_peas_and_carrots', 'parsley', 'brownie', 'ice_cream_n_s', 'salad_dressing', 'dried_meat_n_s', 'chicken_breast', 'mixed_salad_chopped_without_sauce', 'feta', 'praline_n_s', 'walnut', 'potato_salad', 'kolhrabi', 'alfa_sprouts', 'brussel_sprouts', 'gruyere_ch', 'bulgur', 'grapes', 'chocolate_egg_small', 'cappuccino', 'crisp_bread', 'bread_black', 'rosti_n_s', 'mango', 'muesli_dry', 'spinach', 'fish_n_s', 'risotto', 'crisps_ch', 'pork_n_s', 'pomegranate', 'sweet_corn', 'flakes', 'greek_salad', 'sesame_seeds', 'bouillon', 'baked_potato', 'fennel', 'meat_n_s', 'croutons', 'bell_pepper_red_stewed', 'nuts', 'breadcrumbs_unspiced', 'fondue', 'sauce_mushroom', 'strawberries', 'pie_plum_baked_with_cake_dough', 'potatoes_au_gratin_dauphinois_prepared', 'capers', 'bread_wholemeal_toast', 'red_radish', 'fruit_tart', 'beans_kidney', 'sauerkraut', 'mustard', 'country_fries', 'ketchup', 'pasta_linguini_parpadelle_tagliatelle', 'chicken_cut_into_stripes_only_meat', 'cookies', 'sun_dried_tomatoe', 'bread_ticino_ch', 'semi_hard_cheese', 'porridge_prepared_with_partially_skimmed_milk', 'juice', 'chocolate_milk', 'bread_fruit', 'corn', 'dates', 'pistachio', 'cream_cheese_n_s', 'bread_rye', 'witloof_chicory', 'goat_cheese_soft', 'grapefruit_pomelo', 'blue_mould_cheese', 'guacamole', 'tofu', 'cordon_bleu', 'quinoa', 'kefir_drink', 'salad_rocket', 'pizza_with_ham_with_mushrooms_baked', 'fruit_coulis', 'plums', 'pizza_with_ham_baked', 'pineapple', 'seeds_n_s', 'focaccia', 'mixed_milk_beverage', 'coleslaw_chopped_without_sauce', 'sweet_potato', 'chicken_leg', 'croissant', 'cheesecake', 'sauce_cocktail', 'croissant_with_chocolate_filling', 'pumpkin_seeds', 'artichoke', 'soft_drink_with_a_taste', 'apple_pie', 'white_bread_with_butter_eggs_and_milk', 'savoury_pastry_stick', 'tuna_in_oil_drained', 'meat_terrine_pate', 'falafel_balls', 'berries_n_s', 'latte_macchiato', 'sugar_melon_galia_honeydew_cantaloupe', 'mixed_seeds_n_s', 'oil_vinegar_salad_dressing', 'celeriac', 'chocolate_mousse', 'lemon', 'chocolate_cookies', 'birchermuesli_prepared_no_sugar_added', 'muffin', 'pine_nuts', 'french_pizza_from_alsace_baked', 'chocolate_n_s', 'grits_polenta_maize_flour', 'wine_rose', 'cola_based_drink', 'raspberries', 'roll_with_pieces_of_chocolate', 'cake_lemon', 'rice_wild', 'gluten_free_bread', 'pearl_onion', 'tzatziki', 'ham_croissant_ch', 'corn_crisps', 'lentils_green_du_puy_du_berry', 'rice_whole_grain', 'cervelat_ch', 'aperitif_with_alcohol_n_s_aperol_spritz', 'peas', 'tiramisu', 'apricots', 'lasagne_meat_prepared', 'brioche', 'vegetable_au_gratin_baked', 'basil', 'butter_spread_puree_almond', 'pie_apricot', 'rusk_wholemeal', 'pasta_in_conch_form', 'pasta_in_butterfly_form_farfalle', 'damson_plum', 'shoots_n_s', 'coconut', 'banana_cake', 'sauce_curry', 'watermelon_fresh', 'white_asparagus', 'cherries', 'nectarine']\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=1,\n",
        "    workers_per_gpu=2,\n",
        "    train=dict(\n",
        "        type='CocoDataset',\n",
        "        ann_file='data/train/new_ann.json',\n",
        "        img_prefix='data/train/images/',\n",
        "\n",
        "        pipeline=[\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
        "            dict(type='RandomFlip', flip_ratio=0.5),\n",
        "            dict(\n",
        "                type='AutoAugment',\n",
        "                policies=[[{\n",
        "                    'type': 'Resize',\n",
        "                    'img_scale': [(400, 1333), (1200, 1333)],\n",
        "                    'multiscale_mode': 'range',\n",
        "                    'keep_ratio': True\n",
        "                }],\n",
        "                          [{\n",
        "                              'type': 'Resize',\n",
        "                              'img_scale': [(400, 1333), (500, 1333),\n",
        "                                            (600, 1333)],\n",
        "                              'multiscale_mode': 'value',\n",
        "                              'keep_ratio': True\n",
        "                          }, {\n",
        "                              'type': 'RandomCrop',\n",
        "                              'crop_type': 'absolute_range',\n",
        "                              'crop_size': (384, 600),\n",
        "                              'allow_negative_crop': True\n",
        "                          }, {\n",
        "                              'type': 'Resize',\n",
        "                              'img_scale': [(400, 1333), (1200, 1333)],\n",
        "                              'multiscale_mode': 'range',\n",
        "                              'override': True,\n",
        "                              'keep_ratio': True\n",
        "                          }]]),\n",
        "            dict(\n",
        "                type='Normalize',\n",
        "                mean=[123.675, 116.28, 103.53],\n",
        "                std=[58.395, 57.12, 57.375],\n",
        "                to_rgb=True),\n",
        "            dict(type='Pad', size_divisor=32),\n",
        "            dict(type='DefaultFormatBundle'),\n",
        "            dict(\n",
        "                type='Collect',\n",
        "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
        "        ],classes=CLASSES),\n",
        "    val=dict(\n",
        "        type='CocoDataset',\n",
        "        ann_file='data/val/new_ann.json',\n",
        "        img_prefix='data/val/images/',\n",
        "        pipeline=[\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(\n",
        "                type='MultiScaleFlipAug',\n",
        "                img_scale=(1333, 800),\n",
        "                flip=False,\n",
        "                transforms=[\n",
        "                    dict(type='Resize', keep_ratio=True),\n",
        "                    dict(type='RandomFlip'),\n",
        "                    dict(\n",
        "                        type='Normalize',\n",
        "                        mean=[123.675, 116.28, 103.53],\n",
        "                        std=[58.395, 57.12, 57.375],\n",
        "                        to_rgb=True),\n",
        "                    dict(type='Pad', size_divisor=32),\n",
        "                    dict(type='ImageToTensor', keys=['img']),\n",
        "                    dict(type='Collect', keys=['img'])\n",
        "                ])\n",
        "        ],classes=CLASSES),\n",
        "    test=dict(\n",
        "        type='CocoDataset',\n",
        "        ann_file='data/val/new_ann.json',\n",
        "        img_prefix='data/val/images/',\n",
        "        pipeline=[\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(\n",
        "                type='MultiScaleFlipAug',\n",
        "                img_scale=(1333, 800),\n",
        "                flip=False,\n",
        "                transforms=[\n",
        "                    dict(type='Resize', keep_ratio=True),\n",
        "                    dict(type='RandomFlip'),\n",
        "                    dict(\n",
        "                        type='Normalize',\n",
        "                        mean=[123.675, 116.28, 103.53],\n",
        "                        std=[58.395, 57.12, 57.375],\n",
        "                        to_rgb=True),\n",
        "                    dict(type='Pad', size_divisor=32),\n",
        "                    dict(type='ImageToTensor', keys=['img']),\n",
        "                    dict(type='Collect', keys=['img'])\n",
        "                ])\n",
        "        ]))\n",
        "evaluation = dict(metric=['segm'])\n",
        "optimizer = dict(\n",
        "    type='AdamW',\n",
        "    lr=2.5e-05,\n",
        "    weight_decay=0.0001,\n",
        "    paramwise_cfg=dict(\n",
        "        custom_keys=dict(\n",
        "            absolute_pos_embed=dict(decay_mult=0.0),\n",
        "            relative_position_bias_table=dict(decay_mult=0.0),\n",
        "            norm=dict(decay_mult=0.0))))\n",
        "# optimizer_config = dict(\n",
        "#     grad_clip=dict(max_norm=1, norm_type=2),\n",
        "#     type='DistOptimizerHook',\n",
        "#     update_interval=1,\n",
        "#     coalesce=True,\n",
        "#     bucket_size_mb=-1,\n",
        "#     use_fp16=False)\n",
        "lr_config = dict(\n",
        "    policy='step',\n",
        "    warmup='linear',\n",
        "    warmup_iters=1000,\n",
        "    warmup_ratio=0.001,\n",
        "    step=[12])\n",
        "runner = dict(type='EpochBasedRunner', max_epochs=22)\n",
        "checkpoint_config = dict(interval=1)\n",
        "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
        "custom_hooks = [dict(type='NumClassCheckHook')]\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "load_from = '/content/drive/MyDrive/queryinst_swin_large_patch4_window7_fpn_300_queries-832c5813.pth' #None\n",
        "resume_from = None\n",
        "workflow = [('train', 1)]\n",
        "num_stages = 6\n",
        "num_proposals = 300\n",
        "model = dict(\n",
        "    type='QueryInst',\n",
        "    pretrained=None,\n",
        "    backbone=dict(\n",
        "        type='SwinTransformer',\n",
        "        embed_dim=192,\n",
        "        depths=[2, 2, 18, 2],\n",
        "        num_heads=[6, 12, 24, 48],\n",
        "        window_size=7,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.3,\n",
        "        ape=False,\n",
        "        patch_norm=True,\n",
        "        out_indices=(0, 1, 2, 3),\n",
        "        use_checkpoint=False),\n",
        "    neck=dict(\n",
        "        type='FPN',\n",
        "        in_channels=[192, 384, 768, 1536],\n",
        "        out_channels=256,\n",
        "        start_level=0,\n",
        "        add_extra_convs='on_input',\n",
        "        num_outs=4),\n",
        "    rpn_head=dict(\n",
        "        type='EmbeddingRPNHead',\n",
        "        num_proposals=300,\n",
        "        proposal_feature_channel=256),\n",
        "    roi_head=dict(\n",
        "        type='QueryRoIHead',\n",
        "        num_stages=6,\n",
        "        stage_loss_weights=[1, 1, 1, 1, 1, 1],\n",
        "        proposal_feature_channel=256,\n",
        "        bbox_roi_extractor=dict(\n",
        "            type='SingleRoIExtractor',\n",
        "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=2),\n",
        "            out_channels=256,\n",
        "            featmap_strides=[4, 8, 16, 32]),\n",
        "        mask_roi_extractor=dict(\n",
        "            type='SingleRoIExtractor',\n",
        "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=2),\n",
        "            out_channels=256,\n",
        "            featmap_strides=[4, 8, 16, 32]),\n",
        "        bbox_head=[\n",
        "            dict(\n",
        "                type='DIIHead',\n",
        "                num_classes=323,\n",
        "                num_ffn_fcs=2,\n",
        "                num_heads=8,\n",
        "                num_cls_fcs=1,\n",
        "                num_reg_fcs=3,\n",
        "                feedforward_channels=2048,\n",
        "                in_channels=256,\n",
        "                dropout=0.0,\n",
        "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=7,\n",
        "                    with_proj=True,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
        "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
        "                loss_cls=dict(\n",
        "                    type='FocalLoss',\n",
        "                    use_sigmoid=True,\n",
        "                    gamma=2.0,\n",
        "                    alpha=0.25,\n",
        "                    loss_weight=2.0),\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    clip_border=False,\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
        "            dict(\n",
        "                type='DIIHead',\n",
        "                num_classes=323,\n",
        "                num_ffn_fcs=2,\n",
        "                num_heads=8,\n",
        "                num_cls_fcs=1,\n",
        "                num_reg_fcs=3,\n",
        "                feedforward_channels=2048,\n",
        "                in_channels=256,\n",
        "                dropout=0.0,\n",
        "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=7,\n",
        "                    with_proj=True,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
        "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
        "                loss_cls=dict(\n",
        "                    type='FocalLoss',\n",
        "                    use_sigmoid=True,\n",
        "                    gamma=2.0,\n",
        "                    alpha=0.25,\n",
        "                    loss_weight=2.0),\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    clip_border=False,\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
        "            dict(\n",
        "                type='DIIHead',\n",
        "                num_classes=323,\n",
        "                num_ffn_fcs=2,\n",
        "                num_heads=8,\n",
        "                num_cls_fcs=1,\n",
        "                num_reg_fcs=3,\n",
        "                feedforward_channels=2048,\n",
        "                in_channels=256,\n",
        "                dropout=0.0,\n",
        "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=7,\n",
        "                    with_proj=True,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
        "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
        "                loss_cls=dict(\n",
        "                    type='FocalLoss',\n",
        "                    use_sigmoid=True,\n",
        "                    gamma=2.0,\n",
        "                    alpha=0.25,\n",
        "                    loss_weight=2.0),\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    clip_border=False,\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
        "            dict(\n",
        "                type='DIIHead',\n",
        "                num_classes=323,\n",
        "                num_ffn_fcs=2,\n",
        "                num_heads=8,\n",
        "                num_cls_fcs=1,\n",
        "                num_reg_fcs=3,\n",
        "                feedforward_channels=2048,\n",
        "                in_channels=256,\n",
        "                dropout=0.0,\n",
        "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=7,\n",
        "                    with_proj=True,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
        "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
        "                loss_cls=dict(\n",
        "                    type='FocalLoss',\n",
        "                    use_sigmoid=True,\n",
        "                    gamma=2.0,\n",
        "                    alpha=0.25,\n",
        "                    loss_weight=2.0),\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    clip_border=False,\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
        "            dict(\n",
        "                type='DIIHead',\n",
        "                num_classes=323,\n",
        "                num_ffn_fcs=2,\n",
        "                num_heads=8,\n",
        "                num_cls_fcs=1,\n",
        "                num_reg_fcs=3,\n",
        "                feedforward_channels=2048,\n",
        "                in_channels=256,\n",
        "                dropout=0.0,\n",
        "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=7,\n",
        "                    with_proj=True,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
        "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
        "                loss_cls=dict(\n",
        "                    type='FocalLoss',\n",
        "                    use_sigmoid=True,\n",
        "                    gamma=2.0,\n",
        "                    alpha=0.25,\n",
        "                    loss_weight=2.0),\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    clip_border=False,\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
        "            dict(\n",
        "                type='DIIHead',\n",
        "                num_classes=323,\n",
        "                num_ffn_fcs=2,\n",
        "                num_heads=8,\n",
        "                num_cls_fcs=1,\n",
        "                num_reg_fcs=3,\n",
        "                feedforward_channels=2048,\n",
        "                in_channels=256,\n",
        "                dropout=0.0,\n",
        "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=7,\n",
        "                    with_proj=True,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
        "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
        "                loss_cls=dict(\n",
        "                    type='FocalLoss',\n",
        "                    use_sigmoid=True,\n",
        "                    gamma=2.0,\n",
        "                    alpha=0.25,\n",
        "                    loss_weight=2.0),\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    clip_border=False,\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.5, 0.5, 1.0, 1.0]))\n",
        "        ],\n",
        "        mask_head=[\n",
        "            dict(\n",
        "                type='DynamicMaskHead',\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=14,\n",
        "                    with_proj=False,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "               \n",
        "                dropout=0.0,\n",
        "                num_convs=4,\n",
        "                 num_classes=323,\n",
        "                roi_feat_size=14,\n",
        "                in_channels=256,\n",
        "                conv_kernel_size=3,\n",
        "                conv_out_channels=256,\n",
        "                class_agnostic=False,\n",
        "                norm_cfg=dict(type='BN'),\n",
        "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
        "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
        "            dict(\n",
        "                type='DynamicMaskHead',\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=14,\n",
        "                    with_proj=False,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                \n",
        "                dropout=0.0,\n",
        "                num_convs=4,\n",
        "                 num_classes=323,\n",
        "                roi_feat_size=14,\n",
        "                in_channels=256,\n",
        "                conv_kernel_size=3,\n",
        "                conv_out_channels=256,\n",
        "                class_agnostic=False,\n",
        "                norm_cfg=dict(type='BN'),\n",
        "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
        "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
        "            dict(\n",
        "                type='DynamicMaskHead',\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=14,\n",
        "                    with_proj=False,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                \n",
        "                dropout=0.0,\n",
        "                num_convs=4,\n",
        "                 num_classes=323,\n",
        "                roi_feat_size=14,\n",
        "                in_channels=256,\n",
        "                conv_kernel_size=3,\n",
        "                conv_out_channels=256,\n",
        "                class_agnostic=False,\n",
        "                norm_cfg=dict(type='BN'),\n",
        "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
        "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
        "            dict(\n",
        "                type='DynamicMaskHead',\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=14,\n",
        "                    with_proj=False,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "               \n",
        "                dropout=0.0,\n",
        "                num_convs=4,\n",
        "                 num_classes=323,\n",
        "                roi_feat_size=14,\n",
        "                in_channels=256,\n",
        "                conv_kernel_size=3,\n",
        "                conv_out_channels=256,\n",
        "                class_agnostic=False,\n",
        "                norm_cfg=dict(type='BN'),\n",
        "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
        "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
        "            dict(\n",
        "                type='DynamicMaskHead',\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=14,\n",
        "                    with_proj=False,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "               \n",
        "                dropout=0.0,\n",
        "                num_convs=4,\n",
        "                 num_classes=323,\n",
        "                roi_feat_size=14,\n",
        "                in_channels=256,\n",
        "                conv_kernel_size=3,\n",
        "                conv_out_channels=256,\n",
        "                class_agnostic=False,\n",
        "                norm_cfg=dict(type='BN'),\n",
        "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
        "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
        "            dict(\n",
        "                type='DynamicMaskHead',\n",
        "                dynamic_conv_cfg=dict(\n",
        "                    type='DynamicConv',\n",
        "                    in_channels=256,\n",
        "                    feat_channels=64,\n",
        "                    out_channels=256,\n",
        "                    input_feat_shape=14,\n",
        "                    with_proj=False,\n",
        "                    act_cfg=dict(type='ReLU', inplace=True),\n",
        "                    norm_cfg=dict(type='LN')),\n",
        "                dropout=0.0,\n",
        "                num_convs=4,\n",
        "                 num_classes=323,\n",
        "                roi_feat_size=14,\n",
        "                in_channels=256,\n",
        "                conv_kernel_size=3,\n",
        "                conv_out_channels=256,\n",
        "                class_agnostic=False,\n",
        "                norm_cfg=dict(type='BN'),\n",
        "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
        "                loss_dice=dict(type='DiceLoss', loss_weight=8.0))\n",
        "        ]),\n",
        "    train_cfg=dict(\n",
        "        rpn=None,\n",
        "        rcnn=[\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='HungarianAssigner',\n",
        "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
        "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
        "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
        "                                  weight=2.0)),\n",
        "                sampler=dict(type='PseudoSampler'),\n",
        "                pos_weight=1,\n",
        "                mask_size=28,\n",
        "                debug=False),\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='HungarianAssigner',\n",
        "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
        "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
        "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
        "                                  weight=2.0)),\n",
        "                sampler=dict(type='PseudoSampler'),\n",
        "                pos_weight=1,\n",
        "                mask_size=28,\n",
        "                debug=False),\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='HungarianAssigner',\n",
        "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
        "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
        "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
        "                                  weight=2.0)),\n",
        "                sampler=dict(type='PseudoSampler'),\n",
        "                pos_weight=1,\n",
        "                mask_size=28,\n",
        "                debug=False),\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='HungarianAssigner',\n",
        "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
        "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
        "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
        "                                  weight=2.0)),\n",
        "                sampler=dict(type='PseudoSampler'),\n",
        "                pos_weight=1,\n",
        "                mask_size=28,\n",
        "                debug=False),\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='HungarianAssigner',\n",
        "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
        "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
        "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
        "                                  weight=2.0)),\n",
        "                sampler=dict(type='PseudoSampler'),\n",
        "                pos_weight=1,\n",
        "                mask_size=28,\n",
        "                debug=False),\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='HungarianAssigner',\n",
        "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
        "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
        "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
        "                                  weight=2.0)),\n",
        "                sampler=dict(type='PseudoSampler'),\n",
        "                pos_weight=1,\n",
        "                mask_size=28,\n",
        "                debug=False)\n",
        "        ]),\n",
        "    test_cfg=dict(rpn=None, rcnn=dict(max_per_img=300, mask_thr_binary=0.5)))\n",
        "total_epochs = 22\n",
        "min_values = (480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800)\n",
        "optimizer_config = dict(grad_clip=dict(max_norm=15, norm_type=2))\n",
        "fp16 = dict(loss_scale=512.0)\n",
        "work_dir = '/content/drive/MyDrive/log_mmdetQuery'\n",
        "gpu_ids = range(0, 1)"
      ],
      "metadata": {
        "id": "XUVi9rrUk0OL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d9758b-1d16-4a56-cd03-2d35350a9d3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting query_large.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Myconfig base"
      ],
      "metadata": {
        "id": "79sTIvs15Dn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile query_large.py\n",
        "\n",
        "model = dict(\n",
        "    type='HybridTaskCascade',\n",
        "    pretrained=None,\n",
        "    backbone=dict(\n",
        "        type='CBSwinTransformer',\n",
        "        embed_dim=128,\n",
        "        depths=[2, 2, 18, 2],\n",
        "        num_heads=[4, 8, 16, 32],\n",
        "        window_size=7,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.3,\n",
        "        ape=False,\n",
        "        patch_norm=True,\n",
        "        out_indices=(0, 1, 2, 3),\n",
        "        use_checkpoint=False),\n",
        "    neck=dict(\n",
        "        type='CBFPN',\n",
        "        in_channels=[128, 256, 512, 1024],\n",
        "        out_channels=256,\n",
        "        num_outs=5),\n",
        "        rpn_head=dict(\n",
        "        type='RPNHead',\n",
        "        in_channels=256,\n",
        "        feat_channels=256,\n",
        "        anchor_generator=dict(\n",
        "            type='AnchorGenerator',\n",
        "            scales=[8],\n",
        "            ratios=[0.5, 1.0, 2.0],\n",
        "            strides=[4, 8, 16, 32, 64]),\n",
        "        bbox_coder=dict(\n",
        "            type='DeltaXYWHBBoxCoder',\n",
        "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
        "        loss_cls=dict(\n",
        "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
        "        loss_bbox=dict(\n",
        "            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),\n",
        "    roi_head=dict(\n",
        "        type='HybridTaskCascadeRoIHead',\n",
        "        interleaved=True,\n",
        "        mask_info_flow=True,\n",
        "        num_stages=3,\n",
        "        stage_loss_weights=[1, 0.5, 0.25],\n",
        "        bbox_roi_extractor=dict(\n",
        "            type='SingleRoIExtractor',\n",
        "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
        "            out_channels=256,\n",
        "            featmap_strides=[4, 8, 16, 32]),\n",
        "        bbox_head=[\n",
        "            dict(\n",
        "                type='ConvFCBBoxHead',\n",
        "                num_shared_convs=4,\n",
        "                num_shared_fcs=1,\n",
        "                in_channels=256,\n",
        "                conv_out_channels=256,\n",
        "                fc_out_channels=1024,\n",
        "                roi_feat_size=7,\n",
        "                num_classes=323,\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
        "                reg_class_agnostic=True,\n",
        "                reg_decoded_bbox=True,\n",
        "                norm_cfg=dict(type='BN', requires_grad=True),\n",
        "                loss_cls=dict(\n",
        "                    type='SeesawLoss',\n",
        "                    p=0.8,\n",
        "                    q=2.0,\n",
        "                    num_classes=323,\n",
        "                    loss_weight=1.0),\n",
        "                loss_bbox=dict(type='GIoULoss', loss_weight=10.0)),\n",
        "            dict(\n",
        "                type='ConvFCBBoxHead',\n",
        "                num_shared_convs=4,\n",
        "                num_shared_fcs=1,\n",
        "                in_channels=256,\n",
        "                conv_out_channels=256,\n",
        "                fc_out_channels=1024,\n",
        "                roi_feat_size=7,\n",
        "                num_classes=323,\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n",
        "                reg_class_agnostic=True,\n",
        "                reg_decoded_bbox=True,\n",
        "                norm_cfg=dict(type='BN', requires_grad=True),\n",
        "                loss_cls=dict(\n",
        "                    type='SeesawLoss',\n",
        "                    p=0.8,\n",
        "                    q=2.0,\n",
        "                    num_classes=323,\n",
        "                    loss_weight=1.0),\n",
        "                loss_bbox=dict(type='GIoULoss', loss_weight=10.0)),\n",
        "            dict(\n",
        "                type='ConvFCBBoxHead',\n",
        "                num_shared_convs=4,\n",
        "                num_shared_fcs=1,\n",
        "                in_channels=256,\n",
        "                conv_out_channels=256,\n",
        "                fc_out_channels=1024,\n",
        "                roi_feat_size=7,\n",
        "                num_classes=323,\n",
        "                bbox_coder=dict(\n",
        "                    type='DeltaXYWHBBoxCoder',\n",
        "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
        "                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n",
        "                reg_class_agnostic=True,\n",
        "                reg_decoded_bbox=True,\n",
        "                norm_cfg=dict(type='BN', requires_grad=True),\n",
        "                loss_cls=dict(\n",
        "                    type='SeesawLoss',\n",
        "                    p=0.8,\n",
        "                    q=2.0,\n",
        "                    num_classes=323,\n",
        "                    loss_weight=1.0),\n",
        "\n",
        "                loss_bbox=dict(type='GIoULoss', loss_weight=10.0))\n",
        "        ],\n",
        "        mask_roi_extractor=dict(\n",
        "            type='SingleRoIExtractor',\n",
        "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
        "            out_channels=256,\n",
        "            featmap_strides=[4, 8, 16, 32]),\n",
        "        mask_head=[\n",
        "            dict(\n",
        "                type='HTCMaskHead',\n",
        "                with_conv_res=False,\n",
        "                num_convs=4,\n",
        "                in_channels=256,\n",
        "                conv_out_channels=256,\n",
        "                num_classes=323,\n",
        "                loss_mask=dict(\n",
        "                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n",
        "            dict(\n",
        "                type='HTCMaskHead',\n",
        "                num_convs=4,\n",
        "                in_channels=256,\n",
        "                conv_out_channels=256,\n",
        "                num_classes=323,\n",
        "                loss_mask=dict(\n",
        "                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n",
        "            dict(\n",
        "                type='HTCMaskHead',\n",
        "                num_convs=4,\n",
        "                in_channels=256,\n",
        "                conv_out_channels=256,\n",
        "                num_classes=323,\n",
        "                loss_mask=dict(\n",
        "                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))\n",
        "        ]),\n",
        "    train_cfg=dict(\n",
        "        rpn=dict(\n",
        "            assigner=dict(\n",
        "                type='MaxIoUAssigner',\n",
        "                pos_iou_thr=0.7,\n",
        "                neg_iou_thr=0.3,\n",
        "                min_pos_iou=0.3,\n",
        "                ignore_iof_thr=-1),\n",
        "            sampler=dict(\n",
        "                type='RandomSampler',\n",
        "                num=256,\n",
        "                pos_fraction=0.5,\n",
        "                neg_pos_ub=-1,\n",
        "                add_gt_as_proposals=False),\n",
        "            allowed_border=0,\n",
        "            pos_weight=-1,\n",
        "            debug=False),\n",
        "        rpn_proposal=dict(\n",
        "            nms_pre=2000,\n",
        "            max_per_img=2000,\n",
        "            nms=dict(type='nms', iou_threshold=0.7),\n",
        "            min_bbox_size=0),\n",
        "        rcnn=[\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='MaxIoUAssigner',\n",
        "                    pos_iou_thr=0.5,\n",
        "                    neg_iou_thr=0.5,\n",
        "                    min_pos_iou=0.5,\n",
        "                    ignore_iof_thr=-1),\n",
        "                sampler=dict(\n",
        "                    type='RandomSampler',\n",
        "                    num=512,\n",
        "                    pos_fraction=0.25,\n",
        "                    neg_pos_ub=-1,\n",
        "                    add_gt_as_proposals=True),\n",
        "                mask_size=28,\n",
        "                pos_weight=-1,\n",
        "                debug=False),\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='MaxIoUAssigner',\n",
        "                    pos_iou_thr=0.6,\n",
        "                    neg_iou_thr=0.6,\n",
        "                    min_pos_iou=0.6,\n",
        "                    ignore_iof_thr=-1),\n",
        "                sampler=dict(\n",
        "                    type='RandomSampler',\n",
        "                    num=512,\n",
        "                    pos_fraction=0.25,\n",
        "                    neg_pos_ub=-1,\n",
        "                    add_gt_as_proposals=True),\n",
        "                mask_size=28,\n",
        "                pos_weight=-1,\n",
        "                debug=False),\n",
        "            dict(\n",
        "                assigner=dict(\n",
        "                    type='MaxIoUAssigner',\n",
        "                    pos_iou_thr=0.7,\n",
        "                    neg_iou_thr=0.7,\n",
        "                    min_pos_iou=0.7,\n",
        "                    ignore_iof_thr=-1),\n",
        "                sampler=dict(\n",
        "                    type='RandomSampler',\n",
        "                    num=512,\n",
        "                    pos_fraction=0.25,\n",
        "                    neg_pos_ub=-1,\n",
        "                    add_gt_as_proposals=True),\n",
        "                mask_size=28,\n",
        "                pos_weight=-1,\n",
        "                debug=False)\n",
        "        ]),\n",
        "    test_cfg=dict(\n",
        "        rpn=dict(\n",
        "            nms_across_levels=False,\n",
        "            nms_pre=1000,\n",
        "            max_per_img=1000,\n",
        "            nms=dict(type='nms', iou_threshold=0.7),\n",
        "            min_bbox_size=0),\n",
        "        rcnn=dict(\n",
        "            score_thr=0.5,\n",
        "            nms=dict(type='nms', iou_threshold=0.5),\n",
        "            max_per_img=1000,\n",
        "            mask_thr_binary=0.45)))\n",
        "dataset_type = 'CocoDataset'\n",
        "data_root = 'data/coco/'\n",
        "img_norm_cfg = dict(\n",
        "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(\n",
        "        type='LoadAnnotations', with_bbox=True, with_mask=True), #with_seg=True),\n",
        "    dict(\n",
        "        type='Resize',\n",
        "        img_scale=[(1200, 1100)],\n",
        "        multiscale_mode='range',\n",
        "        keep_ratio=True),\n",
        "    dict(type='RandomFlip', flip_ratio=0.5),\n",
        "    dict(\n",
        "        type='Normalize',\n",
        "        mean=[123.675, 116.28, 103.53],\n",
        "        std=[58.395, 57.12, 57.375],\n",
        "        to_rgb=True),\n",
        "    dict(type='Pad', size_divisor=32),\n",
        "    #dict(type='SegRescale', scale_factor=0.125),\n",
        "    dict(type='DefaultFormatBundle'),\n",
        "    dict(\n",
        "        type='Collect',\n",
        "        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
        "]\n",
        "test_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(\n",
        "        type='MultiScaleFlipAug',\n",
        "        img_scale=(1200, 1100),\n",
        "        flip=False,\n",
        "        transforms=[\n",
        "            dict(type='Resize', keep_ratio=True),\n",
        "            dict(type='RandomFlip', flip_ratio=0.5),\n",
        "            dict(\n",
        "                type='Normalize',\n",
        "                mean=[123.675, 116.28, 103.53],\n",
        "                std=[58.395, 57.12, 57.375],\n",
        "                to_rgb=True),\n",
        "            dict(type='Pad', size_divisor=32),\n",
        "            dict(type='ImageToTensor', keys=['img']),\n",
        "            dict(type='Collect', keys=['img'])\n",
        "        ])\n",
        "]\n",
        "\n",
        "CLASSES = ['beetroot-steamed-without-addition-of-salt', 'bread_wholemeal', 'jam', 'water', 'bread', 'banana', 'soft_cheese', 'ham_raw', 'hard_cheese', 'cottage_cheese', 'coffee', 'fruit_mixed', 'pancake', 'tea', 'salmon_smoked', 'avocado', 'spring_onion_scallion', 'ristretto_with_caffeine', 'ham_n_s', 'egg', 'bacon', 'chips_french_fries', 'juice_apple', 'chicken', 'tomato', 'broccoli', 'shrimp_prawn', 'carrot', 'chickpeas', 'french_salad_dressing', 'pasta_hornli_ch', 'sauce_cream', 'pasta_n_s', 'tomato_sauce', 'cheese_n_s', 'pear', 'cashew_nut', 'almonds', 'lentil_n_s', 'mixed_vegetables', 'peanut_butter', 'apple', 'blueberries', 'cucumber', 'yogurt', 'butter', 'mayonnaise', 'soup', 'wine_red', 'wine_white', 'green_bean_steamed_without_addition_of_salt', 'sausage', 'pizza_margherita_baked', 'salami_ch', 'mushroom', 'tart_n_s', 'rice', 'white_coffee', 'sunflower_seeds', 'bell_pepper_red_raw', 'zucchini', 'asparagus', 'tartar_sauce', 'lye_pretzel_soft', 'cucumber_pickled_ch', 'curry_vegetarian', 'soup_of_lentils_dahl_dhal', 'salmon', 'salt_cake_ch_vegetables_filled', 'orange', 'pasta_noodles', 'cream_double_cream_heavy_cream_45', 'cake_chocolate', 'pasta_spaghetti', 'black_olives', 'parmesan', 'spaetzle', 'salad_lambs_ear', 'salad_leaf_salad_green', 'potato', 'white_cabbage', 'halloumi', 'beetroot_raw', 'bread_grain', 'applesauce', 'cheese_for_raclette_ch', 'bread_white', 'curds_natural', 'quiche', 'beef_n_s', 'taboule_prepared_with_couscous', 'aubergine_eggplant', 'mozzarella', 'pasta_penne', 'lasagne_vegetable_prepared', 'mandarine', 'kiwi', 'french_beans', 'spring_roll_fried', 'caprese_salad_tomato_mozzarella', 'leaf_spinach', 'roll_of_half_white_or_white_flour_with_large_void', 'omelette_with_flour_thick_crepe_plain', 'tuna', 'dark_chocolate', 'sauce_savoury_n_s', 'raisins_dried', 'ice_tea_on_black_tea_basis', 'kaki', 'smoothie', 'crepe_with_flour_plain', 'nuggets', 'chili_con_carne_prepared', 'veggie_burger', 'chinese_cabbage', 'hamburger', 'soup_pumpkin', 'sushi', 'chestnuts_ch', 'sauce_soya', 'balsamic_salad_dressing', 'pasta_twist', 'bolognaise_sauce', 'leek', 'fajita_bread_only', 'potato_gnocchi', 'rice_noodles_vermicelli', 'bread_whole_wheat', 'onion', 'garlic', 'hummus', 'pizza_with_vegetables_baked', 'beer', 'glucose_drink_50g', 'ratatouille', 'peanut', 'cauliflower', 'green_olives', 'bread_pita', 'pasta_wholemeal', 'sauce_pesto', 'couscous', 'sauce', 'bread_toast', 'water_with_lemon_juice', 'espresso', 'egg_scrambled', 'juice_orange', 'braided_white_loaf_ch', 'emmental_cheese_ch', 'hazelnut_chocolate_spread_nutella_ovomaltine_caotina', 'tomme_ch', 'hazelnut', 'peach', 'figs', 'mashed_potatoes_prepared_with_full_fat_milk_with_butter', 'pumpkin', 'swiss_chard', 'red_cabbage_raw', 'spinach_raw', 'chicken_curry_cream_coconut_milk_curry_spices_paste', 'crunch_muesli', 'biscuit', 'meatloaf_ch', 'fresh_cheese_n_s', 'honey', 'vegetable_mix_peas_and_carrots', 'parsley', 'brownie', 'ice_cream_n_s', 'salad_dressing', 'dried_meat_n_s', 'chicken_breast', 'mixed_salad_chopped_without_sauce', 'feta', 'praline_n_s', 'walnut', 'potato_salad', 'kolhrabi', 'alfa_sprouts', 'brussel_sprouts', 'gruyere_ch', 'bulgur', 'grapes', 'chocolate_egg_small', 'cappuccino', 'crisp_bread', 'bread_black', 'rosti_n_s', 'mango', 'muesli_dry', 'spinach', 'fish_n_s', 'risotto', 'crisps_ch', 'pork_n_s', 'pomegranate', 'sweet_corn', 'flakes', 'greek_salad', 'sesame_seeds', 'bouillon', 'baked_potato', 'fennel', 'meat_n_s', 'croutons', 'bell_pepper_red_stewed', 'nuts', 'breadcrumbs_unspiced', 'fondue', 'sauce_mushroom', 'strawberries', 'pie_plum_baked_with_cake_dough', 'potatoes_au_gratin_dauphinois_prepared', 'capers', 'bread_wholemeal_toast', 'red_radish', 'fruit_tart', 'beans_kidney', 'sauerkraut', 'mustard', 'country_fries', 'ketchup', 'pasta_linguini_parpadelle_tagliatelle', 'chicken_cut_into_stripes_only_meat', 'cookies', 'sun_dried_tomatoe', 'bread_ticino_ch', 'semi_hard_cheese', 'porridge_prepared_with_partially_skimmed_milk', 'juice', 'chocolate_milk', 'bread_fruit', 'corn', 'dates', 'pistachio', 'cream_cheese_n_s', 'bread_rye', 'witloof_chicory', 'goat_cheese_soft', 'grapefruit_pomelo', 'blue_mould_cheese', 'guacamole', 'tofu', 'cordon_bleu', 'quinoa', 'kefir_drink', 'salad_rocket', 'pizza_with_ham_with_mushrooms_baked', 'fruit_coulis', 'plums', 'pizza_with_ham_baked', 'pineapple', 'seeds_n_s', 'focaccia', 'mixed_milk_beverage', 'coleslaw_chopped_without_sauce', 'sweet_potato', 'chicken_leg', 'croissant', 'cheesecake', 'sauce_cocktail', 'croissant_with_chocolate_filling', 'pumpkin_seeds', 'artichoke', 'soft_drink_with_a_taste', 'apple_pie', 'white_bread_with_butter_eggs_and_milk', 'savoury_pastry_stick', 'tuna_in_oil_drained', 'meat_terrine_pate', 'falafel_balls', 'berries_n_s', 'latte_macchiato', 'sugar_melon_galia_honeydew_cantaloupe', 'mixed_seeds_n_s', 'oil_vinegar_salad_dressing', 'celeriac', 'chocolate_mousse', 'lemon', 'chocolate_cookies', 'birchermuesli_prepared_no_sugar_added', 'muffin', 'pine_nuts', 'french_pizza_from_alsace_baked', 'chocolate_n_s', 'grits_polenta_maize_flour', 'wine_rose', 'cola_based_drink', 'raspberries', 'roll_with_pieces_of_chocolate', 'cake_lemon', 'rice_wild', 'gluten_free_bread', 'pearl_onion', 'tzatziki', 'ham_croissant_ch', 'corn_crisps', 'lentils_green_du_puy_du_berry', 'rice_whole_grain', 'cervelat_ch', 'aperitif_with_alcohol_n_s_aperol_spritz', 'peas', 'tiramisu', 'apricots', 'lasagne_meat_prepared', 'brioche', 'vegetable_au_gratin_baked', 'basil', 'butter_spread_puree_almond', 'pie_apricot', 'rusk_wholemeal', 'pasta_in_conch_form', 'pasta_in_butterfly_form_farfalle', 'damson_plum', 'shoots_n_s', 'coconut', 'banana_cake', 'sauce_curry', 'watermelon_fresh', 'white_asparagus', 'cherries', 'nectarine']\n",
        "\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=1,\n",
        "    workers_per_gpu=2,\n",
        "    train=dict(\n",
        "       type='CocoDataset',\n",
        "        ann_file='data/train/new_ann.json',\n",
        "        img_prefix='data/train/images/',\n",
        "\n",
        "        pipeline=[\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
        "            dict(type='Resize', img_scale=[(1333, 640), (1333, 1000)]),\n",
        "            dict(\n",
        "                type='Albu',\n",
        "                transforms=[dict(type='RandomRotate90', p=1)],\n",
        "                bbox_params=dict(\n",
        "                    type='BboxParams',\n",
        "                    format='pascal_voc',\n",
        "                    label_fields=['gt_labels'],\n",
        "                    min_visibility=0.0,\n",
        "                    filter_lost_elements=True),\n",
        "                keymap=dict(img='image', gt_masks='masks', gt_bboxes='bboxes'),\n",
        "                update_pad_shape=False,\n",
        "                skip_img_without_anno=True),\n",
        "            dict(type='RandomFlip', direction='vertical', flip_ratio=0.5),\n",
        "            dict(type='RandomFlip', direction='horizontal', flip_ratio=0.5),\n",
        "            dict(\n",
        "                type='Normalize',\n",
        "                mean=[123.675, 116.28, 103.53],\n",
        "                std=[58.395, 57.12, 57.375],\n",
        "                to_rgb=True),\n",
        "            dict(type='Pad', size_divisor=32),\n",
        "            dict(type='DefaultFormatBundle'),\n",
        "            dict(\n",
        "                type='Collect',\n",
        "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
        "        ],classes=CLASSES\n",
        "        #seg_prefix='data/coco/stuffthingmaps/train2017/'\n",
        "        ),\n",
        "    val=dict(\n",
        "        type='CocoDataset',\n",
        "        ann_file='data/val/new_ann.json',\n",
        "        img_prefix='data/val/images/',\n",
        "\n",
        "        pipeline=[\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(\n",
        "                type='MultiScaleFlipAug',\n",
        "                img_scale=[(1333, 800), (1333, 1000)],\n",
        "                flip=True,\n",
        "                flip_direction=['horizontal', 'vertical'],\n",
        "                transforms=[\n",
        "                    dict(type='Resize', keep_ratio=True),\n",
        "                    dict(type='RandomFlip'),\n",
        "                    dict(\n",
        "                        type='Normalize',\n",
        "                        mean=[123.675, 116.28, 103.53],\n",
        "                        std=[58.395, 57.12, 57.375],\n",
        "                        to_rgb=True),\n",
        "                    dict(type='Pad', size_divisor=32),\n",
        "                    dict(type='DefaultFormatBundle'),\n",
        "                    dict(type='Collect', keys=['img'])\n",
        "                ])\n",
        "        ],\n",
        "        classes=CLASSES\n",
        "        ),\n",
        "    test=dict(\n",
        "       type='CocoDataset',\n",
        "        ann_file='data/val/new_ann.json',\n",
        "        img_prefix='data/val/images/',\n",
        "\n",
        "        pipeline=[\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(\n",
        "                type='MultiScaleFlipAug',\n",
        "                img_scale=[(1333, 800), (1333, 1000)],\n",
        "                flip=True,\n",
        "                flip_direction=['horizontal', 'vertical'],\n",
        "                transforms=[\n",
        "                    dict(type='Resize', keep_ratio=True),\n",
        "                    dict(type='RandomFlip'),\n",
        "                    dict(\n",
        "                        type='Normalize',\n",
        "                        mean=[123.675, 116.28, 103.53],\n",
        "                        std=[58.395, 57.12, 57.375],\n",
        "                        to_rgb=True),\n",
        "                    dict(type='Pad', size_divisor=32),\n",
        "                    dict(type='DefaultFormatBundle'),\n",
        "                    dict(type='Collect', keys=['img'])\n",
        "                ])\n",
        "        ],\n",
        "        classes=CLASSES))\n",
        "evaluation = dict(interval=1, metric=['segm'])\n",
        "optimizer = dict(\n",
        "    type='AdamW',\n",
        "    lr= 0.0001,  #5e-05,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=0.05,\n",
        "    paramwise_cfg=dict(\n",
        "        custom_keys=dict(\n",
        "            absolute_pos_embed=dict(decay_mult=0.0),\n",
        "            relative_position_bias_table=dict(decay_mult=0.0),\n",
        "            norm=dict(decay_mult=0.0))))\n",
        "optimizer_config = dict(grad_clip=dict(max_norm=15, norm_type=2))\n",
        "fp16 = dict(loss_scale=512.0)\n",
        "# optimizer_config = dict(\n",
        "#     grad_clip=None,\n",
        "#     type='DistOptimizerHook',\n",
        "#     update_interval=1,\n",
        "#     coalesce=True,\n",
        "#     bucket_size_mb=-1,\n",
        "#     use_fp16=True)\n",
        "# lr_config = dict(\n",
        "#     policy='step',\n",
        "#     warmup='linear',\n",
        "#     warmup_iters=500,\n",
        "#     warmup_ratio=0.001,\n",
        "#     step=[16, 19])\n",
        "\n",
        "lr_config = dict(\n",
        "    policy='CosineAnnealing',\n",
        "    by_epoch=False,\n",
        "    warmup='linear',\n",
        "    warmup_iters=720,\n",
        "    warmup_ratio=0.001,\n",
        "    min_lr=5e-06)\n",
        "runner = dict(type='EpochBasedRunner', max_epochs=20)\n",
        "#checkpoint_config = dict(interval=1)\n",
        "checkpoint_config = dict(max_keep_ckpts=2, interval=1)\n",
        "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
        "custom_hooks = [dict(type='NumClassCheckHook')]\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "load_from = '/content/htc_cbv2_swin_base22k_patch4_window7_mstrain_400-1400_giou_4conv1f_adamw_20e_coco.pth' #'https://github.com/CBNetwork/storage/releases/download/v1.0.0/htc_cbv2_swin_base22k_patch4_window7_mstrain_400-1400_giou_4conv1f_adamw_20e_coco.pth.zip' # None\n",
        "resume_from = None\n",
        "workflow = [('train', 1)]\n",
        "#fp16 = None\n",
        "work_dir = '/content/drive/MyDrive/log_mmdetCBNET'\n",
        "gpu_ids = range(0, 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjQ-fGqR5BLl",
        "outputId": "739e9409-e99d-4142-f02b-912216b22e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cbnet_base.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O pre.pth 'https://drive.google.com/file/d/1tqkpaArF0a0WVEolsCC8yrvgoydY7_Ha/view?usp=sharing'"
      ],
      "metadata": {
        "id": "xhQH3RZ3DTR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8dcbbab-7ad9-409a-de57-b2ba68ae8de1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-06 10:25:08--  https://drive.google.com/file/d/1tqkpaArF0a0WVEolsCC8yrvgoydY7_Ha/view?usp=sharing\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.157.139, 142.250.157.101, 142.250.157.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.157.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘pre.pth’\n",
            "\n",
            "pre.pth                 [ <=>                ]  64.87K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-06 10:25:09 (529 KB/s) - ‘pre.pth’ saved [66427]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip *.zip"
      ],
      "metadata": {
        "id": "a__jsg3SZZjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xstkfNrfe16"
      },
      "source": [
        "# Training the Model 🚂\n",
        "\n",
        "Finally training our model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets train the model\n",
        "!pip uninstall pycocotools -y\n",
        "!pip install mmpycocotools\n",
        "!pip install --upgrade albumentations\n",
        "!pip uninstall opencv-python-headless -y\n",
        "!pip install opencv-python-headless==4.5.2.52\n",
        "!python mmdetection/tools/train.py cbnet_base.py --work-dir '/content/drive/MyDrive/log_mmdetCBNET' #--no-validate"
      ],
      "metadata": {
        "id": "c1Zs__yhiZo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python mmdetection/tools/train.py query_large.py --work-dir '/content/drive/MyDrive/log_mmdetQuery' #--no-validate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgUGLRIW3EVM",
        "outputId": "f206d894-5362-4509-bee0-5d6ae13e3787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex is not installed\n",
            "apex is not installed\n",
            "apex is not installed\n",
            "apex is not installed\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2022-04-06 11:18:22,782 - mmdet - INFO - Environment info:\n",
            "------------------------------------------------------------\n",
            "sys.platform: linux\n",
            "Python: 3.7.13 (default, Mar 16 2022, 17:37:17) [GCC 7.5.0]\n",
            "CUDA available: True\n",
            "GPU 0: Tesla P100-PCIE-16GB\n",
            "CUDA_HOME: /usr/local/cuda\n",
            "NVCC: Build cuda_11.1.TC455_06.29190527_0\n",
            "GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "PyTorch: 1.10.0+cu111\n",
            "PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
            "  - CuDNN 8.0.5\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "TorchVision: 0.11.1+cu111\n",
            "OpenCV: 4.1.2\n",
            "MMCV: 1.4.0\n",
            "MMCV Compiler: GCC 7.3\n",
            "MMCV CUDA Compiler: 11.1\n",
            "MMDetection: 2.12.0+\n",
            "------------------------------------------------------------\n",
            "\n",
            "2022-04-06 11:18:26,308 - mmdet - INFO - Distributed training: False\n",
            "2022-04-06 11:18:29,373 - mmdet - INFO - Config:\n",
            "dataset_type = 'CocoDataset'\n",
            "data_root = 'data/coco/'\n",
            "img_norm_cfg = dict(\n",
            "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
            "CLASSES = [\n",
            "    'beetroot-steamed-without-addition-of-salt', 'bread_wholemeal', 'jam',\n",
            "    'water', 'bread', 'banana', 'soft_cheese', 'ham_raw', 'hard_cheese',\n",
            "    'cottage_cheese', 'coffee', 'fruit_mixed', 'pancake', 'tea',\n",
            "    'salmon_smoked', 'avocado', 'spring_onion_scallion',\n",
            "    'ristretto_with_caffeine', 'ham_n_s', 'egg', 'bacon', 'chips_french_fries',\n",
            "    'juice_apple', 'chicken', 'tomato', 'broccoli', 'shrimp_prawn', 'carrot',\n",
            "    'chickpeas', 'french_salad_dressing', 'pasta_hornli_ch', 'sauce_cream',\n",
            "    'pasta_n_s', 'tomato_sauce', 'cheese_n_s', 'pear', 'cashew_nut', 'almonds',\n",
            "    'lentil_n_s', 'mixed_vegetables', 'peanut_butter', 'apple', 'blueberries',\n",
            "    'cucumber', 'yogurt', 'butter', 'mayonnaise', 'soup', 'wine_red',\n",
            "    'wine_white', 'green_bean_steamed_without_addition_of_salt', 'sausage',\n",
            "    'pizza_margherita_baked', 'salami_ch', 'mushroom', 'tart_n_s', 'rice',\n",
            "    'white_coffee', 'sunflower_seeds', 'bell_pepper_red_raw', 'zucchini',\n",
            "    'asparagus', 'tartar_sauce', 'lye_pretzel_soft', 'cucumber_pickled_ch',\n",
            "    'curry_vegetarian', 'soup_of_lentils_dahl_dhal', 'salmon',\n",
            "    'salt_cake_ch_vegetables_filled', 'orange', 'pasta_noodles',\n",
            "    'cream_double_cream_heavy_cream_45', 'cake_chocolate', 'pasta_spaghetti',\n",
            "    'black_olives', 'parmesan', 'spaetzle', 'salad_lambs_ear',\n",
            "    'salad_leaf_salad_green', 'potato', 'white_cabbage', 'halloumi',\n",
            "    'beetroot_raw', 'bread_grain', 'applesauce', 'cheese_for_raclette_ch',\n",
            "    'bread_white', 'curds_natural', 'quiche', 'beef_n_s',\n",
            "    'taboule_prepared_with_couscous', 'aubergine_eggplant', 'mozzarella',\n",
            "    'pasta_penne', 'lasagne_vegetable_prepared', 'mandarine', 'kiwi',\n",
            "    'french_beans', 'spring_roll_fried', 'caprese_salad_tomato_mozzarella',\n",
            "    'leaf_spinach', 'roll_of_half_white_or_white_flour_with_large_void',\n",
            "    'omelette_with_flour_thick_crepe_plain', 'tuna', 'dark_chocolate',\n",
            "    'sauce_savoury_n_s', 'raisins_dried', 'ice_tea_on_black_tea_basis', 'kaki',\n",
            "    'smoothie', 'crepe_with_flour_plain', 'nuggets',\n",
            "    'chili_con_carne_prepared', 'veggie_burger', 'chinese_cabbage',\n",
            "    'hamburger', 'soup_pumpkin', 'sushi', 'chestnuts_ch', 'sauce_soya',\n",
            "    'balsamic_salad_dressing', 'pasta_twist', 'bolognaise_sauce', 'leek',\n",
            "    'fajita_bread_only', 'potato_gnocchi', 'rice_noodles_vermicelli',\n",
            "    'bread_whole_wheat', 'onion', 'garlic', 'hummus',\n",
            "    'pizza_with_vegetables_baked', 'beer', 'glucose_drink_50g', 'ratatouille',\n",
            "    'peanut', 'cauliflower', 'green_olives', 'bread_pita', 'pasta_wholemeal',\n",
            "    'sauce_pesto', 'couscous', 'sauce', 'bread_toast',\n",
            "    'water_with_lemon_juice', 'espresso', 'egg_scrambled', 'juice_orange',\n",
            "    'braided_white_loaf_ch', 'emmental_cheese_ch',\n",
            "    'hazelnut_chocolate_spread_nutella_ovomaltine_caotina', 'tomme_ch',\n",
            "    'hazelnut', 'peach', 'figs',\n",
            "    'mashed_potatoes_prepared_with_full_fat_milk_with_butter', 'pumpkin',\n",
            "    'swiss_chard', 'red_cabbage_raw', 'spinach_raw',\n",
            "    'chicken_curry_cream_coconut_milk_curry_spices_paste', 'crunch_muesli',\n",
            "    'biscuit', 'meatloaf_ch', 'fresh_cheese_n_s', 'honey',\n",
            "    'vegetable_mix_peas_and_carrots', 'parsley', 'brownie', 'ice_cream_n_s',\n",
            "    'salad_dressing', 'dried_meat_n_s', 'chicken_breast',\n",
            "    'mixed_salad_chopped_without_sauce', 'feta', 'praline_n_s', 'walnut',\n",
            "    'potato_salad', 'kolhrabi', 'alfa_sprouts', 'brussel_sprouts',\n",
            "    'gruyere_ch', 'bulgur', 'grapes', 'chocolate_egg_small', 'cappuccino',\n",
            "    'crisp_bread', 'bread_black', 'rosti_n_s', 'mango', 'muesli_dry',\n",
            "    'spinach', 'fish_n_s', 'risotto', 'crisps_ch', 'pork_n_s', 'pomegranate',\n",
            "    'sweet_corn', 'flakes', 'greek_salad', 'sesame_seeds', 'bouillon',\n",
            "    'baked_potato', 'fennel', 'meat_n_s', 'croutons', 'bell_pepper_red_stewed',\n",
            "    'nuts', 'breadcrumbs_unspiced', 'fondue', 'sauce_mushroom', 'strawberries',\n",
            "    'pie_plum_baked_with_cake_dough', 'potatoes_au_gratin_dauphinois_prepared',\n",
            "    'capers', 'bread_wholemeal_toast', 'red_radish', 'fruit_tart',\n",
            "    'beans_kidney', 'sauerkraut', 'mustard', 'country_fries', 'ketchup',\n",
            "    'pasta_linguini_parpadelle_tagliatelle',\n",
            "    'chicken_cut_into_stripes_only_meat', 'cookies', 'sun_dried_tomatoe',\n",
            "    'bread_ticino_ch', 'semi_hard_cheese',\n",
            "    'porridge_prepared_with_partially_skimmed_milk', 'juice', 'chocolate_milk',\n",
            "    'bread_fruit', 'corn', 'dates', 'pistachio', 'cream_cheese_n_s',\n",
            "    'bread_rye', 'witloof_chicory', 'goat_cheese_soft', 'grapefruit_pomelo',\n",
            "    'blue_mould_cheese', 'guacamole', 'tofu', 'cordon_bleu', 'quinoa',\n",
            "    'kefir_drink', 'salad_rocket', 'pizza_with_ham_with_mushrooms_baked',\n",
            "    'fruit_coulis', 'plums', 'pizza_with_ham_baked', 'pineapple', 'seeds_n_s',\n",
            "    'focaccia', 'mixed_milk_beverage', 'coleslaw_chopped_without_sauce',\n",
            "    'sweet_potato', 'chicken_leg', 'croissant', 'cheesecake', 'sauce_cocktail',\n",
            "    'croissant_with_chocolate_filling', 'pumpkin_seeds', 'artichoke',\n",
            "    'soft_drink_with_a_taste', 'apple_pie',\n",
            "    'white_bread_with_butter_eggs_and_milk', 'savoury_pastry_stick',\n",
            "    'tuna_in_oil_drained', 'meat_terrine_pate', 'falafel_balls', 'berries_n_s',\n",
            "    'latte_macchiato', 'sugar_melon_galia_honeydew_cantaloupe',\n",
            "    'mixed_seeds_n_s', 'oil_vinegar_salad_dressing', 'celeriac',\n",
            "    'chocolate_mousse', 'lemon', 'chocolate_cookies',\n",
            "    'birchermuesli_prepared_no_sugar_added', 'muffin', 'pine_nuts',\n",
            "    'french_pizza_from_alsace_baked', 'chocolate_n_s',\n",
            "    'grits_polenta_maize_flour', 'wine_rose', 'cola_based_drink',\n",
            "    'raspberries', 'roll_with_pieces_of_chocolate', 'cake_lemon', 'rice_wild',\n",
            "    'gluten_free_bread', 'pearl_onion', 'tzatziki', 'ham_croissant_ch',\n",
            "    'corn_crisps', 'lentils_green_du_puy_du_berry', 'rice_whole_grain',\n",
            "    'cervelat_ch', 'aperitif_with_alcohol_n_s_aperol_spritz', 'peas',\n",
            "    'tiramisu', 'apricots', 'lasagne_meat_prepared', 'brioche',\n",
            "    'vegetable_au_gratin_baked', 'basil', 'butter_spread_puree_almond',\n",
            "    'pie_apricot', 'rusk_wholemeal', 'pasta_in_conch_form',\n",
            "    'pasta_in_butterfly_form_farfalle', 'damson_plum', 'shoots_n_s', 'coconut',\n",
            "    'banana_cake', 'sauce_curry', 'watermelon_fresh', 'white_asparagus',\n",
            "    'cherries', 'nectarine'\n",
            "]\n",
            "data = dict(\n",
            "    samples_per_gpu=1,\n",
            "    workers_per_gpu=2,\n",
            "    train=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file='data/train/new_ann.json',\n",
            "        img_prefix='data/train/images/',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
            "            dict(type='RandomFlip', flip_ratio=0.5),\n",
            "            dict(\n",
            "                type='AutoAugment',\n",
            "                policies=[[{\n",
            "                    'type': 'Resize',\n",
            "                    'img_scale': [(400, 1333), (1200, 1333)],\n",
            "                    'multiscale_mode': 'range',\n",
            "                    'keep_ratio': True\n",
            "                }],\n",
            "                          [{\n",
            "                              'type': 'Resize',\n",
            "                              'img_scale': [(400, 1333), (500, 1333),\n",
            "                                            (600, 1333)],\n",
            "                              'multiscale_mode': 'value',\n",
            "                              'keep_ratio': True\n",
            "                          }, {\n",
            "                              'type': 'RandomCrop',\n",
            "                              'crop_type': 'absolute_range',\n",
            "                              'crop_size': (384, 600),\n",
            "                              'allow_negative_crop': True\n",
            "                          }, {\n",
            "                              'type': 'Resize',\n",
            "                              'img_scale': [(400, 1333), (1200, 1333)],\n",
            "                              'multiscale_mode': 'range',\n",
            "                              'override': True,\n",
            "                              'keep_ratio': True\n",
            "                          }]]),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='DefaultFormatBundle'),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
            "        ],\n",
            "        classes=[\n",
            "            'beetroot-steamed-without-addition-of-salt', 'bread_wholemeal',\n",
            "            'jam', 'water', 'bread', 'banana', 'soft_cheese', 'ham_raw',\n",
            "            'hard_cheese', 'cottage_cheese', 'coffee', 'fruit_mixed',\n",
            "            'pancake', 'tea', 'salmon_smoked', 'avocado',\n",
            "            'spring_onion_scallion', 'ristretto_with_caffeine', 'ham_n_s',\n",
            "            'egg', 'bacon', 'chips_french_fries', 'juice_apple', 'chicken',\n",
            "            'tomato', 'broccoli', 'shrimp_prawn', 'carrot', 'chickpeas',\n",
            "            'french_salad_dressing', 'pasta_hornli_ch', 'sauce_cream',\n",
            "            'pasta_n_s', 'tomato_sauce', 'cheese_n_s', 'pear', 'cashew_nut',\n",
            "            'almonds', 'lentil_n_s', 'mixed_vegetables', 'peanut_butter',\n",
            "            'apple', 'blueberries', 'cucumber', 'yogurt', 'butter',\n",
            "            'mayonnaise', 'soup', 'wine_red', 'wine_white',\n",
            "            'green_bean_steamed_without_addition_of_salt', 'sausage',\n",
            "            'pizza_margherita_baked', 'salami_ch', 'mushroom', 'tart_n_s',\n",
            "            'rice', 'white_coffee', 'sunflower_seeds', 'bell_pepper_red_raw',\n",
            "            'zucchini', 'asparagus', 'tartar_sauce', 'lye_pretzel_soft',\n",
            "            'cucumber_pickled_ch', 'curry_vegetarian',\n",
            "            'soup_of_lentils_dahl_dhal', 'salmon',\n",
            "            'salt_cake_ch_vegetables_filled', 'orange', 'pasta_noodles',\n",
            "            'cream_double_cream_heavy_cream_45', 'cake_chocolate',\n",
            "            'pasta_spaghetti', 'black_olives', 'parmesan', 'spaetzle',\n",
            "            'salad_lambs_ear', 'salad_leaf_salad_green', 'potato',\n",
            "            'white_cabbage', 'halloumi', 'beetroot_raw', 'bread_grain',\n",
            "            'applesauce', 'cheese_for_raclette_ch', 'bread_white',\n",
            "            'curds_natural', 'quiche', 'beef_n_s',\n",
            "            'taboule_prepared_with_couscous', 'aubergine_eggplant',\n",
            "            'mozzarella', 'pasta_penne', 'lasagne_vegetable_prepared',\n",
            "            'mandarine', 'kiwi', 'french_beans', 'spring_roll_fried',\n",
            "            'caprese_salad_tomato_mozzarella', 'leaf_spinach',\n",
            "            'roll_of_half_white_or_white_flour_with_large_void',\n",
            "            'omelette_with_flour_thick_crepe_plain', 'tuna', 'dark_chocolate',\n",
            "            'sauce_savoury_n_s', 'raisins_dried', 'ice_tea_on_black_tea_basis',\n",
            "            'kaki', 'smoothie', 'crepe_with_flour_plain', 'nuggets',\n",
            "            'chili_con_carne_prepared', 'veggie_burger', 'chinese_cabbage',\n",
            "            'hamburger', 'soup_pumpkin', 'sushi', 'chestnuts_ch', 'sauce_soya',\n",
            "            'balsamic_salad_dressing', 'pasta_twist', 'bolognaise_sauce',\n",
            "            'leek', 'fajita_bread_only', 'potato_gnocchi',\n",
            "            'rice_noodles_vermicelli', 'bread_whole_wheat', 'onion', 'garlic',\n",
            "            'hummus', 'pizza_with_vegetables_baked', 'beer',\n",
            "            'glucose_drink_50g', 'ratatouille', 'peanut', 'cauliflower',\n",
            "            'green_olives', 'bread_pita', 'pasta_wholemeal', 'sauce_pesto',\n",
            "            'couscous', 'sauce', 'bread_toast', 'water_with_lemon_juice',\n",
            "            'espresso', 'egg_scrambled', 'juice_orange',\n",
            "            'braided_white_loaf_ch', 'emmental_cheese_ch',\n",
            "            'hazelnut_chocolate_spread_nutella_ovomaltine_caotina', 'tomme_ch',\n",
            "            'hazelnut', 'peach', 'figs',\n",
            "            'mashed_potatoes_prepared_with_full_fat_milk_with_butter',\n",
            "            'pumpkin', 'swiss_chard', 'red_cabbage_raw', 'spinach_raw',\n",
            "            'chicken_curry_cream_coconut_milk_curry_spices_paste',\n",
            "            'crunch_muesli', 'biscuit', 'meatloaf_ch', 'fresh_cheese_n_s',\n",
            "            'honey', 'vegetable_mix_peas_and_carrots', 'parsley', 'brownie',\n",
            "            'ice_cream_n_s', 'salad_dressing', 'dried_meat_n_s',\n",
            "            'chicken_breast', 'mixed_salad_chopped_without_sauce', 'feta',\n",
            "            'praline_n_s', 'walnut', 'potato_salad', 'kolhrabi',\n",
            "            'alfa_sprouts', 'brussel_sprouts', 'gruyere_ch', 'bulgur',\n",
            "            'grapes', 'chocolate_egg_small', 'cappuccino', 'crisp_bread',\n",
            "            'bread_black', 'rosti_n_s', 'mango', 'muesli_dry', 'spinach',\n",
            "            'fish_n_s', 'risotto', 'crisps_ch', 'pork_n_s', 'pomegranate',\n",
            "            'sweet_corn', 'flakes', 'greek_salad', 'sesame_seeds', 'bouillon',\n",
            "            'baked_potato', 'fennel', 'meat_n_s', 'croutons',\n",
            "            'bell_pepper_red_stewed', 'nuts', 'breadcrumbs_unspiced', 'fondue',\n",
            "            'sauce_mushroom', 'strawberries', 'pie_plum_baked_with_cake_dough',\n",
            "            'potatoes_au_gratin_dauphinois_prepared', 'capers',\n",
            "            'bread_wholemeal_toast', 'red_radish', 'fruit_tart',\n",
            "            'beans_kidney', 'sauerkraut', 'mustard', 'country_fries',\n",
            "            'ketchup', 'pasta_linguini_parpadelle_tagliatelle',\n",
            "            'chicken_cut_into_stripes_only_meat', 'cookies',\n",
            "            'sun_dried_tomatoe', 'bread_ticino_ch', 'semi_hard_cheese',\n",
            "            'porridge_prepared_with_partially_skimmed_milk', 'juice',\n",
            "            'chocolate_milk', 'bread_fruit', 'corn', 'dates', 'pistachio',\n",
            "            'cream_cheese_n_s', 'bread_rye', 'witloof_chicory',\n",
            "            'goat_cheese_soft', 'grapefruit_pomelo', 'blue_mould_cheese',\n",
            "            'guacamole', 'tofu', 'cordon_bleu', 'quinoa', 'kefir_drink',\n",
            "            'salad_rocket', 'pizza_with_ham_with_mushrooms_baked',\n",
            "            'fruit_coulis', 'plums', 'pizza_with_ham_baked', 'pineapple',\n",
            "            'seeds_n_s', 'focaccia', 'mixed_milk_beverage',\n",
            "            'coleslaw_chopped_without_sauce', 'sweet_potato', 'chicken_leg',\n",
            "            'croissant', 'cheesecake', 'sauce_cocktail',\n",
            "            'croissant_with_chocolate_filling', 'pumpkin_seeds', 'artichoke',\n",
            "            'soft_drink_with_a_taste', 'apple_pie',\n",
            "            'white_bread_with_butter_eggs_and_milk', 'savoury_pastry_stick',\n",
            "            'tuna_in_oil_drained', 'meat_terrine_pate', 'falafel_balls',\n",
            "            'berries_n_s', 'latte_macchiato',\n",
            "            'sugar_melon_galia_honeydew_cantaloupe', 'mixed_seeds_n_s',\n",
            "            'oil_vinegar_salad_dressing', 'celeriac', 'chocolate_mousse',\n",
            "            'lemon', 'chocolate_cookies',\n",
            "            'birchermuesli_prepared_no_sugar_added', 'muffin', 'pine_nuts',\n",
            "            'french_pizza_from_alsace_baked', 'chocolate_n_s',\n",
            "            'grits_polenta_maize_flour', 'wine_rose', 'cola_based_drink',\n",
            "            'raspberries', 'roll_with_pieces_of_chocolate', 'cake_lemon',\n",
            "            'rice_wild', 'gluten_free_bread', 'pearl_onion', 'tzatziki',\n",
            "            'ham_croissant_ch', 'corn_crisps', 'lentils_green_du_puy_du_berry',\n",
            "            'rice_whole_grain', 'cervelat_ch',\n",
            "            'aperitif_with_alcohol_n_s_aperol_spritz', 'peas', 'tiramisu',\n",
            "            'apricots', 'lasagne_meat_prepared', 'brioche',\n",
            "            'vegetable_au_gratin_baked', 'basil', 'butter_spread_puree_almond',\n",
            "            'pie_apricot', 'rusk_wholemeal', 'pasta_in_conch_form',\n",
            "            'pasta_in_butterfly_form_farfalle', 'damson_plum', 'shoots_n_s',\n",
            "            'coconut', 'banana_cake', 'sauce_curry', 'watermelon_fresh',\n",
            "            'white_asparagus', 'cherries', 'nectarine'\n",
            "        ]),\n",
            "    val=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file='data/val/new_ann.json',\n",
            "        img_prefix='data/val/images/',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(1333, 800),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ],\n",
            "        classes=[\n",
            "            'beetroot-steamed-without-addition-of-salt', 'bread_wholemeal',\n",
            "            'jam', 'water', 'bread', 'banana', 'soft_cheese', 'ham_raw',\n",
            "            'hard_cheese', 'cottage_cheese', 'coffee', 'fruit_mixed',\n",
            "            'pancake', 'tea', 'salmon_smoked', 'avocado',\n",
            "            'spring_onion_scallion', 'ristretto_with_caffeine', 'ham_n_s',\n",
            "            'egg', 'bacon', 'chips_french_fries', 'juice_apple', 'chicken',\n",
            "            'tomato', 'broccoli', 'shrimp_prawn', 'carrot', 'chickpeas',\n",
            "            'french_salad_dressing', 'pasta_hornli_ch', 'sauce_cream',\n",
            "            'pasta_n_s', 'tomato_sauce', 'cheese_n_s', 'pear', 'cashew_nut',\n",
            "            'almonds', 'lentil_n_s', 'mixed_vegetables', 'peanut_butter',\n",
            "            'apple', 'blueberries', 'cucumber', 'yogurt', 'butter',\n",
            "            'mayonnaise', 'soup', 'wine_red', 'wine_white',\n",
            "            'green_bean_steamed_without_addition_of_salt', 'sausage',\n",
            "            'pizza_margherita_baked', 'salami_ch', 'mushroom', 'tart_n_s',\n",
            "            'rice', 'white_coffee', 'sunflower_seeds', 'bell_pepper_red_raw',\n",
            "            'zucchini', 'asparagus', 'tartar_sauce', 'lye_pretzel_soft',\n",
            "            'cucumber_pickled_ch', 'curry_vegetarian',\n",
            "            'soup_of_lentils_dahl_dhal', 'salmon',\n",
            "            'salt_cake_ch_vegetables_filled', 'orange', 'pasta_noodles',\n",
            "            'cream_double_cream_heavy_cream_45', 'cake_chocolate',\n",
            "            'pasta_spaghetti', 'black_olives', 'parmesan', 'spaetzle',\n",
            "            'salad_lambs_ear', 'salad_leaf_salad_green', 'potato',\n",
            "            'white_cabbage', 'halloumi', 'beetroot_raw', 'bread_grain',\n",
            "            'applesauce', 'cheese_for_raclette_ch', 'bread_white',\n",
            "            'curds_natural', 'quiche', 'beef_n_s',\n",
            "            'taboule_prepared_with_couscous', 'aubergine_eggplant',\n",
            "            'mozzarella', 'pasta_penne', 'lasagne_vegetable_prepared',\n",
            "            'mandarine', 'kiwi', 'french_beans', 'spring_roll_fried',\n",
            "            'caprese_salad_tomato_mozzarella', 'leaf_spinach',\n",
            "            'roll_of_half_white_or_white_flour_with_large_void',\n",
            "            'omelette_with_flour_thick_crepe_plain', 'tuna', 'dark_chocolate',\n",
            "            'sauce_savoury_n_s', 'raisins_dried', 'ice_tea_on_black_tea_basis',\n",
            "            'kaki', 'smoothie', 'crepe_with_flour_plain', 'nuggets',\n",
            "            'chili_con_carne_prepared', 'veggie_burger', 'chinese_cabbage',\n",
            "            'hamburger', 'soup_pumpkin', 'sushi', 'chestnuts_ch', 'sauce_soya',\n",
            "            'balsamic_salad_dressing', 'pasta_twist', 'bolognaise_sauce',\n",
            "            'leek', 'fajita_bread_only', 'potato_gnocchi',\n",
            "            'rice_noodles_vermicelli', 'bread_whole_wheat', 'onion', 'garlic',\n",
            "            'hummus', 'pizza_with_vegetables_baked', 'beer',\n",
            "            'glucose_drink_50g', 'ratatouille', 'peanut', 'cauliflower',\n",
            "            'green_olives', 'bread_pita', 'pasta_wholemeal', 'sauce_pesto',\n",
            "            'couscous', 'sauce', 'bread_toast', 'water_with_lemon_juice',\n",
            "            'espresso', 'egg_scrambled', 'juice_orange',\n",
            "            'braided_white_loaf_ch', 'emmental_cheese_ch',\n",
            "            'hazelnut_chocolate_spread_nutella_ovomaltine_caotina', 'tomme_ch',\n",
            "            'hazelnut', 'peach', 'figs',\n",
            "            'mashed_potatoes_prepared_with_full_fat_milk_with_butter',\n",
            "            'pumpkin', 'swiss_chard', 'red_cabbage_raw', 'spinach_raw',\n",
            "            'chicken_curry_cream_coconut_milk_curry_spices_paste',\n",
            "            'crunch_muesli', 'biscuit', 'meatloaf_ch', 'fresh_cheese_n_s',\n",
            "            'honey', 'vegetable_mix_peas_and_carrots', 'parsley', 'brownie',\n",
            "            'ice_cream_n_s', 'salad_dressing', 'dried_meat_n_s',\n",
            "            'chicken_breast', 'mixed_salad_chopped_without_sauce', 'feta',\n",
            "            'praline_n_s', 'walnut', 'potato_salad', 'kolhrabi',\n",
            "            'alfa_sprouts', 'brussel_sprouts', 'gruyere_ch', 'bulgur',\n",
            "            'grapes', 'chocolate_egg_small', 'cappuccino', 'crisp_bread',\n",
            "            'bread_black', 'rosti_n_s', 'mango', 'muesli_dry', 'spinach',\n",
            "            'fish_n_s', 'risotto', 'crisps_ch', 'pork_n_s', 'pomegranate',\n",
            "            'sweet_corn', 'flakes', 'greek_salad', 'sesame_seeds', 'bouillon',\n",
            "            'baked_potato', 'fennel', 'meat_n_s', 'croutons',\n",
            "            'bell_pepper_red_stewed', 'nuts', 'breadcrumbs_unspiced', 'fondue',\n",
            "            'sauce_mushroom', 'strawberries', 'pie_plum_baked_with_cake_dough',\n",
            "            'potatoes_au_gratin_dauphinois_prepared', 'capers',\n",
            "            'bread_wholemeal_toast', 'red_radish', 'fruit_tart',\n",
            "            'beans_kidney', 'sauerkraut', 'mustard', 'country_fries',\n",
            "            'ketchup', 'pasta_linguini_parpadelle_tagliatelle',\n",
            "            'chicken_cut_into_stripes_only_meat', 'cookies',\n",
            "            'sun_dried_tomatoe', 'bread_ticino_ch', 'semi_hard_cheese',\n",
            "            'porridge_prepared_with_partially_skimmed_milk', 'juice',\n",
            "            'chocolate_milk', 'bread_fruit', 'corn', 'dates', 'pistachio',\n",
            "            'cream_cheese_n_s', 'bread_rye', 'witloof_chicory',\n",
            "            'goat_cheese_soft', 'grapefruit_pomelo', 'blue_mould_cheese',\n",
            "            'guacamole', 'tofu', 'cordon_bleu', 'quinoa', 'kefir_drink',\n",
            "            'salad_rocket', 'pizza_with_ham_with_mushrooms_baked',\n",
            "            'fruit_coulis', 'plums', 'pizza_with_ham_baked', 'pineapple',\n",
            "            'seeds_n_s', 'focaccia', 'mixed_milk_beverage',\n",
            "            'coleslaw_chopped_without_sauce', 'sweet_potato', 'chicken_leg',\n",
            "            'croissant', 'cheesecake', 'sauce_cocktail',\n",
            "            'croissant_with_chocolate_filling', 'pumpkin_seeds', 'artichoke',\n",
            "            'soft_drink_with_a_taste', 'apple_pie',\n",
            "            'white_bread_with_butter_eggs_and_milk', 'savoury_pastry_stick',\n",
            "            'tuna_in_oil_drained', 'meat_terrine_pate', 'falafel_balls',\n",
            "            'berries_n_s', 'latte_macchiato',\n",
            "            'sugar_melon_galia_honeydew_cantaloupe', 'mixed_seeds_n_s',\n",
            "            'oil_vinegar_salad_dressing', 'celeriac', 'chocolate_mousse',\n",
            "            'lemon', 'chocolate_cookies',\n",
            "            'birchermuesli_prepared_no_sugar_added', 'muffin', 'pine_nuts',\n",
            "            'french_pizza_from_alsace_baked', 'chocolate_n_s',\n",
            "            'grits_polenta_maize_flour', 'wine_rose', 'cola_based_drink',\n",
            "            'raspberries', 'roll_with_pieces_of_chocolate', 'cake_lemon',\n",
            "            'rice_wild', 'gluten_free_bread', 'pearl_onion', 'tzatziki',\n",
            "            'ham_croissant_ch', 'corn_crisps', 'lentils_green_du_puy_du_berry',\n",
            "            'rice_whole_grain', 'cervelat_ch',\n",
            "            'aperitif_with_alcohol_n_s_aperol_spritz', 'peas', 'tiramisu',\n",
            "            'apricots', 'lasagne_meat_prepared', 'brioche',\n",
            "            'vegetable_au_gratin_baked', 'basil', 'butter_spread_puree_almond',\n",
            "            'pie_apricot', 'rusk_wholemeal', 'pasta_in_conch_form',\n",
            "            'pasta_in_butterfly_form_farfalle', 'damson_plum', 'shoots_n_s',\n",
            "            'coconut', 'banana_cake', 'sauce_curry', 'watermelon_fresh',\n",
            "            'white_asparagus', 'cherries', 'nectarine'\n",
            "        ]),\n",
            "    test=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file='data/val/new_ann.json',\n",
            "        img_prefix='data/val/images/',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(1333, 800),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ]))\n",
            "evaluation = dict(metric=['segm'])\n",
            "optimizer = dict(\n",
            "    type='AdamW',\n",
            "    lr=2.5e-05,\n",
            "    weight_decay=0.0001,\n",
            "    paramwise_cfg=dict(\n",
            "        custom_keys=dict(\n",
            "            absolute_pos_embed=dict(decay_mult=0.0),\n",
            "            relative_position_bias_table=dict(decay_mult=0.0),\n",
            "            norm=dict(decay_mult=0.0))))\n",
            "lr_config = dict(\n",
            "    policy='step',\n",
            "    warmup='linear',\n",
            "    warmup_iters=1000,\n",
            "    warmup_ratio=0.001,\n",
            "    step=[12])\n",
            "runner = dict(type='EpochBasedRunner', max_epochs=22)\n",
            "checkpoint_config = dict(interval=1)\n",
            "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
            "custom_hooks = [dict(type='NumClassCheckHook')]\n",
            "dist_params = dict(backend='nccl')\n",
            "log_level = 'INFO'\n",
            "load_from = '/content/drive/MyDrive/queryinst_swin_large_patch4_window7_fpn_300_queries-832c5813.pth'\n",
            "resume_from = None\n",
            "workflow = [('train', 1)]\n",
            "num_stages = 6\n",
            "num_proposals = 300\n",
            "model = dict(\n",
            "    type='QueryInst',\n",
            "    pretrained=None,\n",
            "    backbone=dict(\n",
            "        type='SwinTransformer',\n",
            "        embed_dim=192,\n",
            "        depths=[2, 2, 18, 2],\n",
            "        num_heads=[6, 12, 24, 48],\n",
            "        window_size=7,\n",
            "        mlp_ratio=4.0,\n",
            "        qkv_bias=True,\n",
            "        qk_scale=None,\n",
            "        drop_rate=0.0,\n",
            "        attn_drop_rate=0.0,\n",
            "        drop_path_rate=0.3,\n",
            "        ape=False,\n",
            "        patch_norm=True,\n",
            "        out_indices=(0, 1, 2, 3),\n",
            "        use_checkpoint=False),\n",
            "    neck=dict(\n",
            "        type='FPN',\n",
            "        in_channels=[192, 384, 768, 1536],\n",
            "        out_channels=256,\n",
            "        start_level=0,\n",
            "        add_extra_convs='on_input',\n",
            "        num_outs=4),\n",
            "    rpn_head=dict(\n",
            "        type='EmbeddingRPNHead',\n",
            "        num_proposals=300,\n",
            "        proposal_feature_channel=256),\n",
            "    roi_head=dict(\n",
            "        type='QueryRoIHead',\n",
            "        num_stages=6,\n",
            "        stage_loss_weights=[1, 1, 1, 1, 1, 1],\n",
            "        proposal_feature_channel=256,\n",
            "        bbox_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=2),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        mask_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=2),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        bbox_head=[\n",
            "            dict(\n",
            "                type='DIIHead',\n",
            "                num_classes=323,\n",
            "                num_ffn_fcs=2,\n",
            "                num_heads=8,\n",
            "                num_cls_fcs=1,\n",
            "                num_reg_fcs=3,\n",
            "                feedforward_channels=2048,\n",
            "                in_channels=256,\n",
            "                dropout=0.0,\n",
            "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=7,\n",
            "                    with_proj=True,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
            "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
            "                loss_cls=dict(\n",
            "                    type='FocalLoss',\n",
            "                    use_sigmoid=True,\n",
            "                    gamma=2.0,\n",
            "                    alpha=0.25,\n",
            "                    loss_weight=2.0),\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    clip_border=False,\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
            "            dict(\n",
            "                type='DIIHead',\n",
            "                num_classes=323,\n",
            "                num_ffn_fcs=2,\n",
            "                num_heads=8,\n",
            "                num_cls_fcs=1,\n",
            "                num_reg_fcs=3,\n",
            "                feedforward_channels=2048,\n",
            "                in_channels=256,\n",
            "                dropout=0.0,\n",
            "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=7,\n",
            "                    with_proj=True,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
            "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
            "                loss_cls=dict(\n",
            "                    type='FocalLoss',\n",
            "                    use_sigmoid=True,\n",
            "                    gamma=2.0,\n",
            "                    alpha=0.25,\n",
            "                    loss_weight=2.0),\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    clip_border=False,\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
            "            dict(\n",
            "                type='DIIHead',\n",
            "                num_classes=323,\n",
            "                num_ffn_fcs=2,\n",
            "                num_heads=8,\n",
            "                num_cls_fcs=1,\n",
            "                num_reg_fcs=3,\n",
            "                feedforward_channels=2048,\n",
            "                in_channels=256,\n",
            "                dropout=0.0,\n",
            "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=7,\n",
            "                    with_proj=True,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
            "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
            "                loss_cls=dict(\n",
            "                    type='FocalLoss',\n",
            "                    use_sigmoid=True,\n",
            "                    gamma=2.0,\n",
            "                    alpha=0.25,\n",
            "                    loss_weight=2.0),\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    clip_border=False,\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
            "            dict(\n",
            "                type='DIIHead',\n",
            "                num_classes=323,\n",
            "                num_ffn_fcs=2,\n",
            "                num_heads=8,\n",
            "                num_cls_fcs=1,\n",
            "                num_reg_fcs=3,\n",
            "                feedforward_channels=2048,\n",
            "                in_channels=256,\n",
            "                dropout=0.0,\n",
            "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=7,\n",
            "                    with_proj=True,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
            "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
            "                loss_cls=dict(\n",
            "                    type='FocalLoss',\n",
            "                    use_sigmoid=True,\n",
            "                    gamma=2.0,\n",
            "                    alpha=0.25,\n",
            "                    loss_weight=2.0),\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    clip_border=False,\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
            "            dict(\n",
            "                type='DIIHead',\n",
            "                num_classes=323,\n",
            "                num_ffn_fcs=2,\n",
            "                num_heads=8,\n",
            "                num_cls_fcs=1,\n",
            "                num_reg_fcs=3,\n",
            "                feedforward_channels=2048,\n",
            "                in_channels=256,\n",
            "                dropout=0.0,\n",
            "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=7,\n",
            "                    with_proj=True,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
            "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
            "                loss_cls=dict(\n",
            "                    type='FocalLoss',\n",
            "                    use_sigmoid=True,\n",
            "                    gamma=2.0,\n",
            "                    alpha=0.25,\n",
            "                    loss_weight=2.0),\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    clip_border=False,\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
            "            dict(\n",
            "                type='DIIHead',\n",
            "                num_classes=323,\n",
            "                num_ffn_fcs=2,\n",
            "                num_heads=8,\n",
            "                num_cls_fcs=1,\n",
            "                num_reg_fcs=3,\n",
            "                feedforward_channels=2048,\n",
            "                in_channels=256,\n",
            "                dropout=0.0,\n",
            "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=7,\n",
            "                    with_proj=True,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
            "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
            "                loss_cls=dict(\n",
            "                    type='FocalLoss',\n",
            "                    use_sigmoid=True,\n",
            "                    gamma=2.0,\n",
            "                    alpha=0.25,\n",
            "                    loss_weight=2.0),\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    clip_border=False,\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.5, 0.5, 1.0, 1.0]))\n",
            "        ],\n",
            "        mask_head=[\n",
            "            dict(\n",
            "                type='DynamicMaskHead',\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=14,\n",
            "                    with_proj=False,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                dropout=0.0,\n",
            "                num_convs=4,\n",
            "                num_classes=323,\n",
            "                roi_feat_size=14,\n",
            "                in_channels=256,\n",
            "                conv_kernel_size=3,\n",
            "                conv_out_channels=256,\n",
            "                class_agnostic=False,\n",
            "                norm_cfg=dict(type='BN'),\n",
            "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
            "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
            "            dict(\n",
            "                type='DynamicMaskHead',\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=14,\n",
            "                    with_proj=False,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                dropout=0.0,\n",
            "                num_convs=4,\n",
            "                num_classes=323,\n",
            "                roi_feat_size=14,\n",
            "                in_channels=256,\n",
            "                conv_kernel_size=3,\n",
            "                conv_out_channels=256,\n",
            "                class_agnostic=False,\n",
            "                norm_cfg=dict(type='BN'),\n",
            "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
            "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
            "            dict(\n",
            "                type='DynamicMaskHead',\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=14,\n",
            "                    with_proj=False,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                dropout=0.0,\n",
            "                num_convs=4,\n",
            "                num_classes=323,\n",
            "                roi_feat_size=14,\n",
            "                in_channels=256,\n",
            "                conv_kernel_size=3,\n",
            "                conv_out_channels=256,\n",
            "                class_agnostic=False,\n",
            "                norm_cfg=dict(type='BN'),\n",
            "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
            "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
            "            dict(\n",
            "                type='DynamicMaskHead',\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=14,\n",
            "                    with_proj=False,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                dropout=0.0,\n",
            "                num_convs=4,\n",
            "                num_classes=323,\n",
            "                roi_feat_size=14,\n",
            "                in_channels=256,\n",
            "                conv_kernel_size=3,\n",
            "                conv_out_channels=256,\n",
            "                class_agnostic=False,\n",
            "                norm_cfg=dict(type='BN'),\n",
            "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
            "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
            "            dict(\n",
            "                type='DynamicMaskHead',\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=14,\n",
            "                    with_proj=False,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                dropout=0.0,\n",
            "                num_convs=4,\n",
            "                num_classes=323,\n",
            "                roi_feat_size=14,\n",
            "                in_channels=256,\n",
            "                conv_kernel_size=3,\n",
            "                conv_out_channels=256,\n",
            "                class_agnostic=False,\n",
            "                norm_cfg=dict(type='BN'),\n",
            "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
            "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
            "            dict(\n",
            "                type='DynamicMaskHead',\n",
            "                dynamic_conv_cfg=dict(\n",
            "                    type='DynamicConv',\n",
            "                    in_channels=256,\n",
            "                    feat_channels=64,\n",
            "                    out_channels=256,\n",
            "                    input_feat_shape=14,\n",
            "                    with_proj=False,\n",
            "                    act_cfg=dict(type='ReLU', inplace=True),\n",
            "                    norm_cfg=dict(type='LN')),\n",
            "                dropout=0.0,\n",
            "                num_convs=4,\n",
            "                num_classes=323,\n",
            "                roi_feat_size=14,\n",
            "                in_channels=256,\n",
            "                conv_kernel_size=3,\n",
            "                conv_out_channels=256,\n",
            "                class_agnostic=False,\n",
            "                norm_cfg=dict(type='BN'),\n",
            "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
            "                loss_dice=dict(type='DiceLoss', loss_weight=8.0))\n",
            "        ]),\n",
            "    train_cfg=dict(\n",
            "        rpn=None,\n",
            "        rcnn=[\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='HungarianAssigner',\n",
            "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
            "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
            "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
            "                                  weight=2.0)),\n",
            "                sampler=dict(type='PseudoSampler'),\n",
            "                pos_weight=1,\n",
            "                mask_size=28,\n",
            "                debug=False),\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='HungarianAssigner',\n",
            "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
            "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
            "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
            "                                  weight=2.0)),\n",
            "                sampler=dict(type='PseudoSampler'),\n",
            "                pos_weight=1,\n",
            "                mask_size=28,\n",
            "                debug=False),\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='HungarianAssigner',\n",
            "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
            "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
            "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
            "                                  weight=2.0)),\n",
            "                sampler=dict(type='PseudoSampler'),\n",
            "                pos_weight=1,\n",
            "                mask_size=28,\n",
            "                debug=False),\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='HungarianAssigner',\n",
            "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
            "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
            "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
            "                                  weight=2.0)),\n",
            "                sampler=dict(type='PseudoSampler'),\n",
            "                pos_weight=1,\n",
            "                mask_size=28,\n",
            "                debug=False),\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='HungarianAssigner',\n",
            "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
            "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
            "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
            "                                  weight=2.0)),\n",
            "                sampler=dict(type='PseudoSampler'),\n",
            "                pos_weight=1,\n",
            "                mask_size=28,\n",
            "                debug=False),\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='HungarianAssigner',\n",
            "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
            "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
            "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
            "                                  weight=2.0)),\n",
            "                sampler=dict(type='PseudoSampler'),\n",
            "                pos_weight=1,\n",
            "                mask_size=28,\n",
            "                debug=False)\n",
            "        ]),\n",
            "    test_cfg=dict(rpn=None, rcnn=dict(max_per_img=300, mask_thr_binary=0.5)))\n",
            "total_epochs = 22\n",
            "min_values = (480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800)\n",
            "optimizer_config = dict(grad_clip=dict(max_norm=15, norm_type=2))\n",
            "fp16 = dict(loss_scale=512.0)\n",
            "work_dir = '/content/drive/MyDrive/log_mmdetQuery'\n",
            "gpu_ids = range(0, 1)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.7/dist-packages/mmcv/utils/misc.py:334: UserWarning: \"dropout\" is deprecated in `FFN.__init__`, please use \"ffn_drop\" instead\n",
            "  f'\"{src_arg_name}\" is deprecated in '\n",
            "/usr/local/lib/python3.7/dist-packages/mmcv/cnn/bricks/conv_module.py:151: UserWarning: Unnecessary conv bias before batch/instance norm\n",
            "  'Unnecessary conv bias before batch/instance norm')\n",
            "2022-04-06 11:18:34,619 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}\n",
            "2022-04-06 11:18:34,641 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
            "2022-04-06 11:18:34,730 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
            "2022-04-06 11:18:34,821 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
            "2022-04-06 11:18:34,911 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
            "2022-04-06 11:18:35,001 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
            "2022-04-06 11:18:35,093 - mmdet - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
            "loading annotations into memory...\n",
            "Done (t=3.74s)\n",
            "creating index...\n",
            "index created!\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "2022-04-06 11:18:42,865 - mmdet - INFO - load checkpoint from local path: /content/drive/MyDrive/queryinst_swin_large_patch4_window7_fpn_300_queries-832c5813.pth\n",
            "2022-04-06 11:18:44,674 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for roi_head.bbox_head.0.fc_cls.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([323, 256]).\n",
            "size mismatch for roi_head.bbox_head.0.fc_cls.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.bbox_head.1.fc_cls.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([323, 256]).\n",
            "size mismatch for roi_head.bbox_head.1.fc_cls.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.bbox_head.2.fc_cls.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([323, 256]).\n",
            "size mismatch for roi_head.bbox_head.2.fc_cls.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.bbox_head.3.fc_cls.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([323, 256]).\n",
            "size mismatch for roi_head.bbox_head.3.fc_cls.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.bbox_head.4.fc_cls.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([323, 256]).\n",
            "size mismatch for roi_head.bbox_head.4.fc_cls.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.bbox_head.5.fc_cls.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([323, 256]).\n",
            "size mismatch for roi_head.bbox_head.5.fc_cls.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.mask_head.0.conv_logits.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([323, 256, 1, 1]).\n",
            "size mismatch for roi_head.mask_head.0.conv_logits.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.mask_head.1.conv_logits.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([323, 256, 1, 1]).\n",
            "size mismatch for roi_head.mask_head.1.conv_logits.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.mask_head.2.conv_logits.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([323, 256, 1, 1]).\n",
            "size mismatch for roi_head.mask_head.2.conv_logits.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.mask_head.3.conv_logits.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([323, 256, 1, 1]).\n",
            "size mismatch for roi_head.mask_head.3.conv_logits.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.mask_head.4.conv_logits.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([323, 256, 1, 1]).\n",
            "size mismatch for roi_head.mask_head.4.conv_logits.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "size mismatch for roi_head.mask_head.5.conv_logits.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([323, 256, 1, 1]).\n",
            "size mismatch for roi_head.mask_head.5.conv_logits.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([323]).\n",
            "2022-04-06 11:18:44,702 - mmdet - INFO - Start running, host: root@a12a339d4daf, work_dir: /content/drive/MyDrive/log_mmdetQuery\n",
            "2022-04-06 11:18:44,702 - mmdet - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(ABOVE_NORMAL) Fp16OptimizerHook                  \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(NORMAL      ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(NORMAL      ) EvalHook                           \n",
            "(NORMAL      ) NumClassCheckHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(ABOVE_NORMAL) Fp16OptimizerHook                  \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(NORMAL      ) EvalHook                           \n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(NORMAL      ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) NumClassCheckHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_run:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "2022-04-06 11:18:44,703 - mmdet - INFO - workflow: [('train', 1)], max: 22 epochs\n",
            "2022-04-06 11:18:44,707 - mmdet - INFO - Checkpoints will be saved to /content/drive/MyDrive/log_mmdetQuery by HardDiskBackend.\n",
            "2022-04-06 11:19:36,583 - mmdet - INFO - Epoch [1][50/54392]\tlr: 1.249e-06, eta: 14 days, 8:45:01, time: 1.037, data_time: 0.051, memory: 14602, stage0_loss_cls: 2.6991, stage0_pos_acc: 0.0000, stage0_loss_bbox: 1.0663, stage0_loss_iou: 0.6238, stage0_loss_mask: 3.6384, stage1_loss_cls: 2.6229, stage1_pos_acc: 0.0000, stage1_loss_bbox: 1.1157, stage1_loss_iou: 0.6400, stage1_loss_mask: 3.2590, stage2_loss_cls: 2.7503, stage2_pos_acc: 0.0000, stage2_loss_bbox: 1.0948, stage2_loss_iou: 0.6269, stage2_loss_mask: 3.8674, stage3_loss_cls: 2.8019, stage3_pos_acc: 0.0000, stage3_loss_bbox: 1.0809, stage3_loss_iou: 0.6315, stage3_loss_mask: 3.3391, stage4_loss_cls: 2.5647, stage4_pos_acc: 2.0000, stage4_loss_bbox: 1.1262, stage4_loss_iou: 0.6479, stage4_loss_mask: 3.3779, stage5_loss_cls: 2.6559, stage5_pos_acc: 2.0000, stage5_loss_bbox: 1.1320, stage5_loss_iou: 0.6442, stage5_loss_mask: 3.5745, loss: 47.5812, grad_norm: nan\n",
            "2022-04-06 11:20:30,896 - mmdet - INFO - Epoch [1][100/54392]\tlr: 2.498e-06, eta: 14 days, 16:53:10, time: 1.086, data_time: 0.006, memory: 14602, stage0_loss_cls: 2.7664, stage0_pos_acc: 0.0000, stage0_loss_bbox: 1.1740, stage0_loss_iou: 0.6637, stage0_loss_mask: 4.1573, stage1_loss_cls: 2.6402, stage1_pos_acc: 0.0000, stage1_loss_bbox: 1.1294, stage1_loss_iou: 0.6516, stage1_loss_mask: 4.0408, stage2_loss_cls: 2.7162, stage2_pos_acc: 0.6667, stage2_loss_bbox: 1.1657, stage2_loss_iou: 0.6585, stage2_loss_mask: 4.0040, stage3_loss_cls: 2.8194, stage3_pos_acc: 0.0000, stage3_loss_bbox: 1.1290, stage3_loss_iou: 0.6632, stage3_loss_mask: 3.9996, stage4_loss_cls: 2.6169, stage4_pos_acc: 1.0000, stage4_loss_bbox: 1.1498, stage4_loss_iou: 0.6595, stage4_loss_mask: 3.7178, stage5_loss_cls: 2.6928, stage5_pos_acc: 0.0000, stage5_loss_bbox: 1.1805, stage5_loss_iou: 0.6697, stage5_loss_mask: 3.8864, loss: 50.9525, grad_norm: nan\n",
            "2022-04-06 11:21:22,096 - mmdet - INFO - Epoch [1][150/54392]\tlr: 3.746e-06, eta: 14 days, 12:41:25, time: 1.024, data_time: 0.006, memory: 14707, stage0_loss_cls: 2.6441, stage0_pos_acc: 2.0000, stage0_loss_bbox: 1.0899, stage0_loss_iou: 0.6389, stage0_loss_mask: 3.6808, stage1_loss_cls: 2.5763, stage1_pos_acc: 0.0000, stage1_loss_bbox: 1.0060, stage1_loss_iou: 0.5847, stage1_loss_mask: 3.7113, stage2_loss_cls: 2.6298, stage2_pos_acc: 4.0000, stage2_loss_bbox: 1.0291, stage2_loss_iou: 0.6052, stage2_loss_mask: 3.9295, stage3_loss_cls: 2.7343, stage3_pos_acc: 0.0000, stage3_loss_bbox: 1.1167, stage3_loss_iou: 0.6431, stage3_loss_mask: 3.8587, stage4_loss_cls: 2.5948, stage4_pos_acc: 0.5000, stage4_loss_bbox: 1.1234, stage4_loss_iou: 0.6491, stage4_loss_mask: 3.8253, stage5_loss_cls: 2.6178, stage5_pos_acc: 0.0000, stage5_loss_bbox: 1.1501, stage5_loss_iou: 0.6623, stage5_loss_mask: 3.7698, loss: 48.8709, grad_norm: 223.3803\n",
            "2022-04-06 11:22:11,213 - mmdet - INFO - Epoch [1][200/54392]\tlr: 4.995e-06, eta: 14 days, 7:07:27, time: 0.982, data_time: 0.008, memory: 14707, stage0_loss_cls: 2.6311, stage0_pos_acc: 0.0000, stage0_loss_bbox: 1.0938, stage0_loss_iou: 0.5801, stage0_loss_mask: 3.4977, stage1_loss_cls: 2.5380, stage1_pos_acc: 0.0000, stage1_loss_bbox: 1.0252, stage1_loss_iou: 0.5603, stage1_loss_mask: 3.4991, stage2_loss_cls: 2.6752, stage2_pos_acc: 0.0000, stage2_loss_bbox: 1.0203, stage2_loss_iou: 0.5522, stage2_loss_mask: 3.4504, stage3_loss_cls: 2.6192, stage3_pos_acc: 0.0000, stage3_loss_bbox: 1.0166, stage3_loss_iou: 0.5493, stage3_loss_mask: 3.3635, stage4_loss_cls: 2.5628, stage4_pos_acc: 0.0000, stage4_loss_bbox: 1.0438, stage4_loss_iou: 0.5668, stage4_loss_mask: 3.3585, stage5_loss_cls: 2.5713, stage5_pos_acc: 0.0000, stage5_loss_bbox: 1.0703, stage5_loss_iou: 0.5721, stage5_loss_mask: 3.4797, loss: 45.8974, grad_norm: nan\n",
            "2022-04-06 11:23:04,092 - mmdet - INFO - Epoch [1][250/54392]\tlr: 6.244e-06, eta: 14 days, 8:46:45, time: 1.058, data_time: 0.007, memory: 14707, stage0_loss_cls: 2.6188, stage0_pos_acc: 0.0000, stage0_loss_bbox: 0.9454, stage0_loss_iou: 0.5312, stage0_loss_mask: 3.4680, stage1_loss_cls: 2.5979, stage1_pos_acc: 0.0000, stage1_loss_bbox: 0.8774, stage1_loss_iou: 0.5323, stage1_loss_mask: 3.4545, stage2_loss_cls: 2.5941, stage2_pos_acc: 0.0000, stage2_loss_bbox: 0.9068, stage2_loss_iou: 0.5278, stage2_loss_mask: 3.4842, stage3_loss_cls: 2.6362, stage3_pos_acc: 0.0000, stage3_loss_bbox: 0.9388, stage3_loss_iou: 0.5448, stage3_loss_mask: 3.1177, stage4_loss_cls: 2.5559, stage4_pos_acc: 2.0000, stage4_loss_bbox: 0.9379, stage4_loss_iou: 0.5381, stage4_loss_mask: 3.1893, stage5_loss_cls: 2.4794, stage5_pos_acc: 2.0000, stage5_loss_bbox: 0.9521, stage5_loss_iou: 0.5553, stage5_loss_mask: 3.4111, loss: 44.3952, grad_norm: nan\n",
            "2022-04-06 11:23:54,972 - mmdet - INFO - Epoch [1][300/54392]\tlr: 7.493e-06, eta: 14 days, 7:39:47, time: 1.018, data_time: 0.006, memory: 14707, stage0_loss_cls: 2.6253, stage0_pos_acc: 2.0000, stage0_loss_bbox: 1.0333, stage0_loss_iou: 0.6126, stage0_loss_mask: 3.1560, stage1_loss_cls: 2.5326, stage1_pos_acc: 2.0000, stage1_loss_bbox: 0.9270, stage1_loss_iou: 0.5650, stage1_loss_mask: 3.4108, stage2_loss_cls: 2.5702, stage2_pos_acc: 1.0000, stage2_loss_bbox: 0.9156, stage2_loss_iou: 0.5473, stage2_loss_mask: 3.4158, stage3_loss_cls: 2.5316, stage3_pos_acc: 0.0000, stage3_loss_bbox: 0.8732, stage3_loss_iou: 0.5318, stage3_loss_mask: 3.2460, stage4_loss_cls: 2.5346, stage4_pos_acc: 0.0000, stage4_loss_bbox: 0.9100, stage4_loss_iou: 0.5472, stage4_loss_mask: 3.0973, stage5_loss_cls: 2.5590, stage5_pos_acc: 0.0000, stage5_loss_bbox: 0.9019, stage5_loss_iou: 0.5444, stage5_loss_mask: 3.4894, loss: 44.0778, grad_norm: nan\n",
            "2022-04-06 11:24:51,645 - mmdet - INFO - Epoch [1][350/54392]\tlr: 8.741e-06, eta: 14 days, 12:21:46, time: 1.133, data_time: 0.007, memory: 14707, stage0_loss_cls: 2.5067, stage0_pos_acc: 1.0000, stage0_loss_bbox: 1.0357, stage0_loss_iou: 0.6341, stage0_loss_mask: 3.4047, stage1_loss_cls: 2.5013, stage1_pos_acc: 0.0000, stage1_loss_bbox: 0.9514, stage1_loss_iou: 0.5855, stage1_loss_mask: 3.3010, stage2_loss_cls: 2.5129, stage2_pos_acc: 3.6667, stage2_loss_bbox: 0.9132, stage2_loss_iou: 0.5637, stage2_loss_mask: 3.2432, stage3_loss_cls: 2.5895, stage3_pos_acc: 0.0000, stage3_loss_bbox: 0.8980, stage3_loss_iou: 0.5543, stage3_loss_mask: 3.6282, stage4_loss_cls: 2.5582, stage4_pos_acc: 0.0000, stage4_loss_bbox: 0.9223, stage4_loss_iou: 0.5579, stage4_loss_mask: 3.2049, stage5_loss_cls: 2.4834, stage5_pos_acc: 0.5000, stage5_loss_bbox: 0.8651, stage5_loss_iou: 0.5284, stage5_loss_mask: 3.5081, loss: 44.4518, grad_norm: nan\n",
            "2022-04-06 11:25:42,837 - mmdet - INFO - Epoch [1][400/54392]\tlr: 9.990e-06, eta: 14 days, 11:19:48, time: 1.024, data_time: 0.007, memory: 14707, stage0_loss_cls: 2.4871, stage0_pos_acc: 2.0000, stage0_loss_bbox: 0.9930, stage0_loss_iou: 0.5779, stage0_loss_mask: 3.4693, stage1_loss_cls: 2.4809, stage1_pos_acc: 0.4000, stage1_loss_bbox: 0.8811, stage1_loss_iou: 0.5199, stage1_loss_mask: 3.4644, stage2_loss_cls: 2.4242, stage2_pos_acc: 3.0000, stage2_loss_bbox: 0.8819, stage2_loss_iou: 0.5231, stage2_loss_mask: 3.5234, stage3_loss_cls: 2.5256, stage3_pos_acc: 0.0000, stage3_loss_bbox: 0.9052, stage3_loss_iou: 0.5321, stage3_loss_mask: 3.3471, stage4_loss_cls: 2.4670, stage4_pos_acc: 0.0000, stage4_loss_bbox: 0.8861, stage4_loss_iou: 0.5181, stage4_loss_mask: 3.3778, stage5_loss_cls: 2.4744, stage5_pos_acc: 0.7500, stage5_loss_bbox: 0.8979, stage5_loss_iou: 0.5188, stage5_loss_mask: 3.4407, loss: 44.1173, grad_norm: nan\n",
            "2022-04-06 11:26:30,106 - mmdet - INFO - Epoch [1][450/54392]\tlr: 1.124e-05, eta: 14 days, 7:37:38, time: 0.945, data_time: 0.007, memory: 14707, stage0_loss_cls: 2.5693, stage0_pos_acc: 0.0000, stage0_loss_bbox: 0.9654, stage0_loss_iou: 0.5875, stage0_loss_mask: 3.9739, stage1_loss_cls: 2.4817, stage1_pos_acc: 0.4000, stage1_loss_bbox: 0.9606, stage1_loss_iou: 0.5809, stage1_loss_mask: 4.0959, stage2_loss_cls: 2.4643, stage2_pos_acc: 2.0000, stage2_loss_bbox: 0.9105, stage2_loss_iou: 0.5618, stage2_loss_mask: 3.8758, stage3_loss_cls: 2.5091, stage3_pos_acc: 1.0000, stage3_loss_bbox: 0.8914, stage3_loss_iou: 0.5474, stage3_loss_mask: 3.9806, stage4_loss_cls: 2.4470, stage4_pos_acc: 0.0000, stage4_loss_bbox: 0.9026, stage4_loss_iou: 0.5513, stage4_loss_mask: 4.0977, stage5_loss_cls: 2.4740, stage5_pos_acc: 0.0000, stage5_loss_bbox: 0.8897, stage5_loss_iou: 0.5374, stage5_loss_mask: 3.9938, loss: 47.8495, grad_norm: nan\n",
            "2022-04-06 11:27:24,431 - mmdet - INFO - Epoch [1][500/54392]\tlr: 1.249e-05, eta: 14 days, 9:21:04, time: 1.086, data_time: 0.007, memory: 14707, stage0_loss_cls: 2.4121, stage0_pos_acc: 0.5000, stage0_loss_bbox: 1.0264, stage0_loss_iou: 0.6132, stage0_loss_mask: 3.4430, stage1_loss_cls: 2.4738, stage1_pos_acc: 0.6667, stage1_loss_bbox: 0.9245, stage1_loss_iou: 0.5582, stage1_loss_mask: 3.4637, stage2_loss_cls: 2.4153, stage2_pos_acc: 4.0000, stage2_loss_bbox: 0.9121, stage2_loss_iou: 0.5449, stage2_loss_mask: 3.6861, stage3_loss_cls: 2.4464, stage3_pos_acc: 0.0000, stage3_loss_bbox: 0.8857, stage3_loss_iou: 0.5330, stage3_loss_mask: 3.3702, stage4_loss_cls: 2.3606, stage4_pos_acc: 4.0000, stage4_loss_bbox: 0.8595, stage4_loss_iou: 0.5339, stage4_loss_mask: 3.4948, stage5_loss_cls: 2.4355, stage5_pos_acc: 6.0000, stage5_loss_bbox: 0.8764, stage5_loss_iou: 0.5411, stage5_loss_mask: 3.4631, loss: 44.2735, grad_norm: nan\n",
            "2022-04-06 11:28:11,656 - mmdet - INFO - Epoch [1][550/54392]\tlr: 1.374e-05, eta: 14 days, 6:28:11, time: 0.944, data_time: 0.006, memory: 14707, stage0_loss_cls: 2.5122, stage0_pos_acc: 0.0000, stage0_loss_bbox: 0.9430, stage0_loss_iou: 0.5659, stage0_loss_mask: 3.2214, stage1_loss_cls: 2.4484, stage1_pos_acc: 4.0000, stage1_loss_bbox: 0.8798, stage1_loss_iou: 0.5130, stage1_loss_mask: 3.1808, stage2_loss_cls: 2.4189, stage2_pos_acc: 11.0000, stage2_loss_bbox: 0.8995, stage2_loss_iou: 0.5278, stage2_loss_mask: 3.2684, stage3_loss_cls: 2.3873, stage3_pos_acc: 4.5000, stage3_loss_bbox: 0.9234, stage3_loss_iou: 0.5224, stage3_loss_mask: 3.2045, stage4_loss_cls: 2.4027, stage4_pos_acc: 2.0000, stage4_loss_bbox: 0.8807, stage4_loss_iou: 0.5064, stage4_loss_mask: 3.2350, stage5_loss_cls: 2.3961, stage5_pos_acc: 4.0000, stage5_loss_bbox: 0.8790, stage5_loss_iou: 0.5050, stage5_loss_mask: 3.3679, loss: 42.5895, grad_norm: nan\n",
            "2022-04-06 11:29:06,731 - mmdet - INFO - Epoch [1][600/54392]\tlr: 1.499e-05, eta: 14 days, 8:24:47, time: 1.101, data_time: 0.008, memory: 14707, stage0_loss_cls: 2.4115, stage0_pos_acc: 1.3333, stage0_loss_bbox: 0.9446, stage0_loss_iou: 0.5535, stage0_loss_mask: 3.6994, stage1_loss_cls: 2.3872, stage1_pos_acc: 0.0000, stage1_loss_bbox: 0.8031, stage1_loss_iou: 0.4820, stage1_loss_mask: 3.6869, stage2_loss_cls: 2.4184, stage2_pos_acc: 9.7500, stage2_loss_bbox: 0.8708, stage2_loss_iou: 0.5103, stage2_loss_mask: 3.6560, stage3_loss_cls: 2.4316, stage3_pos_acc: 0.5000, stage3_loss_bbox: 0.7531, stage3_loss_iou: 0.4535, stage3_loss_mask: 3.6555, stage4_loss_cls: 2.3601, stage4_pos_acc: 4.2500, stage4_loss_bbox: 0.7940, stage4_loss_iou: 0.4727, stage4_loss_mask: 3.6023, stage5_loss_cls: 2.3432, stage5_pos_acc: 8.0000, stage5_loss_bbox: 0.7760, stage5_loss_iou: 0.4682, stage5_loss_mask: 3.6941, loss: 44.2281, grad_norm: nan\n",
            "2022-04-06 11:30:02,938 - mmdet - INFO - Epoch [1][650/54392]\tlr: 1.623e-05, eta: 14 days, 10:38:01, time: 1.124, data_time: 0.008, memory: 14707, stage0_loss_cls: 2.4264, stage0_pos_acc: 0.0000, stage0_loss_bbox: 0.8238, stage0_loss_iou: 0.4853, stage0_loss_mask: 3.2596, stage1_loss_cls: 2.4292, stage1_pos_acc: 0.5000, stage1_loss_bbox: 0.7549, stage1_loss_iou: 0.4420, stage1_loss_mask: 3.3195, stage2_loss_cls: 2.4574, stage2_pos_acc: 6.0000, stage2_loss_bbox: 0.7223, stage2_loss_iou: 0.4362, stage2_loss_mask: 3.2188, stage3_loss_cls: 2.4354, stage3_pos_acc: 1.0000, stage3_loss_bbox: 0.7127, stage3_loss_iou: 0.4175, stage3_loss_mask: 3.4772, stage4_loss_cls: 2.3515, stage4_pos_acc: 0.0000, stage4_loss_bbox: 0.6841, stage4_loss_iou: 0.4027, stage4_loss_mask: 3.4033, stage5_loss_cls: 2.3569, stage5_pos_acc: 6.0000, stage5_loss_bbox: 0.7025, stage5_loss_iou: 0.4084, stage5_loss_mask: 3.3908, loss: 41.5182, grad_norm: nan\n",
            "2022-04-06 11:30:52,852 - mmdet - INFO - Epoch [1][700/54392]\tlr: 1.748e-05, eta: 14 days, 9:32:54, time: 0.998, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.4427, stage0_pos_acc: 0.0000, stage0_loss_bbox: 0.8250, stage0_loss_iou: 0.4987, stage0_loss_mask: 3.2929, stage1_loss_cls: 2.4180, stage1_pos_acc: 4.0000, stage1_loss_bbox: 0.8047, stage1_loss_iou: 0.4954, stage1_loss_mask: 3.4400, stage2_loss_cls: 2.3909, stage2_pos_acc: 6.0000, stage2_loss_bbox: 0.7804, stage2_loss_iou: 0.4778, stage2_loss_mask: 3.3580, stage3_loss_cls: 2.3803, stage3_pos_acc: 1.0000, stage3_loss_bbox: 0.7273, stage3_loss_iou: 0.4546, stage3_loss_mask: 3.3506, stage4_loss_cls: 2.3686, stage4_pos_acc: 7.0000, stage4_loss_bbox: 0.7340, stage4_loss_iou: 0.4591, stage4_loss_mask: 3.3557, stage5_loss_cls: 2.3931, stage5_pos_acc: 2.0000, stage5_loss_bbox: 0.7387, stage5_loss_iou: 0.4589, stage5_loss_mask: 3.1349, loss: 41.7804, grad_norm: nan\n",
            "2022-04-06 11:31:48,539 - mmdet - INFO - Epoch [1][750/54392]\tlr: 1.873e-05, eta: 14 days, 11:09:47, time: 1.114, data_time: 0.010, memory: 14843, stage0_loss_cls: 2.3604, stage0_pos_acc: 2.0000, stage0_loss_bbox: 0.7875, stage0_loss_iou: 0.5114, stage0_loss_mask: 3.4507, stage1_loss_cls: 2.3043, stage1_pos_acc: 5.5000, stage1_loss_bbox: 0.7500, stage1_loss_iou: 0.4870, stage1_loss_mask: 3.4565, stage2_loss_cls: 2.4024, stage2_pos_acc: 4.0667, stage2_loss_bbox: 0.8169, stage2_loss_iou: 0.5264, stage2_loss_mask: 3.4221, stage3_loss_cls: 2.3055, stage3_pos_acc: 2.0000, stage3_loss_bbox: 0.8117, stage3_loss_iou: 0.5165, stage3_loss_mask: 3.5918, stage4_loss_cls: 2.2696, stage4_pos_acc: 5.5667, stage4_loss_bbox: 0.8462, stage4_loss_iou: 0.5401, stage4_loss_mask: 3.3801, stage5_loss_cls: 2.2332, stage5_pos_acc: 5.2000, stage5_loss_bbox: 0.8551, stage5_loss_iou: 0.5384, stage5_loss_mask: 3.3248, loss: 42.4885, grad_norm: nan\n",
            "2022-04-06 11:32:43,440 - mmdet - INFO - Epoch [1][800/54392]\tlr: 1.998e-05, eta: 14 days, 12:14:50, time: 1.098, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.4146, stage0_pos_acc: 0.0000, stage0_loss_bbox: 0.6813, stage0_loss_iou: 0.4167, stage0_loss_mask: 3.5911, stage1_loss_cls: 2.3653, stage1_pos_acc: 2.6667, stage1_loss_bbox: 0.6982, stage1_loss_iou: 0.4300, stage1_loss_mask: 3.4359, stage2_loss_cls: 2.3806, stage2_pos_acc: 5.0000, stage2_loss_bbox: 0.7163, stage2_loss_iou: 0.4375, stage2_loss_mask: 3.4480, stage3_loss_cls: 2.3417, stage3_pos_acc: 2.6667, stage3_loss_bbox: 0.6842, stage3_loss_iou: 0.4171, stage3_loss_mask: 3.4902, stage4_loss_cls: 2.2661, stage4_pos_acc: 8.6667, stage4_loss_bbox: 0.7022, stage4_loss_iou: 0.4319, stage4_loss_mask: 3.5072, stage5_loss_cls: 2.3185, stage5_pos_acc: 5.4000, stage5_loss_bbox: 0.7030, stage5_loss_iou: 0.4296, stage5_loss_mask: 3.4424, loss: 41.7496, grad_norm: 156.9464\n",
            "2022-04-06 11:33:38,437 - mmdet - INFO - Epoch [1][850/54392]\tlr: 2.123e-05, eta: 14 days, 13:14:23, time: 1.100, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.3745, stage0_pos_acc: 1.0000, stage0_loss_bbox: 0.7336, stage0_loss_iou: 0.4510, stage0_loss_mask: 3.3784, stage1_loss_cls: 2.3482, stage1_pos_acc: 2.0000, stage1_loss_bbox: 0.7227, stage1_loss_iou: 0.4383, stage1_loss_mask: 3.3417, stage2_loss_cls: 2.3692, stage2_pos_acc: 1.7143, stage2_loss_bbox: 0.7404, stage2_loss_iou: 0.4381, stage2_loss_mask: 3.3581, stage3_loss_cls: 2.3616, stage3_pos_acc: 2.2857, stage3_loss_bbox: 0.7205, stage3_loss_iou: 0.4281, stage3_loss_mask: 3.3351, stage4_loss_cls: 2.3638, stage4_pos_acc: 3.0000, stage4_loss_bbox: 0.7054, stage4_loss_iou: 0.4184, stage4_loss_mask: 3.1428, stage5_loss_cls: 2.3315, stage5_pos_acc: 6.0000, stage5_loss_bbox: 0.7370, stage5_loss_iou: 0.4371, stage5_loss_mask: 3.3353, loss: 41.0107, grad_norm: 176.0289\n",
            "2022-04-06 11:34:27,441 - mmdet - INFO - Epoch [1][900/54392]\tlr: 2.248e-05, eta: 14 days, 11:54:31, time: 0.980, data_time: 0.006, memory: 14843, stage0_loss_cls: 2.3035, stage0_pos_acc: 8.0000, stage0_loss_bbox: 0.7162, stage0_loss_iou: 0.4552, stage0_loss_mask: 3.0061, stage1_loss_cls: 2.2942, stage1_pos_acc: 0.0000, stage1_loss_bbox: 0.7215, stage1_loss_iou: 0.4712, stage1_loss_mask: 3.0357, stage2_loss_cls: 2.3474, stage2_pos_acc: 8.0000, stage2_loss_bbox: 0.7086, stage2_loss_iou: 0.4626, stage2_loss_mask: 3.1067, stage3_loss_cls: 2.3962, stage3_pos_acc: 3.0000, stage3_loss_bbox: 0.7236, stage3_loss_iou: 0.4615, stage3_loss_mask: 3.3468, stage4_loss_cls: 2.2431, stage4_pos_acc: 13.0000, stage4_loss_bbox: 0.7569, stage4_loss_iou: 0.4809, stage4_loss_mask: 3.0756, stage5_loss_cls: 2.2252, stage5_pos_acc: 11.6667, stage5_loss_bbox: 0.7447, stage5_loss_iou: 0.4667, stage5_loss_mask: 3.0829, loss: 39.6330, grad_norm: 176.0515\n",
            "2022-04-06 11:35:23,210 - mmdet - INFO - Epoch [1][950/54392]\tlr: 2.373e-05, eta: 14 days, 13:04:53, time: 1.115, data_time: 0.009, memory: 14843, stage0_loss_cls: 2.2280, stage0_pos_acc: 6.0000, stage0_loss_bbox: 0.7758, stage0_loss_iou: 0.4985, stage0_loss_mask: 3.3954, stage1_loss_cls: 2.1816, stage1_pos_acc: 4.4000, stage1_loss_bbox: 0.7787, stage1_loss_iou: 0.4834, stage1_loss_mask: 3.2346, stage2_loss_cls: 2.2444, stage2_pos_acc: 7.6667, stage2_loss_bbox: 0.7455, stage2_loss_iou: 0.4781, stage2_loss_mask: 3.4982, stage3_loss_cls: 2.1937, stage3_pos_acc: 7.6667, stage3_loss_bbox: 0.7735, stage3_loss_iou: 0.5001, stage3_loss_mask: 3.3560, stage4_loss_cls: 2.0990, stage4_pos_acc: 16.0000, stage4_loss_bbox: 0.8005, stage4_loss_iou: 0.5092, stage4_loss_mask: 3.2860, stage5_loss_cls: 2.0880, stage5_pos_acc: 14.2857, stage5_loss_bbox: 0.8323, stage5_loss_iou: 0.5231, stage5_loss_mask: 3.1797, loss: 40.6832, grad_norm: 145.0136\n",
            "2022-04-06 11:36:13,051 - mmdet - INFO - Exp name: query_large.py\n",
            "2022-04-06 11:36:13,052 - mmdet - INFO - Epoch [1][1000/54392]\tlr: 2.498e-05, eta: 14 days, 12:09:59, time: 0.997, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.3769, stage0_pos_acc: 3.0000, stage0_loss_bbox: 0.7116, stage0_loss_iou: 0.4490, stage0_loss_mask: 3.2828, stage1_loss_cls: 2.2205, stage1_pos_acc: 5.7857, stage1_loss_bbox: 0.7089, stage1_loss_iou: 0.4355, stage1_loss_mask: 3.2154, stage2_loss_cls: 2.3056, stage2_pos_acc: 4.6667, stage2_loss_bbox: 0.6903, stage2_loss_iou: 0.4347, stage2_loss_mask: 3.2881, stage3_loss_cls: 2.3322, stage3_pos_acc: 7.0000, stage3_loss_bbox: 0.7037, stage3_loss_iou: 0.4424, stage3_loss_mask: 3.2462, stage4_loss_cls: 2.2865, stage4_pos_acc: 7.2857, stage4_loss_bbox: 0.7203, stage4_loss_iou: 0.4559, stage4_loss_mask: 2.9931, stage5_loss_cls: 2.2033, stage5_pos_acc: 10.0000, stage5_loss_bbox: 0.7523, stage5_loss_iou: 0.4658, stage5_loss_mask: 3.0585, loss: 39.7795, grad_norm: 142.7418\n",
            "2022-04-06 11:37:05,672 - mmdet - INFO - Epoch [1][1050/54392]\tlr: 2.500e-05, eta: 14 days, 12:12:59, time: 1.052, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.3191, stage0_pos_acc: 3.0000, stage0_loss_bbox: 0.6866, stage0_loss_iou: 0.4282, stage0_loss_mask: 3.0243, stage1_loss_cls: 2.2877, stage1_pos_acc: 4.0000, stage1_loss_bbox: 0.6583, stage1_loss_iou: 0.4164, stage1_loss_mask: 3.0659, stage2_loss_cls: 2.3094, stage2_pos_acc: 4.0000, stage2_loss_bbox: 0.6396, stage2_loss_iou: 0.4008, stage2_loss_mask: 3.1294, stage3_loss_cls: 2.3483, stage3_pos_acc: 3.0000, stage3_loss_bbox: 0.6768, stage3_loss_iou: 0.4185, stage3_loss_mask: 3.1603, stage4_loss_cls: 2.3003, stage4_pos_acc: 2.6667, stage4_loss_bbox: 0.7229, stage4_loss_iou: 0.4372, stage4_loss_mask: 3.0573, stage5_loss_cls: 2.2457, stage5_pos_acc: 6.0000, stage5_loss_bbox: 0.7140, stage5_loss_iou: 0.4298, stage5_loss_mask: 3.0671, loss: 38.9439, grad_norm: 147.0202\n",
            "2022-04-06 11:38:00,724 - mmdet - INFO - Epoch [1][1100/54392]\tlr: 2.500e-05, eta: 14 days, 12:59:41, time: 1.101, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.3383, stage0_pos_acc: 7.0000, stage0_loss_bbox: 0.8407, stage0_loss_iou: 0.5018, stage0_loss_mask: 3.6738, stage1_loss_cls: 2.1577, stage1_pos_acc: 11.0000, stage1_loss_bbox: 0.7860, stage1_loss_iou: 0.4717, stage1_loss_mask: 3.5731, stage2_loss_cls: 2.2281, stage2_pos_acc: 5.0000, stage2_loss_bbox: 0.7827, stage2_loss_iou: 0.4769, stage2_loss_mask: 3.5882, stage3_loss_cls: 2.3019, stage3_pos_acc: 7.0000, stage3_loss_bbox: 0.8078, stage3_loss_iou: 0.4916, stage3_loss_mask: 3.6592, stage4_loss_cls: 2.2387, stage4_pos_acc: 8.4000, stage4_loss_bbox: 0.7938, stage4_loss_iou: 0.4884, stage4_loss_mask: 3.7195, stage5_loss_cls: 2.2419, stage5_pos_acc: 8.0000, stage5_loss_bbox: 0.7990, stage5_loss_iou: 0.4832, stage5_loss_mask: 3.6811, loss: 43.1252, grad_norm: 136.6594\n",
            "2022-04-06 11:38:56,346 - mmdet - INFO - Epoch [1][1150/54392]\tlr: 2.500e-05, eta: 14 days, 13:52:06, time: 1.112, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.3249, stage0_pos_acc: 1.3333, stage0_loss_bbox: 0.8199, stage0_loss_iou: 0.5008, stage0_loss_mask: 3.5940, stage1_loss_cls: 2.2997, stage1_pos_acc: 8.0000, stage1_loss_bbox: 0.7052, stage1_loss_iou: 0.4357, stage1_loss_mask: 3.6057, stage2_loss_cls: 2.2855, stage2_pos_acc: 6.0000, stage2_loss_bbox: 0.6896, stage2_loss_iou: 0.4262, stage2_loss_mask: 3.6296, stage3_loss_cls: 2.2508, stage3_pos_acc: 11.3333, stage3_loss_bbox: 0.6875, stage3_loss_iou: 0.4223, stage3_loss_mask: 3.6466, stage4_loss_cls: 2.3179, stage4_pos_acc: 2.0000, stage4_loss_bbox: 0.7053, stage4_loss_iou: 0.4340, stage4_loss_mask: 3.5292, stage5_loss_cls: 2.2674, stage5_pos_acc: 2.0000, stage5_loss_bbox: 0.7177, stage5_loss_iou: 0.4341, stage5_loss_mask: 3.5641, loss: 42.2937, grad_norm: 137.7104\n",
            "2022-04-06 11:39:48,252 - mmdet - INFO - Epoch [1][1200/54392]\tlr: 2.500e-05, eta: 14 days, 13:38:23, time: 1.038, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.2299, stage0_pos_acc: 13.0000, stage0_loss_bbox: 0.8054, stage0_loss_iou: 0.4822, stage0_loss_mask: 3.1867, stage1_loss_cls: 2.2294, stage1_pos_acc: 12.5000, stage1_loss_bbox: 0.7706, stage1_loss_iou: 0.4603, stage1_loss_mask: 3.1210, stage2_loss_cls: 2.2540, stage2_pos_acc: 17.1667, stage2_loss_bbox: 0.7542, stage2_loss_iou: 0.4553, stage2_loss_mask: 3.2714, stage3_loss_cls: 2.3002, stage3_pos_acc: 9.0000, stage3_loss_bbox: 0.7537, stage3_loss_iou: 0.4538, stage3_loss_mask: 3.2614, stage4_loss_cls: 2.1737, stage4_pos_acc: 19.1667, stage4_loss_bbox: 0.7846, stage4_loss_iou: 0.4669, stage4_loss_mask: 3.1768, stage5_loss_cls: 2.1544, stage5_pos_acc: 11.0000, stage5_loss_bbox: 0.8261, stage5_loss_iou: 0.4884, stage5_loss_mask: 3.1185, loss: 39.9788, grad_norm: 159.7048\n",
            "2022-04-06 11:40:43,586 - mmdet - INFO - Epoch [1][1250/54392]\tlr: 2.500e-05, eta: 14 days, 14:20:20, time: 1.107, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.2345, stage0_pos_acc: 10.0000, stage0_loss_bbox: 0.7121, stage0_loss_iou: 0.4605, stage0_loss_mask: 3.2724, stage1_loss_cls: 2.2732, stage1_pos_acc: 9.0000, stage1_loss_bbox: 0.7199, stage1_loss_iou: 0.4589, stage1_loss_mask: 3.2182, stage2_loss_cls: 2.3067, stage2_pos_acc: 9.0000, stage2_loss_bbox: 0.7118, stage2_loss_iou: 0.4578, stage2_loss_mask: 3.1950, stage3_loss_cls: 2.2330, stage3_pos_acc: 12.0000, stage3_loss_bbox: 0.7023, stage3_loss_iou: 0.4549, stage3_loss_mask: 3.1789, stage4_loss_cls: 2.2335, stage4_pos_acc: 13.5000, stage4_loss_bbox: 0.7005, stage4_loss_iou: 0.4565, stage4_loss_mask: 3.1358, stage5_loss_cls: 2.2151, stage5_pos_acc: 15.5000, stage5_loss_bbox: 0.7503, stage5_loss_iou: 0.4773, stage5_loss_mask: 3.1019, loss: 39.6609, grad_norm: 141.0401\n",
            "2022-04-06 11:41:36,792 - mmdet - INFO - Epoch [1][1300/54392]\tlr: 2.500e-05, eta: 14 days, 14:26:23, time: 1.064, data_time: 0.007, memory: 14843, stage0_loss_cls: 2.2290, stage0_pos_acc: 13.1738, stage0_loss_bbox: 0.7471, stage0_loss_iou: 0.4773, stage0_loss_mask: 3.2728, stage1_loss_cls: 2.2083, stage1_pos_acc: 6.5000, stage1_loss_bbox: 0.7368, stage1_loss_iou: 0.4644, stage1_loss_mask: 3.3609, stage2_loss_cls: 2.2372, stage2_pos_acc: 17.3333, stage2_loss_bbox: 0.7373, stage2_loss_iou: 0.4691, stage2_loss_mask: 3.3692, stage3_loss_cls: 2.1691, stage3_pos_acc: 16.1500, stage3_loss_bbox: 0.7580, stage3_loss_iou: 0.4862, stage3_loss_mask: 3.0375, stage4_loss_cls: 2.1496, stage4_pos_acc: 15.6667, stage4_loss_bbox: 0.7525, stage4_loss_iou: 0.4807, stage4_loss_mask: 3.2591, stage5_loss_cls: 2.2011, stage5_pos_acc: 15.7810, stage5_loss_bbox: 0.7546, stage5_loss_iou: 0.4786, stage5_loss_mask: 3.1123, loss: 39.9487, grad_norm: 122.5996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDlb1YZHAVxL"
      },
      "outputs": [],
      "source": [
        "!du -sh log_mmdet/epoch_1.pth\n",
        "!ls log_mmdet\n",
        "## loss at epoch3 1.0179 and s0.loss_mask=0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnL44_XEAVxL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink('log_mmdet/epoch_1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q-g3EAviAVxL"
      },
      "outputs": [],
      "source": [
        "!pip install ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_unLu4QUaB5"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1-yrXVmhUlp"
      },
      "outputs": [],
      "source": [
        "#lets get the latest checkpoint file \n",
        "\n",
        "work_dir = \"/content/drive/MyDrive/log_mmdet\"\n",
        "checkpoint_file = os.path.join(work_dir, \"latest.pth\")\n",
        "assert os.path.isfile(\n",
        "    checkpoint_file), '`{}` not exist'.format(checkpoint_file)\n",
        "checkpoint_file = os.path.abspath(checkpoint_file)\n",
        "checkpoint_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF7gRgCPhUh6"
      },
      "outputs": [],
      "source": [
        "#Lets visualize some results\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "plt.rcParams[\"axes.grid\"] = False\n",
        "\n",
        "import mmcv\n",
        "from mmcv.runner import load_checkpoint\n",
        "import mmcv.visualization.image as mmcv_image\n",
        "# fix for colab\n",
        "\n",
        "def imshow(img, win_name='', wait_time=0): plt.figure(\n",
        "    figsize=(50, 50)); plt.imshow(img)\n",
        "\n",
        "\n",
        "mmcv_image.imshow = imshow\n",
        "from mmdet.models import build_detector\n",
        "from mmdet.apis import inference_detector, show_result_pyplot, init_detector\n",
        "\n",
        "\n",
        "score_thr = 0.1 #decrease the threshold if you feel like you are missing some predictions\n",
        "\n",
        "\n",
        "# build the model from a config file and a checkpoint file\n",
        "model = init_detector(config_fname, checkpoint_file)\n",
        "\n",
        "# test a single image and show the results\n",
        "img = '/content/data/val/images/008082.jpg'   #you can change this to any image you want!\n",
        "\n",
        "result = inference_detector(model, img)\n",
        "show_result_pyplot(model, img, result, score_thr=0.1, title='result', wait_time=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DZypqZS0GVD"
      },
      "source": [
        "## Create categories files for correct annotations during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MM7vkq0UgCg"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json, os\n",
        "annotation_path = os.path.join(\"data\", \"train/annotations.json\")\n",
        "json_file = open(annotation_path)\n",
        "coco = json.load(json_file)\n",
        "\n",
        "with open(\"classes.json\",'w') as f:\n",
        "    json.dump(coco[\"categories\"],f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PKWw5xpLZB5"
      },
      "source": [
        "## Copy the config file and trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snj_hOld14N6"
      },
      "outputs": [],
      "source": [
        "#copy the trained model and config file to home directory\n",
        "%cp /content/drive/MyDrive/log_mmdet/htc_without_semantic_r50_fpn_1x_coco.py /content/htc_without_semantic_r50_fpn_1x_coco.py\n",
        "%cp /content/drive/MyDrive/log_mmdet/latest.pth /content/latest.pth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxkRJocSI6li"
      },
      "source": [
        "# Quick Submission 💪\n",
        "\n",
        "## Inference on the public test set\n",
        "*   loading the model config and setting up related paths\n",
        "*   running inference and generating json file for submission\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S3I3dlegLmxA"
      },
      "outputs": [],
      "source": [
        "#@title Inference code from myfood exp repo, Used for quick submission\n",
        "%%writefile inference_mmdet.py\n",
        "'''\n",
        "@Author: Gaurav Singhal\n",
        "@Description: Standalone file for testing and evaluating\n",
        "the models. It doesn't do any post-processing or ensembling.\n",
        "'''\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import warnings\n",
        "import glob\n",
        "import json\n",
        "import mmcv\n",
        "import torch\n",
        "from mmcv import Config, DictAction\n",
        "from mmcv.cnn import fuse_conv_bn\n",
        "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
        "from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,\n",
        "                         wrap_fp16_model)\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "\n",
        "from mmdet.apis import multi_gpu_test\n",
        "from mmdet.datasets import (build_dataloader, build_dataset,\n",
        "                            replace_ImageToTensor)\n",
        "from mmdet.models import build_detector\n",
        "# import aicrowd_helpers\n",
        "import os.path as osp\n",
        "import traceback\n",
        "import pickle\n",
        "import shutil\n",
        "import tempfile\n",
        "import time\n",
        "import torch.distributed as dist\n",
        "from mmcv.image import tensor2imgs\n",
        "from mmdet.core import encode_mask_results\n",
        "\n",
        "import uuid\n",
        "\n",
        "# TEST_IMAGES_PATH = \"/mnt/public/xxx/imrec/data/val/images\"\n",
        "\n",
        "def create_test_predictions(images_path):\n",
        "    test_predictions_file = tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".json\")\n",
        "\t\n",
        "    annotations = {'categories': [], 'info': {}, 'images': []}\n",
        "    for item in glob.glob(images_path+'/*.jpg'):\n",
        "        image_dict = dict()\n",
        "        img = mmcv.imread(item)\n",
        "        height,width,__ = img.shape\n",
        "        id = int(os.path.basename(item).split('.')[0])\n",
        "        image_dict['id'] = id\n",
        "        image_dict['file_name'] = os.path.basename(item)\n",
        "        image_dict['width'] = width\n",
        "        image_dict['height'] = height\n",
        "        annotations['images'].append(image_dict)\n",
        "    annotations['categories'] = json.loads(open(\"classes.json\").read())\n",
        "    json.dump(annotations, open(test_predictions_file.name, 'w'))\n",
        "\n",
        "    return test_predictions_file\n",
        "\n",
        "def single_gpu_test(model,\n",
        "                    data_loader,\n",
        "                    show=False,\n",
        "                    out_dir=None,\n",
        "                    show_score_thr=0.3):\n",
        "    \n",
        "    model.eval()\n",
        "    results = []\n",
        "    dataset = data_loader.dataset\n",
        "    prog_bar = mmcv.ProgressBar(len(dataset))\n",
        "    for i, data in enumerate(data_loader):\n",
        "        # aicrowd_helpers.execution_progress({\"image_ids\" : [i]})\n",
        "        with torch.no_grad():\n",
        "            result = model(return_loss=False, rescale=True, **data)\n",
        "\n",
        "        batch_size = len(result)\n",
        "        if show or out_dir:\n",
        "            if batch_size == 1 and isinstance(data['img'][0], torch.Tensor):\n",
        "                img_tensor = data['img'][0]\n",
        "            else:\n",
        "                img_tensor = data['img'][0].data[0]\n",
        "            img_metas = data['img_metas'][0].data[0]\n",
        "            imgs = tensor2imgs(img_tensor, **img_metas[0]['img_norm_cfg'])\n",
        "            assert len(imgs) == len(img_metas)\n",
        "\n",
        "            for i, (img, img_meta) in enumerate(zip(imgs, img_metas)):\n",
        "                h, w, _ = img_meta['img_shape']\n",
        "                img_show = img[:h, :w, :]\n",
        "\n",
        "                ori_h, ori_w = img_meta['ori_shape'][:-1]\n",
        "                img_show = mmcv.imresize(img_show, (ori_w, ori_h))\n",
        "\n",
        "                if out_dir:\n",
        "                    out_file = osp.join(out_dir, img_meta['ori_filename'])\n",
        "                else:\n",
        "                    out_file = None\n",
        "\n",
        "                model.module.show_result(\n",
        "                    img_show,\n",
        "                    result[i],\n",
        "                    show=show,\n",
        "                    out_file=out_file,\n",
        "                    score_thr=show_score_thr)\n",
        "\n",
        "        # Perform RLE encode for masks\n",
        "        if isinstance(result[0], tuple):\n",
        "            result = [(bbox_results, encode_mask_results(mask_results))\n",
        "                      for bbox_results, mask_results in result]\n",
        "        results.extend(result)\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            prog_bar.update()\n",
        "    return results\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='MMDet test (and eval) a model')\n",
        "    parser.add_argument('--config', help='test config file path')\n",
        "    parser.add_argument('--checkpoint', help='checkpoint file')\n",
        "    parser.add_argument('--data', help='test data folder path')\n",
        "    parser.add_argument('--out', help='output result file in pickle format')\n",
        "    parser.add_argument(\n",
        "        '--fuse-conv-bn',\n",
        "        action='store_true',\n",
        "        help='Whether to fuse conv and bn, this will slightly increase'\n",
        "        'the inference speed')\n",
        "    parser.add_argument(\n",
        "        '--format-only',\n",
        "        action='store_true',\n",
        "        help='Format the output results without perform evaluation. It is'\n",
        "        'useful when you want to format the result to a specific format and '\n",
        "        'submit it to the test server')\n",
        "    parser.add_argument(\n",
        "        '--eval',\n",
        "        type=str,\n",
        "        nargs='+',\n",
        "        help='evaluation metrics, which depends on the dataset, e.g., \"bbox\",'\n",
        "        ' \"segm\", \"proposal\" for COCO, and \"mAP\", \"recall\" for PASCAL VOC')\n",
        "    parser.add_argument('--show', action='store_true', help='show results')\n",
        "    parser.add_argument(\n",
        "        '--show-dir', help='directory where painted images will be saved')\n",
        "    parser.add_argument(\n",
        "        '--show-score-thr',\n",
        "        type=float,\n",
        "        default=0.3,\n",
        "        help='score threshold (default: 0.3)')\n",
        "    parser.add_argument(\n",
        "        '--gpu-collect',\n",
        "        action='store_true',\n",
        "        help='whether to use gpu to collect results.')\n",
        "    parser.add_argument(\n",
        "        '--tmpdir',\n",
        "        help='tmp directory used for collecting results from multiple '\n",
        "        'workers, available when gpu-collect is not specified')\n",
        "    parser.add_argument(\n",
        "        '--cfg-options',\n",
        "        nargs='+',\n",
        "        action=DictAction,\n",
        "        help='override some settings in the used config, the key-value pair '\n",
        "        'in xxx=yyy format will be merged into config file.')\n",
        "    parser.add_argument(\n",
        "        '--options',\n",
        "        nargs='+',\n",
        "        action=DictAction,\n",
        "        help='custom options for evaluation, the key-value pair in xxx=yyy '\n",
        "        'format will be kwargs for dataset.evaluate() function (deprecate), '\n",
        "        'change to --eval-options instead.')\n",
        "    parser.add_argument(\n",
        "        '--eval-options',\n",
        "        nargs='+',\n",
        "        action=DictAction,\n",
        "        help='custom options for evaluation, the key-value pair in xxx=yyy '\n",
        "        'format will be kwargs for dataset.evaluate() function')\n",
        "    parser.add_argument(\n",
        "        '--launcher',\n",
        "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
        "        default='none',\n",
        "        help='job launcher')\n",
        "    parser.add_argument('--out_file', help='output result file')\n",
        "    parser.add_argument('--local_rank', type=int, default=0)\n",
        "    parser.add_argument('--type', type=str, choices=['val', 'test'], default='test')\n",
        "    parser.add_argument('--reduce_ms', action='store_true',\n",
        "        help='Whether to reduce the multi-scale aug')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if 'LOCAL_RANK' not in os.environ:\n",
        "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
        "\n",
        "    if args.options and args.eval_options:\n",
        "        raise ValueError(\n",
        "            '--options and --eval-options cannot be both '\n",
        "            'specified, --options is deprecated in favor of --eval-options')\n",
        "    if args.options:\n",
        "        warnings.warn('--options is deprecated in favor of --eval-options')\n",
        "        args.eval_options = args.options\n",
        "    return args\n",
        "\n",
        "def reduce_multiscale_TTA(cfg):\n",
        "    '''\n",
        "    Keep only 1st and last image sizes from Multi-Scale TTA\n",
        "    \n",
        "    @input\n",
        "    cfg -> Configuration file\n",
        "    '''\n",
        "\n",
        "    scale = cfg.data.test.pipeline[1]['img_scale']\n",
        "    if len(scale) > 2:\n",
        "        new_scale = [scale[0], scale[-1]]\n",
        "        cfg.data.test.pipeline[1]['img_scale'] = new_scale   \n",
        "    return cfg\n",
        "\n",
        "def main():\n",
        "    ########################################################################\n",
        "    # Register Prediction Start\n",
        "    ########################################################################\n",
        "\n",
        "    # aicrowd_helpers.execution_start()\n",
        "    args = parse_args()\n",
        "    data_folder = args.data\n",
        "    # Create annotations if not already created\n",
        "    test_predictions_file = create_test_predictions(data_folder)\n",
        "    \n",
        "    # Load annotations\n",
        "    with open(test_predictions_file.name) as f:\n",
        "        annotations = json.loads(f.read())\n",
        "\n",
        "    assert args.out or args.eval or args.format_only or args.show \\\n",
        "        or args.show_dir, \\\n",
        "        ('Please specify at least one operation (save/eval/format/show the '\n",
        "         'results / save the results) with the argument \"--out\", \"--eval\"'\n",
        "         ', \"--format-only\", \"--show\" or \"--show-dir\"')\n",
        "\n",
        "    if args.eval and args.format_only:\n",
        "        raise ValueError('--eval and --format_only cannot be both specified')\n",
        "\n",
        "    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):\n",
        "        raise ValueError('The output file must be a pkl file.')\n",
        "\n",
        "    cfg = Config.fromfile(args.config)\n",
        "    if args.cfg_options is not None:\n",
        "        cfg.merge_from_dict(args.cfg_options)\n",
        "    \n",
        "    JSONFILE_PREFIX=\"predictions_{}\".format(str(uuid.uuid4())) \n",
        "    # import modules present in list of strings.\n",
        "    if cfg.get('custom_imports', None):\n",
        "        from mmcv.utils import import_modules_from_strings\n",
        "        import_modules_from_strings(**cfg['custom_imports'])\n",
        "    \n",
        "    # set cudnn_benchmark\n",
        "    if cfg.get('cudnn_benchmark', False):\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "    cfg.data.samples_per_gpu = 1\n",
        "    cfg.data.workers_per_gpu = 2\n",
        "    cfg.model.pretrained = None\n",
        "    cfg.data.test.test_mode = True\n",
        "    cfg.data.test.ann_file = test_predictions_file.name\n",
        "    cfg.data.test.img_prefix = data_folder\n",
        "\n",
        "    if cfg.model.get('neck'):\n",
        "        if isinstance(cfg.model.neck, list):\n",
        "            for neck_cfg in cfg.model.neck:\n",
        "                if neck_cfg.get('rfp_backbone'):\n",
        "                    if neck_cfg.rfp_backbone.get('pretrained'):\n",
        "                        neck_cfg.rfp_backbone.pretrained = None\n",
        "        elif cfg.model.neck.get('rfp_backbone'):\n",
        "            if cfg.model.neck.rfp_backbone.get('pretrained'):\n",
        "                cfg.model.neck.rfp_backbone.pretrained = None\n",
        "\n",
        "    # in case the test dataset is concatenated\n",
        "    if isinstance(cfg.data.test, dict):\n",
        "        cfg.data.test.test_mode = True\n",
        "    elif isinstance(cfg.data.test, list):\n",
        "        for ds_cfg in cfg.data.test:\n",
        "            ds_cfg.test_mode = True\n",
        "\n",
        "    cfg.data.test.ann_file = test_predictions_file.name\n",
        "    cfg.data.test.img_prefix = data_folder\n",
        "        \n",
        "    # if args.reduce_ms:\n",
        "    #     print(\"Reduce multi-scale TTA\")\n",
        "    #     cfg = reduce_multiscale_tta(cfg)\n",
        "    #     print(cfg.data.test.pipeline[1]['img_scale'])\n",
        "        \n",
        "    if args.launcher == 'none':\n",
        "        distributed = False\n",
        "    else:\n",
        "        distributed = True\n",
        "        init_dist(args.launcher, **cfg.dist_params)\n",
        "    \n",
        "    # build the dataloader\n",
        "    samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)\n",
        "    if samples_per_gpu > 1:\n",
        "        # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
        "        cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
        "    dataset = build_dataset(cfg.data.test)\n",
        "    print(dataset)\n",
        "    dataset.cat_ids = [category[\"id\"] for category in annotations[\"categories\"]]\n",
        "    data_loader = build_dataloader(\n",
        "        dataset,\n",
        "        samples_per_gpu=1,\n",
        "        workers_per_gpu=2,\n",
        "        dist=distributed,\n",
        "        shuffle=False)\n",
        "\n",
        "    # build the model and load checkpoint\n",
        "    # model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.model.test_cfg)\n",
        "    model = init_detector(args.config,args.checkpoint,device='cuda:0')\n",
        "\n",
        "    fp16_cfg = cfg.get('fp16', None)\n",
        "    if fp16_cfg is not None:\n",
        "        wrap_fp16_model(model)\n",
        "    # checkpoint = load_checkpoint(model, args.checkpoint, map_location='cuda')\n",
        "    if args.fuse_conv_bn:\n",
        "        model = fuse_conv_bn(model)\n",
        "\n",
        "    model.CLASSES = [category['name'] for category in annotations['categories']]\n",
        "    # if 'CLASSES' in checkpoint['meta']:\n",
        "        # model.CLASSES = checkpoint['meta']['CLASSES']\n",
        "    # else:\n",
        "        # model.CLASSES = dataset.CLASSES\n",
        "\n",
        "    if not distributed:\n",
        "        model = MMDataParallel(model, device_ids=[0])\n",
        "        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir,\n",
        "                                  args.show_score_thr)\n",
        "    else:\n",
        "        model = MMDistributedDataParallel(\n",
        "            model.cuda(),\n",
        "            device_ids=[torch.cuda.current_device()],\n",
        "            broadcast_buffers=False)\n",
        "        outputs = multi_gpu_test(model, data_loader, args.tmpdir,\n",
        "                                 args.gpu_collect)\n",
        "\n",
        "    rank, _ = get_dist_info()\n",
        "    if rank == 0:\n",
        "        if args.out:\n",
        "            print(f'\\nwriting results to {args.out}')\n",
        "            mmcv.dump(outputs, args.out)\n",
        "        kwargs = {} if args.eval_options is None else args.eval_options\n",
        "        if args.format_only:\n",
        "            dataset.format_results(outputs, **kwargs)\n",
        "        if args.eval:\n",
        "            eval_kwargs = cfg.get('evaluation', {}).copy()\n",
        "            for key in ['interval', 'tmpdir', 'start', 'gpu_collect']:\n",
        "                eval_kwargs.pop(key, None)\n",
        "            eval_kwargs.update(dict(metric=args.eval, **kwargs))\n",
        "            print(dataset.evaluate(outputs, **eval_kwargs))\n",
        "    \n",
        "    # consolidate_results([\"predictions.segm.json\"], 'test_predictions.json', args.out_file)\n",
        "    ########################################################################\n",
        "    # Register Prediction Complete\n",
        "    ########################################################################\n",
        "    # aicrowd_helpers.execution_success({\n",
        "    #     \"predictions_output_path\" : args.out_file\n",
        "    # })\n",
        "    print(\"\\nAICrowd register complete\")\n",
        "    # preds = []\n",
        "    # with open(\"predictions.segm.json\", \"r\") as pred_file:\n",
        "    #     preds.extend(json.loads(pred_file.read()))\n",
        "    # print(preds)\n",
        "    JSONFILE_PREFIX = args.eval_options['jsonfile_prefix']\n",
        "    shutil.move(\"{}.segm.json\".format(JSONFILE_PREFIX), args.out_file)\n",
        "    os.remove(\"{}.bbox.json\".format(JSONFILE_PREFIX))\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        error = traceback.format_exc()\n",
        "        print(error)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaRs_44KVWje"
      },
      "outputs": [],
      "source": [
        "#setting the paths for images and output file\n",
        "test_images_dir=\"/content/data/test/images\"\n",
        "output_filepath=\"/content/predictions_mmdetection.json\"\n",
        "\n",
        "#path of trained model & config\n",
        "model_path=\"/content/latest.pth\"\n",
        "config_file=\"/content/htc_without_semantic_r50_fpn_1x_coco.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N_pxuV4O6vie"
      },
      "outputs": [],
      "source": [
        "!python inference_mmdet.py --config $config_file --checkpoint $model_path \\\n",
        "--data $test_images_dir \\\n",
        "--format-only --eval-options \"jsonfile_prefix=preds\" --out_file $output_filepath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tALk5JdRkJsk"
      },
      "source": [
        "Now that the prediction file is generated for public test set, To make quick submission:\n",
        "* Use AIcrowd CLL `aicrowd submit` command to do a quick submission. </br>\n",
        "\n",
        "**Alternatively:**\n",
        "* download the `predictions_mmdetection.json` file by running below cell\n",
        "* visit the [create submission page](https://www.aicrowd.com/challenges/food-recognition-benchmark-2022/submissions/new) \n",
        "* Upload the `predictions_mmdetection.json` file \n",
        "* Voila!! You just made your first submission!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk-T7fKAxR2c"
      },
      "outputs": [],
      "source": [
        "#use aicrowd CLI to make quick submission\n",
        "!aicrowd submission create -c food-recognition-benchmark-2022 -f $output_filepath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iijfdw1VgXke"
      },
      "source": [
        "#Active submission 🤩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTtj3n0RrcjH"
      },
      "source": [
        "Step 0 : Fork the baseline to make your own changes to it. Go to settings and make the repo private.\n",
        "\n",
        "\n",
        "Step 1 : For first time setup, Setting up SSH to login to Gitlab.\n",
        "\n",
        "  0. Run the next cell to check if you already have SSH keys in your drive, if yes, skip this step. \n",
        "  1. Run `ssh-keygen -t ecdsa -b 521` \n",
        "  2. Run `cat ~./ssh/id_ecdsa.pub` and copy the output\n",
        "  3. Go to [Gitlab SSH Keys](https://gitlab.aicrowd.com/profile/keys) and then paste the output inside the key and use whaever title you like. \n",
        "\n",
        "\n",
        "Step 2: Clone your forked Repo & Add Models & Push Changes\n",
        "\n",
        "  1. Run `git clone git@gitlab.aicrowd.com:[Your Username]/mmdetection-starter-food-2022.git`\n",
        "  2. Put your model inside the models directioary and then run `git-lfs track \"*.pth\"`\n",
        "  3. Run `git add .` then `git commit -m \" adding model\"`\n",
        "  3. Run `git push origin master`\n",
        "\n",
        "Step 3. Create Submission\n",
        "\n",
        "  1. Go to the repo and then tags and then New Tag. \n",
        "  2. In the tag name,you can use `submission_v1`, ( Everytime you make a new submission, just increase the no. like - `submission_v2`,  `submission_v3` )\n",
        "  3. A new issue will be created with showing the process. Enjoy!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "If you do not have SSH Keys, Check this [Page](https://docs.gitlab.com/ee/ssh/index.html#generate-an-ssh-key-pair)\n",
        "\n",
        "Add your SSH Keys to your GitLab account by following the instructions here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMX3ttbbnRLP"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "SSH_PRIV_KEY=/content/drive/MyDrive/id_ecdsa\n",
        "SSH_PUB_KEY=/content/drive/MyDrive/id_ecdsa.pub\n",
        "if [ -f \"$SSH_PRIV_KEY\" ]; then\n",
        "    echo \"SSH Key found! ✅\\n\"\n",
        "    mkdir -p /root/.ssh\n",
        "    cp /content/drive/MyDrive/id_ecdsa ~/.ssh/id_ecdsa\n",
        "    cp /content/drive/MyDrive/id_ecdsa.pub ~/.ssh/id_ecdsa.pub\n",
        "    echo \"SSH key successfully copied to local!\"\n",
        "else\n",
        "    echo \"SSH Key does not exist.\"\n",
        "    ssh-keygen -t ecdsa -b521 -f ~/.ssh/id_ecdsa\n",
        "    cat ~/.ssh/id_ecdsa.pub\n",
        "    echo \"❗️Please open https://gitlab.aicrowd.com/profile/keys and copy-paste the above text in the **key** textbox.\"\n",
        "    cp  ~/.ssh/id_ecdsa /content/drive/MyDrive/id_ecdsa\n",
        "    cp  ~/.ssh/id_ecdsa.pub /content/drive/MyDrive/id_ecdsa.pub\n",
        "    echo \"SSH key successfully created and copied to drive!\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5NUAlB1ONYZ"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "html = \"<b>Copy paste below SSH key in your GitLab account here (one time):</b><br/>\"\n",
        "html += '<a href=\"https://gitlab.aicrowd.com/-/profile/keys\" target=\"_blank\">https://gitlab.aicrowd.com/-/profile/keys</a><br><br>'\n",
        "\n",
        "public_key = open(\"/content/drive/MyDrive/id_ecdsa.pub\").read()\n",
        "html += '<br/><textarea>'+public_key+'</textarea><button onclick=\"navigator.clipboard.writeText(\\''+public_key.strip()+'\\');this.innerHTML=\\'Copied ✅\\'\">Click to copy</button>'\n",
        "IPython.display.HTML(html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4bMsMOuqXWG"
      },
      "source": [
        "Clone the gitlab starter repo and add submission files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8VOx9WUWvqW"
      },
      "outputs": [],
      "source": [
        "# Set your AIcrowd username for action submission.\n",
        "# This username will store repository and used for submitter's username, etc\n",
        "username = \"jerome_patel\"\n",
        "!echo -n {username} > author.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX54KjC7qKXq"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "username=$(cat author.txt)\n",
        "echo \"Username $username\"\n",
        "\n",
        "git config --global user.name \"$username\"\n",
        "git config --global user.email \"$username@noreply.gitlab.aicrowd.com\"\n",
        "\n",
        "touch ${HOME}/.ssh/known_hosts\n",
        "ssh-keyscan -H gitlab.aicrowd.com >> ${HOME}/.ssh/known_hosts 2> /dev/null\n",
        "\n",
        "\n",
        "apt install -qq -y jq git-lfs &> /dev/null\n",
        "\n",
        "git lfs install\n",
        "cd /content/\n",
        "\n",
        "echo \"Checking if repository already exist, otherwise create one\"\n",
        "export SUBMISSION_REPO=\"git@gitlab.aicrowd.com:$username/mmdetection-starter-food-2022.git\"\n",
        "echo \"cloning the $SUBMISSION_REPO\"\n",
        "git clone $SUBMISSION_REPO mmdetection-starter-food-2022\n",
        "ALREADYEXIST=$?\n",
        "\n",
        "if [ $ALREADYEXIST -ne 0 ]; then\n",
        "  echo \"Project didn't exist, forking from upstream\"\n",
        "  git clone https://github.com/AIcrowd/food-recognition-benchmark-starter-kit.git mmdetection-starter-food-2022\n",
        "fi\n",
        "\n",
        "cd /content/mmdetection-starter-food-2022\n",
        "git remote remove origin\n",
        "git remote add origin \"$SUBMISSION_REPO\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXGI6THoJglP"
      },
      "source": [
        "## To make active submission:\n",
        "* Required Files are `aicrowd.json, apt.txt, requirements.txt, predict.py` (already configured for mmdetection)\n",
        "* **[IMP]** Copy mmdetection trained model, corresponding config file to repo\n",
        "* for inference place these files : `predict_mmdetection.py mmdet_inference.py` (already present in repo)\n",
        "* Modify requirements.txt and `predict.py` for mmdetection\n",
        "* **[IMP]** Modify `aicrowd.json` for your submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy7mjKukKKMH"
      },
      "source": [
        "**Note:** You only need to place your trained model and modify aicrowd.json to create your first easy submission. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QdHHINoCb9bD"
      },
      "outputs": [],
      "source": [
        "#@title Modify mmdet_inference.py (modify and run only if you want to change the inference)\n",
        "%%writefile /content/mmdetection-starter-food-2022/utils/mmdet_inference.py\n",
        "\n",
        "import mmcv\n",
        "import numpy as np\n",
        "import torch\n",
        "from mmcv.ops import RoIPool\n",
        "from mmcv.parallel import collate, scatter\n",
        "from mmcv.runner import load_checkpoint\n",
        "\n",
        "from mmdet.core import get_classes\n",
        "from mmdet.datasets import replace_ImageToTensor\n",
        "from mmdet.datasets.pipelines import Compose\n",
        "from mmdet.models import build_detector\n",
        "# import time\n",
        "\n",
        "\n",
        "def inference(model, imgs):\n",
        "\n",
        "    # start = time.process_time()\n",
        "    imgs = [imgs]\n",
        "    cfg = model.cfg\n",
        "    device = 'cuda:0'\n",
        "    if isinstance(imgs[0], np.ndarray):\n",
        "        cfg = cfg.copy()\n",
        "        # set loading pipeline type\n",
        "        cfg.data.test.pipeline[0].type = 'LoadImageFromWebcam'\n",
        "\n",
        "    cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
        "    test_pipeline = Compose(cfg.data.test.pipeline)\n",
        "\n",
        "    datas = []\n",
        "    data = dict(img_info=dict(filename=imgs[0]), img_prefix=None)\n",
        "    # build the data pipeline\n",
        "    data = test_pipeline(data)\n",
        "    datas.append(data)\n",
        "\n",
        "    data = collate(datas, samples_per_gpu=len(imgs))\n",
        "    # just get the actual data from DataContainer\n",
        "    data['img_metas'] = [img_metas.data[0] for img_metas in data['img_metas']]\n",
        "    data['img'] = [img.data[0] for img in data['img']]\n",
        "    # scatter to specified GPU\n",
        "    data = scatter(data, [device])[0]\n",
        "    \n",
        "    # forward the model\n",
        "    with torch.no_grad():\n",
        "        results = model(return_loss=False, rescale=True, **data)\n",
        "    # your code here    \n",
        "    # print(time.process_time() - start)\n",
        "    return results[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "66EXvv_OAeB4"
      },
      "outputs": [],
      "source": [
        "#@title Modify predict_mmdetection.py (modify & run this only if you want to change inference code part)\n",
        "%%writefile /content/mmdetection-starter-food-2022/predict_mmdetection.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "from PIL import Image\n",
        "import importlib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import traceback\n",
        "import pickle\n",
        "import shutil\n",
        "import glob\n",
        "import tempfile\n",
        "import time\n",
        "import mmcv\n",
        "import torch.distributed as dist\n",
        "from mmcv.image import tensor2imgs\n",
        "from mmdet.core import encode_mask_results\n",
        "from mmcv import Config, DictAction\n",
        "from mmcv.cnn import fuse_conv_bn\n",
        "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
        "from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,\n",
        "                         wrap_fp16_model)\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "\n",
        "from mmdet.apis import multi_gpu_test\n",
        "from mmdet.datasets import (build_dataloader, build_dataset,\n",
        "                            replace_ImageToTensor)\n",
        "from mmdet.models import build_detector\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "from utils.mmdet_inference import inference\n",
        "from evaluator.food_challenge import FoodChallengePredictor\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Expected ENVIRONMENT Variables\n",
        "* AICROWD_TEST_IMAGES_PATH : abs path to  folder containing all the test images\n",
        "* AICROWD_PREDICTIONS_OUTPUT_PATH : path where you are supposed to write the output predictions.json\n",
        "\"\"\"\n",
        "\n",
        "class MMDetectionPredictor(FoodChallengePredictor):\n",
        "\n",
        "    \"\"\"\n",
        "    PARTICIPANT_TODO:\n",
        "    You can do any preprocessing required for your codebase here like loading up models into memory, etc.\n",
        "    \"\"\"\n",
        "    def prediction_setup(self):\n",
        "        # self.PADDING = 50\n",
        "        # self.SEGMENTATION_LENGTH = 10\n",
        "        # self.MAX_NUMBER_OF_ANNOTATIONS = 10\n",
        "\n",
        "        #set the config parameters, including the architecture which was previously used\n",
        "        self.cfg_name, self.checkpoint_name = self.get_mmdetection_config()\n",
        "        self.cfg = Config.fromfile(self.cfg_name)\n",
        "        # self.test_img_path = os.getenv(\"AICROWD_TEST_IMAGES_PATH\", os.getcwd() + \"/data/images/\")\n",
        "        self.test_predictions_file = self.create_test_predictions(self.test_data_path)\n",
        "\n",
        "        if self.cfg.get('cudnn_benchmark', False):\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "        self.cfg.data.samples_per_gpu = 1\n",
        "        self.cfg.data.workers_per_gpu = 2\n",
        "        self.cfg.model.pretrained = None\n",
        "        self.cfg.data.test.test_mode = True\n",
        "        self.cfg.data.test.ann_file = self.test_predictions_file.name\n",
        "        self.cfg.data.test.img_prefix = self.test_data_path\n",
        "\n",
        "        self.model = init_detector(self.cfg_name,self.checkpoint_name,device='cuda:0')\n",
        "\n",
        "        fp16_cfg = self.cfg.get('fp16', None)\n",
        "        if fp16_cfg is not None:\n",
        "            wrap_fp16_model(self.model)\n",
        "\n",
        "         # Load annotations\n",
        "        with open(self.test_predictions_file.name) as f:\n",
        "            self.annotations = json.loads(f.read())\n",
        "        self.cat_ids = [category[\"id\"] for category in self.annotations[\"categories\"]]\n",
        "\n",
        "        self.model.CLASSES = [category['name'] for category in self.annotations['categories']]\n",
        "\n",
        "    \"\"\"\n",
        "    PARTICIPANT_TODO:\n",
        "    During the evaluation all image file path will be provided one by one.\n",
        "    NOTE: In case you want to load your model, please do so in `predict_setup` function.\n",
        "    \"\"\"\n",
        "    def prediction(self, image_path):\n",
        "        print(\"Generating for\", image_path)\n",
        "        # read the image\n",
        "        result = inference(self.model, image_path)\n",
        "        #RLE Encode the masks\n",
        "        result = (result[0], encode_mask_results(result[1]))\n",
        "        result = self.segm2jsonformat(result,image_path)\n",
        "        return result\n",
        "\n",
        "    def xyxy2xywh(self,bbox):\n",
        "        _bbox = bbox.tolist()\n",
        "        return [\n",
        "            _bbox[0],\n",
        "            _bbox[1],\n",
        "            _bbox[2] - _bbox[0] + 1,\n",
        "            _bbox[3] - _bbox[1] + 1,\n",
        "        ]\n",
        "\n",
        "    def segm2jsonformat(self, result,image_path):\n",
        "        segm_json_results = []\n",
        "        img_id = int(os.path.basename(image_path).split(\".\")[0])\n",
        "        det, seg = result\n",
        "        # print(\"image:\",img_id)\n",
        "        for label in range(len(det)):\n",
        "                bboxes = det[label]\n",
        "                #print(type(bboxes))\n",
        "                segms = seg[label]\n",
        "                mask_score = [bbox[4] for bbox in bboxes]\n",
        "                for i in range(len(bboxes)):\n",
        "                        data = dict()\n",
        "                        data['image_id'] = img_id\n",
        "                        data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
        "                        data['score'] = float(mask_score[i])\n",
        "                        data['category_id'] = self.cat_ids[label]\n",
        "                        if isinstance(segms[i]['counts'], bytes):\n",
        "                                segms[i]['counts'] = segms[i]['counts'].decode()\n",
        "                        data['segmentation'] = segms[i]\n",
        "                        segm_json_results.append(data)\n",
        "        return segm_json_results\n",
        "\n",
        "\n",
        "    def create_test_predictions(self,images_path):\n",
        "        test_predictions_file = tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".json\")\n",
        "        annotations = {'categories': [], 'info': {}, 'images': []}\n",
        "        for item in glob.glob(images_path+'/*.jpg'):\n",
        "            image_dict = dict()\n",
        "            img = mmcv.imread(item)\n",
        "            height,width,__ = img.shape\n",
        "            id = int(os.path.basename(item).split('.')[0])\n",
        "            image_dict['image_id'] = id\n",
        "            image_dict['file_name'] = os.path.basename(item)\n",
        "            image_dict['width'] = width\n",
        "            image_dict['height'] = height\n",
        "            annotations['images'].append(image_dict)\n",
        "        annotations['categories'] = json.loads(open(\"classes.json\").read())\n",
        "        json.dump(annotations, open(test_predictions_file.name, 'w'))\n",
        "\n",
        "        return test_predictions_file\n",
        "\n",
        "    def get_mmdetection_config(self):\n",
        "        with open('aicrowd.json') as f:\n",
        "            content = json.load(f)\n",
        "            config_fname = content['model_config_file']\n",
        "            checkpoint_fname = content['model_path']\n",
        "        # config = Config.fromfile(config_fname)\n",
        "        return (config_fname, checkpoint_fname)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    submission = MMDetectionPredictor()\n",
        "    submission.run()\n",
        "    print(\"Successfully generated predictions!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIekmrfZLOJD"
      },
      "outputs": [],
      "source": [
        "MODEL_ARCH = \"htc_without_semantic_r50_fpn_1x_coco.py\"\n",
        "aicrowd_json = {\n",
        "  \"challenge_id\" : \"food-recognition-benchmark-2022\",\n",
        "  \"authors\" : [\"pola_saidinesh\"],\n",
        "  \"description\" : \"Food Recognition Benchmark 2022 Submission mmdetection\",\n",
        "  \"license\" : \"MIT\",\n",
        "  \"gpu\": True,\n",
        "  \"debug\": False,\n",
        "  \"model_path\": \"models/latest.pth\",\n",
        "  \"model_type\": \"mmdetection\",\n",
        "  \"model_config_file\": \"models/\" + MODEL_ARCH\n",
        "}\n",
        "import json\n",
        "with open('/content/mmdetection-starter-food-2022/aicrowd.json', 'w') as fp:\n",
        "  fp.write(json.dumps(aicrowd_json, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aRgmsuQPBUJ"
      },
      "source": [
        "### Copy required files (trained model, config, classes.json) to mmdetection repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WorE1uF3OTJH"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/mmdetection-starter-food-2022/models\n",
        "!cp /content/classes.json /content/mmdetection-starter-food-2022/utils/classes.json\n",
        "!cp /content/latest.pth /content/mmdetection-starter-food-2022/models/latest.pth\n",
        "!cp $MODEL_ARCH /content/mmdetection-starter-food-2022/models/$MODEL_ARCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJnxrSS0PZAp"
      },
      "source": [
        "### Finally push the repo for active submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO1Zu8V5MH_Y"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "## Set your unique tag for this submission (no spaces), example:\n",
        "# export MSG=\"v1\"\n",
        "# export MSG=\"v2\" ...\n",
        "# or something more informative...\n",
        "export MSG=\"mmdetection_submission_v0_1\"\n",
        "\n",
        "username=$(cat author.txt)\n",
        "echo \"Username $username\"\n",
        "\n",
        "\n",
        "cd /content/mmdetection-starter-food-2022\n",
        "git lfs track \"*.pth\"\n",
        "git add .gitattributes\n",
        "git add --all\n",
        "git commit -m \"$MSG\" || true\n",
        "\n",
        "find . -type f -size +5M -exec git lfs migrate import --include={} &> /dev/null \\;\n",
        "\n",
        "git tag -am \"submission_$MSG\" \"submission_$MSG\"\n",
        "git config lfs.https://gitlab.aicrowd.com/$username/mmdetection-starter-food-2022.git/info/lfs.locksverify false\n",
        "\n",
        "git remote remove origin\n",
        "git remote add origin git@gitlab.aicrowd.com:$username/mmdetection-starter-food-2022.git\n",
        "\n",
        "git lfs push origin master\n",
        "git push origin master\n",
        "git push origin \"submission_$MSG\"\n",
        "\n",
        "echo \"Track your submission status here: https://gitlab.aicrowd.com/$username/mmdetection-starter-food-2022/issues\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKuN7XOKJFT2"
      },
      "source": [
        "## Local Evaluation for Active Submission Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOATB0cWXbt2"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /content/mmdetection-starter-food-2022\n",
        "\n",
        "export TEST_DATASET_PATH=../data/test/images\n",
        "export RESULTS_DATASET_PATH=../data\n",
        "./run.sh"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oswZEmthIcAf",
        "Q4Uwa47-XuNu",
        "pZ0LiLXFI9k4",
        "REa66aaYKKXE",
        "1pEFUeZH1JEp",
        "SPmik-f4VE-W",
        "2A8_s1WAv29h",
        "iwrJY9SekxTi",
        "6_unLu4QUaB5",
        "_DZypqZS0GVD",
        "0PKWw5xpLZB5",
        "DXGI6THoJglP",
        "8aRgmsuQPBUJ",
        "wJnxrSS0PZAp",
        "gKuN7XOKJFT2"
      ],
      "machine_shape": "hm",
      "name": "queryInst-mmdet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}